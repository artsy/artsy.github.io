<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: EC2 | Artsy Engineering]]></title>
  <link href="https://artsy.github.io/blog/categories/ec2/atom.xml" rel="self"/>
  <link href="https://artsy.github.io/"/>
  <updated>2024-08-07T18:22:11+00:00</updated>
  <id>https://artsy.github.io/</id>
  <author>
    <name><![CDATA[]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[On-Demand Jenkins Agents with Amazon EC2]]></title>
    <link href="https://artsy.github.io/blog/2012/07/10/on-demand-jenkins-agents-with-amazon-ec2/"/>
    <updated>2012-07-10T13:30:00+00:00</updated>
    <id>https://artsy.github.io/blog/2012/07/10/on-demand-jenkins-agents-with-amazon-ec2</id>
    <content type="html"><![CDATA[<p>The <a href="http://artsy.net">Artsy</a> team faithfully uses <a href="http://jenkins-ci.org">Jenkins</a> for continuous integration.
<a href="http://artsy.github.com/blog/2012/05/27/using-jenkins-for-ruby-and-ruby-on-rails-teams/">As we’ve described before</a>,
our Jenkins controller and 8 agents run on Linode. This arrangement has at least a few drawbacks:</p>

<ul>
  <li>Our Linode servers are manually configured. They require frequent maintenance, and inconsistencies lead to
surprising build failures.</li>
  <li>The fixed set of agents don’t match the pattern of our build jobs: jobs get backed up during the day, but servers
are mostly unused overnight and on weekends.</li>
</ul>

<p>The <a href="https://wiki.jenkins-ci.org/display/JENKINS/Amazon+EC2+Plugin">Amazon EC2 Plugin</a> allowed us to replace those
agents with a totally scripted environment. Now, agents are spun up in the cloud whenever build jobs need them.</p>

<!-- more -->

<p>To set up the build agent’s Amazon Machine Image (AMI), we started from an
<a href="http://cloud-images.ubuntu.com/releases/oneiric/release/">official Ubuntu 11.10</a> (Oneiric Ocelot) AMI, ran
initialization scripts to set up our build dependencies (MongoDB, Redis, ImageMagick, Firefox, RVM, NVM, etc.),
packaged our modified instance into its own AMI, and then set up the EC2 Plugin to launch instances from this
custom AMI.</p>

<p>Our AMI setup steps are captured entirely in a <a href="https://gist.github.com/3085368">GitHub gist</a>, but because our
build requirements are specific to our applications and frameworks, most organizations will need to modify these
scripts to their own use cases. Given that caveat, here’s how we went from base Ubuntu AMI to custom build agent
AMI:</p>

<ol>
  <li>We <a href="https://console.aws.amazon.com/ec2/home?region=us-east-1#launchAmi=ami-4dad7424">launched</a> an Ubuntu 11.10
AMI <code class="language-plaintext highlighter-rouge">4dad7424</code> via the AWS console.</li>
  <li>Once the instance was launched, we logged in with the SSH key we generated during setup.</li>
  <li>
    <p>We ran the following commands to configure the instance:</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl -L https://raw.github.com/gist/3085368/_base-setup.sh | sudo bash -s
sudo su -l jenkins
curl -L https://raw.github.com/gist/3085368/_jenkins-user-setup.sh | bash -s
</code></pre></div>    </div>
  </li>
  <li>From the “Instances” tab of the AWS Console, we chose the now-configured instance, and from the “Instance
Actions” dropdown, selected “Stop”, followed by “Create Image (EBS AMI)”.</li>
</ol>

<p>Next we installed the Amazon EC2 Plugin on our Jenkins controller, and entered the following configuration
arguments for the plugin. (Replace the AMI ID with your own, the result of Step 4 above.)</p>

<p><img src="/images/2012-07-10-on-demand-jenkins-agents-with-amazon-ec2/ec2-plugin-config.png" alt="Jenkins EC2 Plugin
configuration" /></p>

<p>New build agents began spawning immediately in response to job demand! Our new “Computers” page on Jenkins looks
like this:</p>

<p><img src="/images/2012-07-10-on-demand-jenkins-agents-with-amazon-ec2/computer-list.png" alt="Jenkins computer list" /></p>

<p>We have the option of provisioning a new build agent via a single click, but so far, this hasn’t been necessary,
since agents have automatically scaled up and down with demand. We average around 4-8 build agents during the day,
and 0-1 overnight and on weekends.</p>

<h2 id="outcome-and-next-steps">Outcome and Next Steps</h2>

<p>This arrangement hasn’t been in place for long, but we’re excited about the benefits it’s already delivered:</p>

<ul>
  <li>Builds now take a predictable amount of time, since agents automatically scale up to match demand.</li>
  <li>Agents offer a more consistent and easily maintained configuration, so there are fewer spurious failures.</li>
  <li>Despite higher costs on EC2, we hope to spend about the same (or maybe even less) now that we’ll need to operate
only the controller server during periods of inactivity (like nights and weekends).</li>
</ul>

<p>As proponents of <em>automating the hard stuff</em>, we get a real kick out of watching identical agents spin up as builds
trickle in each morning, then disappear as the queue quiets down in the evening. Still, there are a few
improvements to be made:</p>

<ul>
  <li>Our canonical agent’s configuration should be scripted with <a href="http://www.opscode.com/chef/">Chef</a>.</li>
  <li>Sharp-eyed readers will notice that our Jenkins controller is still a Linode server. It might benefit from the
same type of scripted configuration as the agents.</li>
  <li>Cooler still would be for the EC2 plugin to take advantage of Amazon’s
<a href="http://aws.amazon.com/ec2/spot-instances/">spot pricing</a>. Though not supported at the moment, it would allow us
to spend a fraction as much (or spend the same amount, but on more powerful resources).</li>
</ul>

<hr />

<p><em>Editor’s Note: This post has been updated as part of an effort to adopt more inclusive language across Artsy’s
GitHub repositories and editorial content (<a href="https://github.com/artsy/README/issues/427">RFC</a>).</em></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Spend Time With Your Site]]></title>
    <link href="https://artsy.github.io/blog/2012/07/05/spend-time-with-your-site/"/>
    <updated>2012-07-05T10:51:00+00:00</updated>
    <id>https://artsy.github.io/blog/2012/07/05/spend-time-with-your-site</id>
    <content type="html"><![CDATA[<p>Empathy with end users is critical when developing consumer-facing software. Many go <a href="http://innonate.com/2011/03/09/hackers-the-canon-of-consumer-facing-products/">even</a> <a href="http://www.uie.com/articles/self_design/">further</a> and argue that you should <em>be</em> your own user to effectively deliver the best experience.</p>

<blockquote>
  <p><em>I’d encourage anyone starting a startup to become one of its users, however unnatural it seems.</em></p>

  <p>— Paul Graham <a href="http://paulgraham.com/organic.html">Organic Startup Ideas</a></p>
</blockquote>

<p>In practice, though, this can be difficult:</p>

<ul>
  <li>As a developer, you’re just not representative of the intended audience.</li>
  <li>You’re [appropriately] focused on the product’s next iteration, while your audience is occupied with the current state.</li>
  <li>You spend countless hours focused on product details—of course it’s a challenge to empathize with a casual visitor’s first impression.</li>
</ul>

<h2 id="keeping-it-real">Keeping it Real</h2>

<p>We’ve tried some best practices to overcome these tendencies. User feedback is emailed to everyone in the company. Engineers share customer support responsibilities. But one simple tool has been surprisingly useful: we stole a page from the agile development handbook and built an <a href="http://alistair.cockburn.us/Information+radiator">information radiator</a>. Like a <a href="http://en.wikipedia.org/wiki/Kanban_board">kanban board</a>, news ticker, or <a href="https://demo.geckoboard.com/dashboard/B6782E562794C2F2/">analytics wall board</a>, our information radiator gives us an ambient awareness of end users’ experiences. How?</p>

<!-- more -->

<p><strong>It’s our site, as a slideshow.</strong></p>

<p><a href="/images/2012-07-05-spend-time-with-your-site/slideshow_screenshot.jpg">
  <img src="/images/2012-07-05-spend-time-with-your-site/slideshow_screenshot.jpg" alt="Artsy as a slideshow" style="" />
</a></p>

<p>That’s all. Our wall-mounted display shows the same web page that a visitor to our site recently requested. Every 20 seconds, it refreshes and shows a new, more recently requested page.</p>

<p>Without much effort, this gives us a sense of where users spend time on the site (<em>nudes seem popular today</em>). The impact of events such as email blasts or celebrity mentions is immediately apparent (<em>did <a href="https://twitter.com/aplusk">@aplusk</a> just tweet us?</em>). And when problems happen, we notice them as soon as errors pop up on the screen (<em><a href="http://gigaom.com/cloud/some-of-amazon-web-services-are-down-again/">AWS down again?</a></em>).</p>

<p>Of course, this doesn’t replace proper user research, analytics, or monitoring. And the approach might need tweaking to work for your site. The lesson, though, is <em>find a way to spend time with your site</em>.</p>

<h2 id="implementation-notes">Implementation Notes</h2>

<p>Using <a href="https://github.com/matschaffer/knife-solo">knife-solo</a> and <a href="http://www.opscode.com/chef/">chef</a>, we spawned an <a href="http://aws.amazon.com/ec2/">EC2</a> instance and configured it to <a href="https://devcenter.heroku.com/articles/logging#syslog_drains">drain our main site’s logs from heroku</a>. A single, static web page contains a full-screen iframe and a bit of javascript that periodically requests the most recent URL from a tiny <a href="http://www.sinatrarb.com/">sinatra</a> app, loading the resulting URL into the iframe. The sinatra app performs an ugly bash command to grep the last appropriate GET request from the drained log, filtering out requests from Artsy HQ and other uninteresting cases. Via a special flag, our site suppresses the usual tracking and analytics when loaded in this context (you didn’t want to juice your stats, right?).</p>

<p>Have other tricks for keeping it real? Let us know in the comments.</p>

<p><em>Update:</em> See a gist with <a href="https://gist.github.com/3073907">sample code for the slideshow app</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Beyond Heroku: "Satellite" Delayed Job Workers on EC2]]></title>
    <link href="https://artsy.github.io/blog/2012/01/31/beyond-heroku-satellite-delayed-job-workers-on-ec2/"/>
    <updated>2012-01-31T11:45:00+00:00</updated>
    <id>https://artsy.github.io/blog/2012/01/31/beyond-heroku-satellite-delayed-job-workers-on-ec2</id>
    <content type="html"><![CDATA[<p>[TL;DR: To supplement Heroku-managed app servers, we launched custom EC2 instances to host Delayed Job worker processes. See the <a href="https://github.com/joeyAghion/satellite_setup">satellite_setup github repo</a> for rake tasks and Chef recipes that make it easy.]</p>

<p><a href="http://artsy.net">Artsy</a> engineers are big users and abusers of <a href="http://heroku.com">Heroku</a>. It’s a neat abstraction of server resources, so we were conflicted when parts of our application started to bump into Heroku’s limitations. While we weren’t eager to start managing additional infrastructure, we found that–with a few good tools–we could migrate some components away from Heroku without fragmenting the codebase or over-complicating our development environments.</p>

<p>There are a number of reasons your app might need to go beyond Heroku. It might rely on a locally installed tool (not possible on Heroku’s locked-down servers), or require heavy file-system usage (limited to <code class="language-plaintext highlighter-rouge">tmp/</code> and <code class="language-plaintext highlighter-rouge">log/</code>, and not permanent or shared). In our case, the culprit was Heroku’s 512 MB RAM limit–reasonable for most web processes, but quickly exceeded by the image-processing tasks of our <a href="https://github.com/collectiveidea/delayed_job">delayed_job</a> workers. We considered building a specialized image-processing service, but decided instead to supplement our web apps with a custom <a href="http://aws.amazon.com/ec2/">EC2</a> instance dedicated to processing background tasks. We call these servers “satellites.”</p>

<p>We’ll walk through the pertinent sections here, but you can find Rake tasks that correspond with these scripts, plus all of the necessary cookbooks, in the <a href="https://github.com/joeyAghion/satellite_setup">satellite_setup github repo</a>. Now, on to the code!</p>

<p>First, generate a key-pair from <a href="https://console.aws.amazon.com/ec2/home?#s=KeyPairs">Amazon’s AWS Management Console</a>. Then we’ll use <a href="http://fog.io">Fog</a> to spawn the EC2 instance.</p>

<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">require</span> <span class="s1">'fog'</span>

<span class="c1"># Update these values according to your environment...</span>
<span class="no">S3_ACCESS_KEY_ID</span> <span class="o">=</span> <span class="s1">'XXXXXXXXXXXXXXXXXXXX'</span>
<span class="no">S3_SECRET_ACCESS_KEY</span> <span class="o">=</span> <span class="s1">'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'</span>
<span class="no">KEY_NAME</span> <span class="o">=</span> <span class="s1">'satellite_keypair'</span>
<span class="no">KEY_PATH</span> <span class="o">=</span> <span class="s2">"</span><span class="si">#{</span><span class="no">ENV</span><span class="p">[</span><span class="s1">'HOME'</span><span class="p">]</span><span class="si">}</span><span class="s2">/.ssh/</span><span class="si">#{</span><span class="no">KEY_NAME</span><span class="si">}</span><span class="s2">.pem"</span>
<span class="no">IMAGE_ID</span> <span class="o">=</span> <span class="s1">'ami-c162a9a8'</span>  <span class="c1"># 64-bit Ubuntu 11.10</span>
<span class="no">FLAVOR_ID</span> <span class="o">=</span> <span class="s1">'m1.large'</span>

<span class="n">connection</span> <span class="o">=</span> <span class="no">Fog</span><span class="o">::</span><span class="no">Compute</span><span class="p">.</span><span class="nf">new</span><span class="p">(</span><span class="ss">provider: </span><span class="s1">'AWS'</span><span class="p">,</span>
  <span class="ss">aws_access_key_id: </span><span class="no">S3_ACCESS_KEY_ID</span><span class="p">,</span>
  <span class="ss">aws_secret_access_key: </span><span class="no">S3_SECRET_ACCESS_KEY</span><span class="p">)</span>

<span class="n">server</span> <span class="o">=</span> <span class="n">connection</span><span class="p">.</span><span class="nf">servers</span><span class="p">.</span><span class="nf">bootstrap</span><span class="p">(</span>
  <span class="ss">key_name: </span><span class="no">KEY_NAME</span><span class="p">,</span>
  <span class="ss">private_key_path: </span><span class="no">KEY_PATH</span><span class="p">,</span>
  <span class="ss">image_id: </span><span class="no">IMAGE_ID</span><span class="p">,</span>
  <span class="ss">flavor_id: </span><span class="no">FLAVOR_ID</span><span class="p">)</span>
</code></pre></div></div>

<p>Next, we’ll do some basic server prep and install our preferred Ruby version.<!-- more --> We’ll again use Fog, this time to SSH into the new instance and run the following commands:</p>

<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">[</span> <span class="sx">%{sudo sh -c "grep '^[^#].*us-east-1\.ec2' /etc/apt/sources.list | sed 's/us-east-1\.ec2/us/g' &gt; /etc/apt/sources.list.d/better.list"}</span><span class="p">,</span>
  <span class="sx">%{sudo apt-get update; sudo apt-get install -y make gcc rsync curl zlib1g-dev libssl-dev libreadline-dev libreadline5}</span><span class="p">,</span>
  <span class="sx">%{mkdir -p /tmp/bootstrap}</span><span class="p">,</span>
  <span class="sx">%{cd /tmp/bootstrap &amp;&amp; curl -L 'ftp://ftp.ruby-lang.org/pub/ruby/1.9/ruby-1.9.2-p290.tar.gz' | tar xvzf - &amp;&amp; cd ruby-1.9.2-p290 &amp;&amp; ./configure &amp;&amp; make &amp;&amp; sudo make install}</span><span class="p">,</span>
  <span class="sx">%{cd /tmp/bootstrap &amp;&amp; curl -L 'http://production.cf.rubygems.org/rubygems/rubygems-1.6.2.tgz' | tar xvzf - &amp;&amp; cd rubygems* &amp;&amp; sudo ruby setup.rb --no-ri --no-rdoc}</span><span class="p">,</span>
  <span class="sx">%{sudo gem install rdoc chef ohai --no-ri --no-rdoc --source http://gems.rubyforge.org}</span>
<span class="p">].</span><span class="nf">each</span> <span class="k">do</span> <span class="o">|</span><span class="n">command</span><span class="o">|</span>
  <span class="n">server</span><span class="p">.</span><span class="nf">ssh</span> <span class="n">command</span>
<span class="k">end</span>
</code></pre></div></div>

<p>The first command replaces some broken links in the AMI’s <code class="language-plaintext highlighter-rouge">/etc/apt/sources.list</code>, so that the later package installation commands have a chance. We then install a few necessary packages, download and install Ruby 1.9.2 from source, and install RubyGems. Finally, we install <a href="http://www.opscode.com/chef/">Chef</a> so we can let <code class="language-plaintext highlighter-rouge">chef-solo</code> drive the remainder of the configuration (and manage it going forward).</p>

<p>Chef deserves a few blog posts of its own, but know that it provides a platform-independent DSL for specifying an environment’s configuration. <a href="http://www.opscode.com">Opscode</a> supplies a deep set of related tools, but for our purposes, <code class="language-plaintext highlighter-rouge">chef-solo</code>–which applies a local set of configuration “cookbooks” to an individual server–will be plenty. These lines copy our local cookbooks folder (originally in <code class="language-plaintext highlighter-rouge">config/satellite</code>) up to a temporary location on the server, then execute the <code class="language-plaintext highlighter-rouge">chef-solo</code> command via SSH:</p>

<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">system</span> <span class="s2">"rsync -rlP --rsh=</span><span class="se">\"</span><span class="s2">ssh -i </span><span class="si">#{</span><span class="no">KEY_PATH</span><span class="si">}</span><span class="se">\"</span><span class="s2"> --delete --exclude '.*' ./config/satellite ubuntu@</span><span class="si">#{</span><span class="n">server</span><span class="p">.</span><span class="nf">dns_name</span><span class="si">}</span><span class="s2">:/tmp/chef/"</span>
<span class="n">server</span><span class="p">.</span><span class="nf">ssh</span> <span class="s2">"cd /tmp/chef/satellite; RAILS_ENV=staging sudo -H -E chef-solo -c solo.rb -j configure.json -l info"</span>
</code></pre></div></div>

<p>The first file referenced in that command simply declares where cookbooks will be found. In our case, they’re stored alongside <code class="language-plaintext highlighter-rouge">solo.rb</code> in the temporary directory.</p>

<p>```ruby config/satellite/solo.rb
cookbook_path File.expand_path(File.join(File.dirname(<strong>FILE</strong>), “cookbooks”))</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
The next file, `configure.json`, specifies that Chef should apply the `configure` recipe found in the `example_app` cookbook.

```javascript config/satellite/configure.json
{"recipes": ["example_app::configure"]}
</code></pre></div></div>

<p>Finally, let’s look at the <code class="language-plaintext highlighter-rouge">example_app::configure</code> recipe. First it will make sure necessary packages and gems are installed. These, of course, might be different for your app.</p>

<p>```ruby config/satellite/cookbooks/example_app/recipes/configure.rb
[ ‘build-essential’,
  ‘binutils-doc’,
  ‘autoconf’,
  ‘flex’,
  ‘bison’,
  ‘openssl’,
  ‘libreadline5’,
  ‘libreadline-dev’,
  ‘git-core’,
  ‘zlib1g’,
  ‘zlib1g-dev’,
  ‘libssl-dev’,
  ‘libxml2-dev’,
  ‘autoconf’,
  ‘libxslt-dev’,
  ‘imagemagick’,
  ‘libmagick9-dev’
].each do |pkg|
  package(pkg)
end</p>

<p>gem_package ‘bundler’ do
  version ‘1.0.10’
end</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
Next, it will ensure that a user and group (named for `example_app`) are created and configured:

``` ruby config/satellite/cookbooks/example_app/recipes/configure.rb (cont'd)
group 'example_app'

user 'example_app' do
  gid 'example_app'
  home '/home/example_app'
  shell '/bin/bash'
  supports :manage_home =&gt; true
end

directory '/home/example_app/.ssh' do
  owner 'example_app'
  group 'example_app'
  mode 0700
end

# Ensure example_app can sudo
cookbook_file '/etc/sudoers.d/example_app' do
  mode 0440
end

[ 'authorized_keys',  # place the keys you want to authorize for SSH in this file
  'id_dsa',  # a new private key file, authorized to pull from your git repo
   'config'  # avoid prompt when pulling from github
].each do |config_file|
  cookbook_file "/home/example_app/.ssh/#{config_file}" do
    owner 'example_app'
    group 'example_app'
    mode 0600
  end
end

# Allow other developers to SSH as primary ubuntu account as well.
cookbook_file "/home/ubuntu/.ssh/authorized_keys" do
  owner 'ubuntu'
  group 'ubuntu'
  mode 0600
end
</code></pre></div></div>

<p>Next, it creates supporting directories.</p>

<p>``` ruby config/satellite/cookbooks/example_app/recipes/configure.rb (cont’d)
[ ‘/app’,
  ‘/app/example_app’,
  ‘/app/example_app/shared’,
  ‘/app/example_app/shared/log’,
  ‘/app/example_app/shared/pids’,
  ‘/app/example_app/shared/config’
].each do |dir|
  directory dir do
    owner ‘example_app’
    group ‘example_app’
    mode 0755
  end
end</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
We rely heavily on the [New Relic](http://newrelic.com/) plug-in, and want to enable it in our custom environment as well. Heroku usually takes care of injecting a `newrelic.yml` configuration file into our app servers, so we'll have to replicate that in our custom environment. Depending on what plug-ins you've enabled (e.g., [Sendgrid](http://sendgrid.com)), you might need to replicate other configuration files or initializers.

``` ruby config/satellite/cookbooks/example_app/recipes/configure.rb (cont'd)
cookbook_file "/app/example_app/shared/config/newrelic.yml" do
  owner 'example_app'
  group 'example_app'
  mode 0755
end
</code></pre></div></div>

<p><a href="http://mmonit.com/monit/">Monit</a> is a great tool for starting, managing, and monitoring long-running processes. It can be configured with all sorts of thresholds and alerting. (We’ve included only a simple configuration in the <a href="https://github.com/joeyAghion/satellite_setup">github repo</a> for now.) Let’s include the <code class="language-plaintext highlighter-rouge">monit</code> recipe, to ensure that monit is installed and running, and then add the necessary configuration for monit to start and monitor our delayed_job worker process.</p>

<p>``` ruby config/satellite/cookbooks/example_app/recipes/configure.rb (cont’d)
include_recipe ‘monit’
monitrc ‘delayed_job’, {}, :immediately</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
To keep disk space from becoming a problem, we'll automatically rotate all of the logs in our app's shared `log/` directory.

``` ruby config/satellite/cookbooks/example_app/recipes/configure.rb (cont'd)
logrotate_app "example_app" do
  path "/app/example_app/shared/log/*.log"
  frequency "daily"
  rotate 30
end

include_recipe 'example_app::deploy'
</code></pre></div></div>

<p>That last line loads up the neighboring <code class="language-plaintext highlighter-rouge">deploy.rb</code> recipe, which is responsible for checking out the appropriate version of the codebase to the server and [re]starting the delayed_job worker:</p>

<p>```ruby config/satellite/cookbooks/example_app/deploy.rb
deploy “/app/example_app” do</p>

<p>repo “git@github.com:my_org/example_app.git”  # update this!
  branch node[‘rails_env’]  # assumes a branch named for this RAILS_ENV (e.g., staging, production)
  shallow_clone true
  environment node[‘rails_env’]
  symlinks ‘pids’ =&gt; ‘tmp/pids’, ‘log’ =&gt; ‘log’, ‘config/newrelic.yml’ =&gt; ‘config/newrelic.yml’
  before_restart do
    current_release = release_path
    execute(“bundle install –without development test”) do
      cwd current_release
      user ‘example_app’
      group ‘example_app’
      environment ‘HOME’ =&gt; ‘/home/example_app’
    end
  end
  restart_command { execute “monit restart delayed_job” }
  user ‘example_app’
  group ‘example_app’</p>

<p>end</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
The `deploy` Chef command creates a new timestamped directory under `/app/example_app/releases` and cleans up any older release directories, leaving the most recent 5 (much in the style of [Capistrano](https://github.com/capistrano/capistrano)). It also symlinks necessary directories and files and restarts the delayed_job worker.

Did you notice how we ensure that Heroku and the satellite use the same version of the codebase? We've formalized `staging` and `production` branches and update them with the commits we intend to deploy. Of course, we can't simply `git push heroku master` anymore. Instead, we push to the appropriate branch, then push that to heroku and re-run the satellite's `deploy` recipe. Here, we've wrapped the process up into a single `deploy:[staging|production]` rake task. (The precise branches might vary depending on your development workflow.)

``` ruby lib/tasks/deploy.rake
namespace :deploy do
  desc 'Merge master into a staging branch and deploy it to example_app-staging.'
  task :staging =&gt; 'git:tag:staging' do
    puts `git push git@heroku.com:example_app-staging.git origin/staging:master`
    ENV['RAILS_ENV'] = 'staging'
    task('satellite:deploy').invoke
  end

  desc 'Merge staging into a production branch and deploy it to example_app-production.'
  task :production =&gt; 'git:tag:production' do
    puts `git push git@heroku.com:example_app-production.git origin/production:master`
    ENV['RAILS_ENV'] = 'production'
    task('satellite:deploy').invoke
  end
end

namespace :git do
  task :tag, [:from, :to] =&gt; :confirm_clean do |t, args|
    raise "Must specify 'from' and 'to' branches" unless args[:from] &amp;&amp; args[:to]
    `git fetch`
    `git branch -f #{args[:to]} origin/#{args[:to]}`
    `git checkout #{args[:to]}`
    `git reset --hard origin/#{args[:from]}`
    puts "Updating #{args[:to]} with these commits:"
    puts `git log origin/#{args[:to]}..#{args[:to]} --format=format:"%h  %s  (%an)"`
    `git push -f origin #{args[:to]}`
    `git checkout master`
    `git branch -D #{args[:to]}`
  end

  task :confirm_clean do
    raise "Must have clean working directory" unless `git status` =~ /working directory clean/
  end

  namespace :tag do
    task :staging do
      task('git:tag').invoke('master',  'staging')
    end
    task :production do
      task('git:tag').invoke('staging', 'production')
    end
  end
end
</code></pre></div></div>

<p>Our satellite requires access to some of the same environment variables as our Heroku web apps (such as a database host or mail server credentials). To keep these synchronized, we rely on a <code class="language-plaintext highlighter-rouge">config/heroku.yml</code> file. (This duplication is an obvious hazard and deserves improvement, but we’ve actually found it convenient to have these locally for easy access from Rake tasks, etc.)</p>

<p>In the <a href="https://github.com/joeyAghion/satellite_setup">satellite_setup</a> github repo, we’ve simplified this set-up into a few tasks that we use on an ongoing basis:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>rake satellite:spawn <span class="nv">RAILS_ENV</span><span class="o">=</span>staging
<span class="nv">$ </span>rake satellite:configure <span class="nv">RAILS_ENV</span><span class="o">=</span>staging
<span class="nv">$ </span>rake satellite:info <span class="nv">RAILS_ENV</span><span class="o">=</span>staging
	1 satellite server<span class="o">(</span>s<span class="o">)</span>
		state: running <span class="o">(</span>1<span class="o">)</span>
			i-f4583196	staging	ec2-170-179-113-159.compute-1.amazonaws.com	2012-01-03 14:48:11 UTC
<span class="nv">$ </span>rake satellite:deploy <span class="nv">RAILS_ENV</span><span class="o">=</span>staging
<span class="nv">$ </span>rake satellite:destroy <span class="nv">RAILS_ENV</span><span class="o">=</span>staging
</code></pre></div></div>

<p>This arrangement has worked for us so far. Satellite servers cost a little more, but can benefit from arbitrary customization and are served out of the same data-centers as our Heroku-based web apps and S3 storage. Since the same app is running transparently in 2 different environments, our developers’ workflow has hardly needed modification. In fact, the portability enforced by Heroku’s design (elaborated in <a href="http://12factor.net">The Twelve-Factor App</a>) made this transition relatively straightforward.</p>

<p>Some worthy enhancements might be:</p>

<ul>
  <li>Improving monitoring and notifications</li>
  <li>Extending the recipes to manage multiple parallel workers</li>
  <li>Using Chef attributes to replace uses of <code class="language-plaintext highlighter-rouge">example_app</code> with a parameter</li>
  <li>Cleaning up the duplication of Heroku configuration values in <code class="language-plaintext highlighter-rouge">heroku.yml</code></li>
</ul>
]]></content>
  </entry>
  
</feed>
