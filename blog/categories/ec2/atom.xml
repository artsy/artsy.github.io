<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: ec2 | Artsy Engineering]]></title>
  <link href="http://artsy.github.io/blog/categories/ec2/atom.xml" rel="self"/>
  <link href="http://artsy.github.io/"/>
  <updated>2018-12-16T10:16:31+00:00</updated>
  <id>http://artsy.github.io/</id>
  <author>
    <name><![CDATA[]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[On-Demand Jenkins Slaves with Amazon EC2]]></title>
    <link href="http://artsy.github.io/blog/2012/07/10/on-demand-jenkins-slaves-with-amazon-ec2/"/>
    <updated>2012-07-10T13:30:00+00:00</updated>
    <id>http://artsy.github.io/blog/2012/07/10/on-demand-jenkins-slaves-with-amazon-ec2</id>
    <content type="html"><![CDATA[<p>The <a href="http://artsy.net">Artsy</a> team faithfully uses <a href="http://jenkins-ci.org">Jenkins</a> for continuous integration. <a href="http://artsy.github.com/blog/2012/05/27/using-jenkins-for-ruby-and-ruby-on-rails-teams/">As we've described before</a>, our Jenkins master and 8 slaves run on Linode. This arrangement has at least a few drawbacks:</p>

<ul>
<li>Our Linode servers are manually configured. They require frequent maintenance, and inconsistencies lead to surprising build failures.</li>
<li>The fixed set of slaves don't match the pattern of our build jobs: jobs get backed up during the day, but servers are mostly unused overnight and on weekends.</li>
</ul>


<p>The <a href="https://wiki.jenkins-ci.org/display/JENKINS/Amazon+EC2+Plugin">Amazon EC2 Plugin</a> allowed us to replace those slaves with a totally scripted environment. Now, slaves are spun up in the cloud whenever build jobs need them.</p>

<!-- more -->


<p>To set up the build slave's Amazon Machine Image (AMI), we started from an <a href="http://cloud-images.ubuntu.com/releases/oneiric/release/">official Ubuntu 11.10</a> (Oneiric Ocelot) AMI, ran initialization scripts to set up our build dependencies (MongoDB, Redis, ImageMagick, Firefox, RVM, NVM, etc.), packaged our modified instance into its own AMI, and then set up the EC2 Plugin to launch instances from this custom AMI.</p>

<p>Our AMI setup steps are captured entirely in a <a href="https://gist.github.com/3085368">GitHub gist</a>, but because our build requirements are specific to our applications and frameworks, most organizations will need to modify these scripts to their own use cases. Given that caveat, here's how we went from base Ubuntu AMI to custom build slave AMI:</p>

<ol>
<li>We <a href="https://console.aws.amazon.com/ec2/home?region=us-east-1#launchAmi=ami-4dad7424">launched</a> an Ubuntu 11.10 AMI <code>4dad7424</code> via the AWS console.</li>
<li>Once the instance was launched, we logged in with the SSH key we generated during setup.</li>
<li><p>We ran the following commands to configure the instance:</p>

<pre><code> curl -L https://raw.github.com/gist/3085368/_base-setup.sh | sudo bash -s
 sudo su -l jenkins
 curl -L https://raw.github.com/gist/3085368/_jenkins-user-setup.sh | bash -s
</code></pre></li>
<li><p>From the "Instances" tab of the AWS Console, we chose the now-configured instance, and from the "Instance Actions" dropdown, selected "Stop", followed by "Create Image (EBS AMI)".</p></li>
</ol>


<p>Next we installed the Amazon EC2 Plugin on our Jenkins master, and entered the following configuration arguments for the plugin. (Replace the AMI ID with your own, the result of Step 4 above.)</p>

<p><img src="Jenkins%20EC2%20Plugin%20configuration" alt="/images/2012-07-10-on-demand-jenkins-slaves-with-amazon-ec2/ec2-plugin-config.png" /></p>

<p>New build slaves began spawning immediately in response to job demand! Our new "Computers" page on Jenkins looks like this:</p>

<p><img src="Jenkins%20computer%20list" alt="/images/2012-07-10-on-demand-jenkins-slaves-with-amazon-ec2/computer-list.png" /></p>

<p>We have the option of provisioning a new build slave via a single click, but so far, this hasn't been necessary, since slaves have automatically scaled up and down with demand. We average around 4-8 build slaves during the day, and 0-1 overnight and on weekends.</p>

<a name="Outcome.and.Next.Steps"></a>
<h2>Outcome and Next Steps</h2>

<p>This arrangement hasn't been in place for long, but we're excited about the benefits it's already delivered:</p>

<ul>
<li>Builds now take a predictable amount of time, since slaves automatically scale up to match demand.</li>
<li>Slaves offer a more consistent and easily maintained configuration, so there are fewer spurious failures.</li>
<li>Despite higher costs on EC2, we hope to spend about the same (or maybe even less) now that we'll need to operate only the master server during periods of inactivity (like nights and weekends).</li>
</ul>


<p>As proponents of <em>automating the hard stuff</em>, we get a real kick out of watching identical slaves spin up as builds trickle in each morning, then disappear as the queue quiets down in the evening. Still, there are a few improvements to be made:</p>

<ul>
<li>Our canonical slave's configuration should be scripted with <a href="http://www.opscode.com/chef/">Chef</a>.</li>
<li>Sharp-eyed readers will notice that our Jenkins master is still a Linode server. It might benefit from the same type of scripted configuration as the slaves.</li>
<li>Cooler still would be for the EC2 plugin to take advantage of Amazon's <a href="http://aws.amazon.com/ec2/spot-instances/">spot pricing</a>. Though not supported at the moment, it would allow us to spend a fraction as much (or spend the same amount, but on more powerful resources).</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Spend Time With Your Site]]></title>
    <link href="http://artsy.github.io/blog/2012/07/05/spend-time-with-your-site/"/>
    <updated>2012-07-05T10:51:00+00:00</updated>
    <id>http://artsy.github.io/blog/2012/07/05/spend-time-with-your-site</id>
    <content type="html"><![CDATA[<p>Empathy with end users is critical when developing consumer-facing software. Many go <a href="http://innonate.com/2011/03/09/hackers-the-canon-of-consumer-facing-products/">even</a> <a href="http://www.uie.com/articles/self_design/">further</a> and argue that you should <em>be</em> your own user to effectively deliver the best experience.</p>

<blockquote><p><em>I'd encourage anyone starting a startup to become one of its users, however unnatural it seems.</em></p>

<p>&mdash; Paul Graham <a href="http://paulgraham.com/organic.html">Organic Startup Ideas</a></p></blockquote>

<p>In practice, though, this can be difficult:</p>

<ul>
<li>As a developer, you're just not representative of the intended audience.</li>
<li>You're [appropriately] focused on the product's next iteration, while your audience is occupied with the current state.</li>
<li>You spend countless hours focused on product details&mdash;of course it's a challenge to empathize with a casual visitor's first impression.</li>
</ul>


<a name="Keeping.it.Real"></a>
<h2>Keeping it Real</h2>

<p>We've tried some best practices to overcome these tendencies. User feedback is emailed to everyone in the company. Engineers share customer support responsibilities. But one simple tool has been surprisingly useful: we stole a page from the agile development handbook and built an <a href="http://alistair.cockburn.us/Information+radiator">information radiator</a>. Like a <a href="http://en.wikipedia.org/wiki/Kanban_board">kanban board</a>, news ticker, or <a href="https://demo.geckoboard.com/dashboard/B6782E562794C2F2/">analytics wall board</a>, our information radiator gives us an ambient awareness of end users' experiences. How?</p>

<!-- more -->


<p><strong>It's our site, as a slideshow.</strong></p>

<p></div></div>
<a href='/images/2012-07-05-spend-time-with-your-site/slideshow_screenshot.jpg'>
  <img src="/images/2012-07-05-spend-time-with-your-site/slideshow_screenshot.jpg" alt="Artsy as a slideshow" style="">
</a>
<div class='meta-container'><header>&nbsp;</header></div><div class='content-container'><div class='entry-content'>
</p>

<p>That's all. Our wall-mounted display shows the same web page that a visitor to our site recently requested. Every 20 seconds, it refreshes and shows a new, more recently requested page.</p>

<p>Without much effort, this gives us a sense of where users spend time on the site (<em>nudes seem popular today</em>). The impact of events such as email blasts or celebrity mentions is immediately apparent (<em>did <a href="https://twitter.com/aplusk">@aplusk</a> just tweet us?</em>). And when problems happen, we notice them as soon as errors pop up on the screen (<em><a href="http://gigaom.com/cloud/some-of-amazon-web-services-are-down-again/">AWS down again?</a></em>).</p>

<p>Of course, this doesn't replace proper user research, analytics, or monitoring. And the approach might need tweaking to work for your site. The lesson, though, is <em>find a way to spend time with your site</em>.</p>

<a name="Implementation.Notes"></a>
<h2>Implementation Notes</h2>

<p>Using <a href="https://github.com/matschaffer/knife-solo">knife-solo</a> and <a href="http://www.opscode.com/chef/">chef</a>, we spawned an <a href="http://aws.amazon.com/ec2/">EC2</a> instance and configured it to <a href="https://devcenter.heroku.com/articles/logging#syslog_drains">drain our main site's logs from heroku</a>. A single, static web page contains a full-screen iframe and a bit of javascript that periodically requests the most recent URL from a tiny <a href="http://www.sinatrarb.com/">sinatra</a> app, loading the resulting URL into the iframe. The sinatra app performs an ugly bash command to grep the last appropriate GET request from the drained log, filtering out requests from Artsy HQ and other uninteresting cases. Via a special flag, our site suppresses the usual tracking and analytics when loaded in this context (you didn't want to juice your stats, right?).</p>

<p>Have other tricks for keeping it real? Let us know in the comments.</p>

<p><em>Update:</em> See a gist with <a href="https://gist.github.com/3073907">sample code for the slideshow app</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Beyond Heroku: "Satellite" Delayed Job Workers on EC2]]></title>
    <link href="http://artsy.github.io/blog/2012/01/31/beyond-heroku-satellite-delayed-job-workers-on-ec2/"/>
    <updated>2012-01-31T11:45:00+00:00</updated>
    <id>http://artsy.github.io/blog/2012/01/31/beyond-heroku-satellite-delayed-job-workers-on-ec2</id>
    <content type="html"><![CDATA[<p>[TL;DR: To supplement Heroku-managed app servers, we launched custom EC2 instances to host Delayed Job worker processes. See the <a href="https://github.com/joeyAghion/satellite_setup">satellite_setup github repo</a> for rake tasks and Chef recipes that make it easy.]</p>

<p><a href="http://artsy.net">Artsy</a> engineers are big users and abusers of <a href="http://heroku.com">Heroku</a>. It's a neat abstraction of server resources, so we were conflicted when parts of our application started to bump into Heroku's limitations. While we weren't eager to start managing additional infrastructure, we found that--with a few good tools--we could migrate some components away from Heroku without fragmenting the codebase or over-complicating our development environments.</p>

<p>There are a number of reasons your app might need to go beyond Heroku. It might rely on a locally installed tool (not possible on Heroku's locked-down servers), or require heavy file-system usage (limited to <code>tmp/</code> and <code>log/</code>, and not permanent or shared). In our case, the culprit was Heroku's 512 MB RAM limit--reasonable for most web processes, but quickly exceeded by the image-processing tasks of our <a href="https://github.com/collectiveidea/delayed_job">delayed_job</a> workers. We considered building a specialized image-processing service, but decided instead to supplement our web apps with a custom <a href="http://aws.amazon.com/ec2/">EC2</a> instance dedicated to processing background tasks. We call these servers "satellites."</p>

<p>We'll walk through the pertinent sections here, but you can find Rake tasks that correspond with these scripts, plus all of the necessary cookbooks, in the <a href="https://github.com/joeyAghion/satellite_setup">satellite_setup github repo</a>. Now, on to the code!</p>

<p>First, generate a key-pair from <a href="https://console.aws.amazon.com/ec2/home?#s=KeyPairs">Amazon's AWS Management Console</a>. Then we'll use <a href="http://fog.io">Fog</a> to spawn the EC2 instance.</p>

<pre><code class="ruby">require 'fog'

# Update these values according to your environment...
S3_ACCESS_KEY_ID = 'XXXXXXXXXXXXXXXXXXXX'
S3_SECRET_ACCESS_KEY = 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'
KEY_NAME = 'satellite_keypair'
KEY_PATH = "#{ENV['HOME']}/.ssh/#{KEY_NAME}.pem"
IMAGE_ID = 'ami-c162a9a8'  # 64-bit Ubuntu 11.10
FLAVOR_ID = 'm1.large'

connection = Fog::Compute.new(provider: 'AWS',
  aws_access_key_id: S3_ACCESS_KEY_ID,
  aws_secret_access_key: S3_SECRET_ACCESS_KEY)

server = connection.servers.bootstrap(
  key_name: KEY_NAME,
  private_key_path: KEY_PATH,
  image_id: IMAGE_ID,
  flavor_id: FLAVOR_ID)
</code></pre>

<p>Next, we'll do some basic server prep and install our preferred Ruby version.<!-- more --> We'll again use Fog, this time to SSH into the new instance and run the following commands:</p>

<pre><code class="ruby">[ %{sudo sh -c "grep '^[^#].*us-east-1\.ec2' /etc/apt/sources.list | sed 's/us-east-1\.ec2/us/g' &gt; /etc/apt/sources.list.d/better.list"},
  %{sudo apt-get update; sudo apt-get install -y make gcc rsync curl zlib1g-dev libssl-dev libreadline-dev libreadline5},
  %{mkdir -p /tmp/bootstrap},
  %{cd /tmp/bootstrap &amp;&amp; curl -L 'ftp://ftp.ruby-lang.org/pub/ruby/1.9/ruby-1.9.2-p290.tar.gz' | tar xvzf - &amp;&amp; cd ruby-1.9.2-p290 &amp;&amp; ./configure &amp;&amp; make &amp;&amp; sudo make install},
  %{cd /tmp/bootstrap &amp;&amp; curl -L 'http://production.cf.rubygems.org/rubygems/rubygems-1.6.2.tgz' | tar xvzf - &amp;&amp; cd rubygems* &amp;&amp; sudo ruby setup.rb --no-ri --no-rdoc},
  %{sudo gem install rdoc chef ohai --no-ri --no-rdoc --source http://gems.rubyforge.org}
].each do |command|
  server.ssh command
end
</code></pre>

<p>The first command replaces some broken links in the AMI's <code>/etc/apt/sources.list</code>, so that the later package installation commands have a chance. We then install a few necessary packages, download and install Ruby 1.9.2 from source, and install RubyGems. Finally, we install <a href="http://www.opscode.com/chef/">Chef</a> so we can let <code>chef-solo</code> drive the remainder of the configuration (and manage it going forward).</p>

<p>Chef deserves a few blog posts of its own, but know that it provides a platform-independent DSL for specifying an environment's configuration. <a href="http://www.opscode.com">Opscode</a> supplies a deep set of related tools, but for our purposes, <code>chef-solo</code>--which applies a local set of configuration "cookbooks" to an individual server--will be plenty. These lines copy our local cookbooks folder (originally in <code>config/satellite</code>) up to a temporary location on the server, then execute the <code>chef-solo</code> command via SSH:</p>

<pre><code class="ruby">system "rsync -rlP --rsh=\"ssh -i #{KEY_PATH}\" --delete --exclude '.*' ./config/satellite ubuntu@#{server.dns_name}:/tmp/chef/"
server.ssh "cd /tmp/chef/satellite; RAILS_ENV=staging sudo -H -E chef-solo -c solo.rb -j configure.json -l info"
</code></pre>

<p>The first file referenced in that command simply declares where cookbooks will be found. In our case, they're stored alongside <code>solo.rb</code> in the temporary directory.</p>

<pre><code class="ruby config/satellite/solo.rb">cookbook_path File.expand_path(File.join(File.dirname(__FILE__), "cookbooks"))
</code></pre>

<p>The next file, <code>configure.json</code>, specifies that Chef should apply the <code>configure</code> recipe found in the <code>example_app</code> cookbook.</p>

<pre><code class="javascript config/satellite/configure.json">{"recipes": ["example_app::configure"]}
</code></pre>

<p>Finally, let's look at the <code>example_app::configure</code> recipe. First it will make sure necessary packages and gems are installed. These, of course, might be different for your app.</p>

<pre><code class="ruby config/satellite/cookbooks/example_app/recipes/configure.rb">[ 'build-essential',
  'binutils-doc',
  'autoconf',
  'flex',
  'bison',
  'openssl',
  'libreadline5',
  'libreadline-dev',
  'git-core',
  'zlib1g',
  'zlib1g-dev',
  'libssl-dev',
  'libxml2-dev',
  'autoconf',
  'libxslt-dev',
  'imagemagick',
  'libmagick9-dev'
].each do |pkg|
  package(pkg)
end

gem_package 'bundler' do
  version '1.0.10'
end
</code></pre>

<p>Next, it will ensure that a user and group (named for <code>example_app</code>) are created and configured:</p>

<pre><code class="ruby config/satellite/cookbooks/example_app/recipes/configure.rb (cont'd)">group 'example_app'

user 'example_app' do
  gid 'example_app'
  home '/home/example_app'
  shell '/bin/bash'
  supports :manage_home =&gt; true
end

directory '/home/example_app/.ssh' do
  owner 'example_app'
  group 'example_app'
  mode 0700
end

# Ensure example_app can sudo
cookbook_file '/etc/sudoers.d/example_app' do
  mode 0440
end

[ 'authorized_keys',  # place the keys you want to authorize for SSH in this file
  'id_dsa',  # a new private key file, authorized to pull from your git repo
   'config'  # avoid prompt when pulling from github
].each do |config_file|
  cookbook_file "/home/example_app/.ssh/#{config_file}" do
    owner 'example_app'
    group 'example_app'
    mode 0600
  end
end

# Allow other developers to SSH as primary ubuntu account as well.
cookbook_file "/home/ubuntu/.ssh/authorized_keys" do
  owner 'ubuntu'
  group 'ubuntu'
  mode 0600
end
</code></pre>

<p>Next, it creates supporting directories.</p>

<pre><code class="ruby config/satellite/cookbooks/example_app/recipes/configure.rb (cont'd)">[ '/app',
  '/app/example_app',
  '/app/example_app/shared',
  '/app/example_app/shared/log',
  '/app/example_app/shared/pids',
  '/app/example_app/shared/config'
].each do |dir|
  directory dir do
    owner 'example_app'
    group 'example_app'
    mode 0755
  end
end
</code></pre>

<p>We rely heavily on the <a href="http://newrelic.com/">New Relic</a> plug-in, and want to enable it in our custom environment as well. Heroku usually takes care of injecting a <code>newrelic.yml</code> configuration file into our app servers, so we'll have to replicate that in our custom environment. Depending on what plug-ins you've enabled (e.g., <a href="http://sendgrid.com">Sendgrid</a>), you might need to replicate other configuration files or initializers.</p>

<pre><code class="ruby config/satellite/cookbooks/example_app/recipes/configure.rb (cont'd)">cookbook_file "/app/example_app/shared/config/newrelic.yml" do
  owner 'example_app'
  group 'example_app'
  mode 0755
end
</code></pre>

<p><a href="http://mmonit.com/monit/">Monit</a> is a great tool for starting, managing, and monitoring long-running processes. It can be configured with all sorts of thresholds and alerting. (We've included only a simple configuration in the <a href="https://github.com/joeyAghion/satellite_setup">github repo</a> for now.) Let's include the <code>monit</code> recipe, to ensure that monit is installed and running, and then add the necessary configuration for monit to start and monitor our delayed_job worker process.</p>

<pre><code class="ruby config/satellite/cookbooks/example_app/recipes/configure.rb (cont'd)">include_recipe 'monit'
monitrc 'delayed_job', {}, :immediately
</code></pre>

<p>To keep disk space from becoming a problem, we'll automatically rotate all of the logs in our app's shared <code>log/</code> directory.</p>

<pre><code class="ruby config/satellite/cookbooks/example_app/recipes/configure.rb (cont'd)">logrotate_app "example_app" do
  path "/app/example_app/shared/log/*.log"
  frequency "daily"
  rotate 30
end

include_recipe 'example_app::deploy'
</code></pre>

<p>That last line loads up the neighboring <code>deploy.rb</code> recipe, which is responsible for checking out the appropriate version of the codebase to the server and [re]starting the delayed_job worker:</p>

<pre><code class="ruby config/satellite/cookbooks/example_app/deploy.rb">deploy "/app/example_app" do

  repo "git@github.com:my_org/example_app.git"  # update this!
  branch node['rails_env']  # assumes a branch named for this RAILS_ENV (e.g., staging, production)
  shallow_clone true
  environment node['rails_env']
  symlinks 'pids' =&gt; 'tmp/pids', 'log' =&gt; 'log', 'config/newrelic.yml' =&gt; 'config/newrelic.yml'
  before_restart do
    current_release = release_path
    execute("bundle install --without development test") do
      cwd current_release
      user 'example_app'
      group 'example_app'
      environment 'HOME' =&gt; '/home/example_app'
    end
  end
  restart_command { execute "monit restart delayed_job" }
  user 'example_app'
  group 'example_app'

end
</code></pre>

<p>The <code>deploy</code> Chef command creates a new timestamped directory under <code>/app/example_app/releases</code> and cleans up any older release directories, leaving the most recent 5 (much in the style of <a href="https://github.com/capistrano/capistrano">Capistrano</a>). It also symlinks necessary directories and files and restarts the delayed_job worker.</p>

<p>Did you notice how we ensure that Heroku and the satellite use the same version of the codebase? We've formalized <code>staging</code> and <code>production</code> branches and update them with the commits we intend to deploy. Of course, we can't simply <code>git push heroku master</code> anymore. Instead, we push to the appropriate branch, then push that to heroku and re-run the satellite's <code>deploy</code> recipe. Here, we've wrapped the process up into a single <code>deploy:[staging|production]</code> rake task. (The precise branches might vary depending on your development workflow.)</p>

<pre><code class="ruby lib/tasks/deploy.rake">namespace :deploy do
  desc 'Merge master into a staging branch and deploy it to example_app-staging.'
  task :staging =&gt; 'git:tag:staging' do
    puts `git push git@heroku.com:example_app-staging.git origin/staging:master`
    ENV['RAILS_ENV'] = 'staging'
    task('satellite:deploy').invoke
  end

  desc 'Merge staging into a production branch and deploy it to example_app-production.'
  task :production =&gt; 'git:tag:production' do
    puts `git push git@heroku.com:example_app-production.git origin/production:master`
    ENV['RAILS_ENV'] = 'production'
    task('satellite:deploy').invoke
  end
end

namespace :git do
  task :tag, [:from, :to] =&gt; :confirm_clean do |t, args|
    raise "Must specify 'from' and 'to' branches" unless args[:from] &amp;&amp; args[:to]
    `git fetch`
    `git branch -f #{args[:to]} origin/#{args[:to]}`
    `git checkout #{args[:to]}`
    `git reset --hard origin/#{args[:from]}`
    puts "Updating #{args[:to]} with these commits:"
    puts `git log origin/#{args[:to]}..#{args[:to]} --format=format:"%h  %s  (%an)"`
    `git push -f origin #{args[:to]}`
    `git checkout master`
    `git branch -D #{args[:to]}`
  end

  task :confirm_clean do
    raise "Must have clean working directory" unless `git status` =~ /working directory clean/
  end

  namespace :tag do
    task :staging do
      task('git:tag').invoke('master',  'staging')
    end
    task :production do
      task('git:tag').invoke('staging', 'production')
    end
  end
end
</code></pre>

<p>Our satellite requires access to some of the same environment variables as our Heroku web apps (such as a database host or mail server credentials). To keep these synchronized, we rely on a <code>config/heroku.yml</code> file. (This duplication is an obvious hazard and deserves improvement, but we've actually found it convenient to have these locally for easy access from Rake tasks, etc.)</p>

<p>In the <a href="https://github.com/joeyAghion/satellite_setup">satellite_setup</a> github repo, we've simplified this set-up into a few tasks that we use on an ongoing basis:
<code>bash
$ rake satellite:spawn RAILS_ENV=staging
$ rake satellite:configure RAILS_ENV=staging
$ rake satellite:info RAILS_ENV=staging
    1 satellite server(s)
        state: running (1)
            i-f4583196  staging ec2-170-179-113-159.compute-1.amazonaws.com 2012-01-03 14:48:11 UTC
$ rake satellite:deploy RAILS_ENV=staging
$ rake satellite:destroy RAILS_ENV=staging
</code></p>

<p>This arrangement has worked for us so far. Satellite servers cost a little more, but can benefit from arbitrary customization and are served out of the same data-centers as our Heroku-based web apps and S3 storage. Since the same app is running transparently in 2 different environments, our developers' workflow has hardly needed modification. In fact, the portability enforced by Heroku's design (elaborated in <a href="http://12factor.net">The Twelve-Factor App</a>) made this transition relatively straightforward.</p>

<p>Some worthy enhancements might be:</p>

<ul>
<li>Improving monitoring and notifications</li>
<li>Extending the recipes to manage multiple parallel workers</li>
<li>Using Chef attributes to replace uses of <code>example_app</code> with a parameter</li>
<li>Cleaning up the duplication of Heroku configuration values in <code>heroku.yml</code></li>
</ul>

]]></content>
  </entry>
  
</feed>
