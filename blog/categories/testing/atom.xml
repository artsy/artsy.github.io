<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: testing | Artsy Engineering]]></title>
  <link href="https://artsy.github.io/blog/categories/testing/atom.xml" rel="self"/>
  <link href="https://artsy.github.io/"/>
  <updated>2021-09-13T13:47:31-05:00</updated>
  <id>https://artsy.github.io/</id>
  <author>
    <name><![CDATA[]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[What JavaScript Tests Could Learn From RSpec]]></title>
    <link href="https://artsy.github.io/blog/2021/09/10/what-javascript-tests-could-learn-from-rspec/"/>
    <updated>2021-09-10T00:00:00-05:00</updated>
    <id>https://artsy.github.io/blog/2021/09/10/what-javascript-tests-could-learn-from-rspec</id>
    <content type="html"><![CDATA[<p>When I started at Artsy a few years ago, I'd never written a line of Ruby. I feel at home with JavaScript ‚Äî it's
been my buddy since I started my career over 20 years ago. I've written enough tests in JavaScript that I sometimes
feel like I can write them in my sleep (as long as they don't involve async React events üòÖ).</p>

<p>Most of the code I write at Artsy is still JavaScript, but now I write some Ruby code too, and I've written enough
RSpec tests that I'm starting to form opinions about what I think they should look like.</p>

<p>My most recent work has been JavaScript again. I've been writing Jest tests against one of our React apps. But
rather than reaching for the testing patterns I'd become accustomed to over my years of JavaScripting, I'm finding
that something's missing in my Jest tests! My experiences with RSpec have me longing for two features in Jest:</p>

<!-- more -->


<ol>
<li><code>context</code> blocks</li>
<li><code>let</code> blocks</li>
</ol>


<a name="L1...code.context..code..blocks"></a>
<h2>1. <code>context</code> blocks</h2>

<p>A <a href="https://relishapp.com/rspec/rspec-core/v/2-11/docs/example-groups/basic-structure-describe-it"><code>context</code> block</a>
in an RSpec test is, as I understand it, literally the same thing as a <code>describe</code> block. Like it's just an alias of
<code>describe</code>. What's the point, you ask?</p>

<p>The difference is in, well, context. Well-organized RSpec tests use <code>describe</code> to describe what's being
tested...and <code>context</code> to describe scenarios of the thing being tested.</p>

<p>For example, if I wanted to test the <code>multiply</code> method of a <code>Calculator</code> class, I might write some test scenarios
that look like this:</p>

<pre><code class="rb">describe "Calculator" do
  describe ".multiply" do
    context "when the first value is negative" do
      context "when the second value is negative" do
        it "returns a positive number" do
        end
      end
      context "when the second value is positive" do
        it "returns a negative number" do
        end
      end
    end
  end
end
</code></pre>

<p>See the difference in those test cases between <code>describe</code> and <code>context</code>? The way I think about it is: if the
statement coming after my <code>describe</code>/<code>context</code> describes a pre-condition for the test, it's a <code>context</code>; otherwise
it's a <code>describe</code>.</p>

<p><code>context</code> wouldn't be hard to implement in JavaScript ‚Äî I'd bet there are test frameworks that have it. It'd just
be an alias of <code>describe</code>.</p>

<a name="L2...code.let..code..blocks"></a>
<h2>2. <code>let</code> blocks</h2>

<p><a href="https://relishapp.com/rspec/rspec-core/v/2-11/docs/helper-methods/let-and-let"><code>let</code> blocks</a> are used in an RSpec
test to set things up for your test scenario.</p>

<p>Here's a test for a <code>Counter</code> class, verifying that when I call the <code>increment</code> method on an instance, its stored
value becomes <code>1</code>.</p>

<pre><code class="rb">describe "counter" do

  let(:counter) { Counter.new }

  describe "increment" do
    it "increments by 1" do
      counter.increment

      counter.value.should eq(1)
    end
  end
end
</code></pre>

<p>If you're new to Ruby, the only line that doesn't translate almost directly to a similar JavaScript expression is
the <code>let</code> statement.</p>

<p>The <code>let</code> statement in RSpec <a href="https://medium.com/@tomkadwill/all-about-rspec-let-a3b642e08d39">creates a method with a specified name, which lazily evaluates to the result of a
block</a>. In this case, we get a method named <code>counter</code>, which is evaluated to a new instance of the
<code>Counter</code> class. There are a few important things to note about <code>let</code> blocks:</p>

<ol>
<li>They're evaluated lazily (by default). That <code>counter</code> doesn't actually get created until I reference it.</li>
<li>They're memoized. Wherever I reference <code>counter</code> within that <code>describe "counter"</code> block, I'm getting the same
instance. It's initialized to whatever I return inside the <code>let</code> block.</li>
<li>I can override a <code>let</code> block deeper inside the tree of tests, by declaring another <code>let(:counter)</code> later. When I
do this, the closest <code>let</code> block in the tree for that thing is the one that gets used.</li>
</ol>


<p>I don't think it's possible to implement <code>let</code> in JavaScript ‚Äî at least not in the way it exists in RSpec. It
relies on
<a href="https://www.leighhalliday.com/ruby-metaprogramming-method-missing">Ruby meta-programming to intercept calls to missing methods</a>,
which just doesn't exist in JavaScript. The <a href="https://github.com/enova/givens">givens</a> library does something pretty
close, but it relies on string keys to define things, and there's a bit of extra work when working with TypeScript.</p>

<a name="What.s.the.big.deal."></a>
<h2>What's the big deal?</h2>

<p>On the surface these two features don't seem like much, but they provide a really powerful framework for organizing
test cases and the associated test setup.</p>

<p>With the <code>let</code> blocks being lazily evaluated, and override-able, I can set up data at the exact level of tests that
I need it. When I need to override it for a certain set of tests, I can put another <code>let</code> block in that set. In
JavaScript I can define functions to set up my data for me with just the changes I need at each test level, or I
can use <code>beforeEach</code> blocks, but all that can get pretty noisy.</p>

<p>And with <code>context</code> blocks, I can more clearly lay out the scenarios of my tests. Yes, I <em>could</em> just do this with
more <code>describe</code> blocks in JavaScript, but how often have you found JavaScript tests that actually do this? I've
personally seen/written too many tests to count named something like
<code>it("returns false when the flag is enabled, they're located in the US, but they have brown hair.")</code>. That's three
scenarios rolled into one test name. It works, but being able to nest different <code>context</code> blocks to define my
complex scenarios is much easier to read.</p>

<a name="Examples"></a>
<h2>Examples</h2>

<p>Here's an example of some tests I could write in RSpec with <code>let</code> and <code>context</code>:</p>

<pre><code class="rb">describe "Calculator" do
  let(:calculator) { Calculator.new }

  describe ".multiply" do
    let(:result) { calculator.multiply(first, second) }

    context "when the first value is negative" do
      let(:first) { -1 }

      context "when the second value is negative" do
        let(:second) { -3 }

        it "returns a positive number" do
          result.should eq(3)
        end
      end

      context "when the second value is positive" do
        let(:second) { 3 }

        it "returns a negative number" do
          result.should eq(-3)
        end
      end
    end
  end
end
</code></pre>

<p>HOW COOL IS THAT! Every <code>describe</code>/<code>context</code> block has <em>exactly</em> the setup data it needs defined clearly inside it.
And each block has very little noise to distract you.</p>

<p>Here's what I'd do in JavaScript/Jest to accomplish something similar:</p>

<pre><code class="JavaScript">describe("Calculator", () =&gt; {
  let calculator
  beforeEach(() =&gt; { calculator = new Calculator() })

  describe(".multiply", () =&gt; {
    let first, second
    function getResult() {
      return calculator.multiply(first, second)
    }

    describe("when the first value is negative", () =&gt; {
      beforeEach(() =&gt; { first = -1 })

      describe("when the second value is negative", () =&gt; {
        beforeEach(() =&gt; { second = -3 })

        it("returns a positive number", () =&gt; {
          expect(getResult()).toEqual(3)
        })
      })

      describe("when the second value is positive", () =&gt; {
        beforeEach(() =&gt; { second = 3 })

        it("returns a negative number", () =&gt; {
          expect(getResult()).toEqual(-3)
        })
      })
    })
  })
})
</code></pre>

<p>There's definitely a bit more noise here, especially with the <code>beforeEach</code> and <code>let</code> statements. It's not a <em>lot</em>
more noise, but it is definitely more noise.</p>

<p>In real life I wouldn't expect to find tests like the above JavaScript example. I'd expect to find the tests in
JavaScript looking more like this:</p>

<pre><code class="JavaScript">describe("Calculator", () =&gt; {
  let calculator
  beforeEach(() =&gt; { calculator = new Calculator() })

  describe(".multiply", () =&gt; {
    it("returns a positive number when the first number is negative and the second number is negative", () =&gt; {
      const result = calculator.multiply(-1, -3)

      expect(result).toEqual(3)
    })

    it("returns a negative number when the first number is negative and the second number is positive", () =&gt; {
      const result = calculator.multiply(-1, 3)

      expect(result).toEqual(-3)
    })
  })
})
</code></pre>

<p>You could certainly make the case for this contrived example that <em>this</em> is actually the most readable set of
tests, because there's less code. I would have a hard time arguing. But most real-life tests are more complex than
these examples, with state and side-effects to mock out, and more scenarios and edge cases worth testing. Each test
case here includes multiple conditions, but there are only two permutations represented. Once things get a little
more complicated than these contrived examples, the RSpec tests become the clear winner for me ‚Äî they're easier to
read and manage, with their <code>let</code> and <code>context</code> blocks more discretely describing your test scenarios.</p>

<p>You could also argue that the bigger win here would be breaking scenarios into individual <code>describe</code> blocks in
JavaScript tests, instead of cramming the entire scenario into one long <code>it("...")</code> statement. I wouldn't argue
that either.</p>

<a name="Caveats"></a>
<h2>Caveats</h2>

<p>The day after I wrote this article, a conversation started in the Artsy slack about how confusing <code>let</code> was because
it moved variable initializations far away from where the tests used them.</p>

<p>This makes sense! I think it points to two truths in software development:</p>

<a name="Code.readability.is.subjective"></a>
<h3>Code readability is subjective</h3>

<p>For years I was convinced that practices like small functions or long and descriptive function names were
<em>objectively</em> more readable. I leaned into this, and my code reviews almost always included comments on what I
thought would make the code more readable.</p>

<p>As more people pushed back on my feedback over time, I realized that the feedback I was giving was <em>subjective</em>. I
still like code that uses many short functions wired together, but not everyone finds that more readable! I've
stopped giving readability feedback on PRs, unless I can provide nearly-objective facts or scenarios that point to
a readability improvement.</p>

<p>In this article, I find the RSpec <code>let</code> examples to be much more readable than the JavaScript examples. But you and
your team might not! Maybe the distance between a <code>let</code> block's definition and its method's usage makes it hard for
you to follow the test. That's cool!</p>

<a name="Any.cool.thing.can.be.abused"></a>
<h3>Any cool thing can be abused</h3>

<p>Earlier in this article I linked to <a href="https://medium.com/@tomkadwill/all-about-rspec-let-a3b642e08d39">an article that describes <code>let</code> blocks in more detail</a>. It includes
<a href="https://www.rubydoc.info/github/rspec/rspec-core/RSpec%2FCore%2FMemoizedHelpers%2FClassMethods%3Alet">a warning from the actual <code>let</code> docs</a>:</p>

<blockquote><p>Note: <code>let</code> can enhance readability when used sparingly (1,2, or maybe 3 declarations) in any given example
group, but that can quickly degrade with overuse. YMMV.</p></blockquote>

<p>I've definitely seen code where I had a hard time following a stream of <code>let</code> blocks. The RSpec example I gave
above reads nicely to me ‚Äî but it's probably teetering on the edge of where <code>let</code> usage becomes confusing. I'm
guessing I have a slightly higher tolerance for this particular abstraction than my friends who don't like
it...again pointing to readability being subjective.</p>

<p>Having said all that ‚Äî lately every time I try to write JavaScript tests, I find myself trying (unsuccessfully) to
recreate that RSpec example above. It represents exactly how I want to think about complex test scenarios. Each
level of the tests has exactly the setup that is unique to that level. There's very little distraction or noise at
each <code>context</code> and <code>it</code>. It totally aligns with
<a href="https://www.stevenhicks.me/blog/2018/01/chekhovs-gun-and-better-unit-tests/">my desire to minimize irrelevant test setup</a>.
I'm in ‚ù§Ô∏è ‚ù§Ô∏è ‚ù§Ô∏è ‚ù§Ô∏è ‚ù§Ô∏è.</p>

<blockquote><p><em>This post originally appeared on
<a href="https://www.stevenhicks.me/blog/2021/09/what-javascript-tests-could-learn-from-rspec/">Steve's blog</a>.</em></p></blockquote>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Testing React Tracking with Jest and Enzyme]]></title>
    <link href="https://artsy.github.io/blog/2021/04/15/testing-react-tracking-with-jest-and-enzyme/"/>
    <updated>2021-04-15T00:00:00-05:00</updated>
    <id>https://artsy.github.io/blog/2021/04/15/testing-react-tracking-with-jest-and-enzyme</id>
    <content type="html"><![CDATA[<p>Recently, I needed to test a button that would make an analytics tracking call using
<a href="https://github.com/NYTimes/react-tracking">react-tracking</a> and then navigate to a new page in a callback. This
presented some challenges - I wasn't sure how to create a mocked version of react-tracking that would allow a
callback to be passed.</p>

<p>With some help from fellow Artsy engineers <a href="https://github.com/damassi">Christopher Pappas</a> and
<a href="https://twitter.com/pvinis">Pavlos Vinieratos</a>, I got the tracking and testing to work. Here's how we did it.</p>

<!-- more -->


<a name="A.little.context"></a>
<h1>A little context</h1>

<p>This work took place in Volt, our partner CMS (it's sadly a private repository, but I'll do my best to paste in
relevant code snippets so you're not totally in the dark). Volt has been around for a long time and has had several
different client-side tracking implementations over the years. In this case, I wanted to take the opportunity to
bring Volt up to standard with our other big apps, <a href="https://github.com/artsy/force/">Force</a> and
<a href="https://github.com/artsy/eigen">Eigen</a>. They both use react-tracking and
<a href="https://github.com/artsy/cohesion/">Cohesion</a>, our centralized analytics schema.</p>

<p>Our use-case was a button that would navigate the user to a new page. The button had been implemented in a previous
PR, and now we wanted to make it execute a tracking call before navigating.</p>

<p>We use Segment for tracking, and their tracking setup relies on a JS snippet being available on your pages. That
snippet sets a <code>window.analytics</code> property, which in turn
<a href="https://segment.com/docs/connections/sources/catalog/libraries/website/javascript/#track">has a</a> <code>.track()</code>
method. On a fundamental level, all of our tracking calls boil down to a call to <code>window.analytics.track()</code>. We
pass a list of properties to <code>.track()</code>, Segment receives the event and properties, and the resulting JSON is
stored in our data warehouse.</p>

<a name="Adding.react-tracking"></a>
<h1>Adding react-tracking</h1>

<p>First, there was a bit of setup required to get react-tracking working. The react-tracking package
<a href="https://github.com/NYTimes/react-tracking#custom-optionsdispatch-for-tracking-data">assumes you're using Google Tag Manager by default</a>,
but allows you to override that behavior with a custom <code>dispatch</code> function. In our case, we wrap our React apps in
a <code>&lt;BaseApp&gt;</code> component, so we added a new <code>&lt;TrackerContextCohesion&gt;</code> component with a custom <code>dispatch</code> that would
be available to all of our React apps:</p>

<pre><code class="ts">import React from "react"
import { track } from "react-tracking"

// We're following the instructions from react-tracking's README on overriding the dispatch function
const TrackerContextCohesion = track(
  // This is blank because we're not actually tracking a specific event here, just modifying dispatch
  // so that all components in the tree can use it
  {},
  {
    dispatch: ({ data, options, callback }) =&gt; {
      trackEvent(window.analytics.track(data.action, data, options, callback))
    },
  }
)((props) =&gt; {
  return props.children
})

export const BaseApp = (props) =&gt; {
  return &lt;TrackerContextCohesion&gt;{props.children}&lt;/TrackerContextCohesion&gt;
}
</code></pre>

<p>This allows us to make tracking calls in our components, including the passing of custom callback functions:</p>

<pre><code class="ts">import { useTracking } from "react-tracking"
import { Box, Button } from "@artsy/palette"

// Assumes that MyComponent is wrapped in &lt;BaseApp&gt; wherever it's used, giving
// it access to tracking context
const MyComponent = () =&gt; {
  const { trackEvent } = useTracking()

  const handleClick = () =&gt; {
    trackEvent({
      data: { action: "Click", somePropToTrack: "andy-warhol" },
      options: {}, // Additional Segment options, e.g. integrations: { 'Intercom': false, }
      callback: () =&gt; {
        window.location.assign("/artworks")
      },
    })
  }

  return (
    &lt;Box&gt;
      &lt;Button onClick={handleClick}&gt;Track and navigate&lt;/Button&gt;
    &lt;/Box&gt;
  )
}
</code></pre>

<p>Being able to pass a callback was especially important in our case. We realized that if we needed to track <em>and
then navigate</em>, the callback was necessary. In our testing, we saw that if we simply tried to fire the tracking
call then run <code>window.location.assign()</code> synchronously, the tracking call might not get executed before the
navigation started, so we would effectively lose that event. Segment specifically
<a href="https://segment.com/docs/connections/sources/catalog/libraries/website/javascript/#track">allows you to pass a callback</a>
to their tracking function to their track function for this situation. They describe the optional <code>callback</code>
parameter as:</p>

<blockquote><p>A function that is executed after a short timeout, giving the browser time to make outbound requests first.</p></blockquote>

<p>Thus, we pass the tracking data and the callback to the custom <code>track</code> call we implemented, and we're good to go.</p>

<a name="The.problem.with.testing"></a>
<h1>The problem with testing</h1>

<p>Our use-case is simple enough, but we wanted to make sure that when the button was pressed, we would both execute
the tracking call and then navigate. A test checking that the navigation worked had already been implemented.
However, after moving the <code>window.location.assign</code> call into a callback, our test started failing because our
component was trying to execute a tracking call before navigating.</p>

<p>The test that predated the addition of tracking looked like this:</p>

<pre><code class="ts">import React from "react"
import { mount } from "enzyme"
import { MyComponent } from "./MyComponent"
import { TestApp } from "testing/components/TestApp"

window.location.assign = jest.fn()

it("Navigates to artworks page", () =&gt; {
  const wrapper = mount(
    &lt;TestApp&gt;
      &lt;MyComponent /&gt;
    &lt;/TestApp&gt;
  )

  wrapper.find("Button").simulate("click")
  expect(window.location.assign).toBeCalledWith("/artworks")
})
</code></pre>

<p>So we were rendering our button, clicking on it, and expecting to try to navigate. How could we mock our tracking
call while still executing a passed callback?</p>

<a name="The.final.solution"></a>
<h1>The final solution</h1>

<p>Our mock ended up looking like this:</p>

<pre><code class="ts">window.analytics = {
  track: jest.fn(),
}

jest.mock("react-tracking", () =&gt; ({
  useTracking: jest.fn(),
  track: () =&gt; (children) =&gt; children,
}))

const trackEvent = jest.fn((args) =&gt; {
  args.callback()
})

useTracking.mockImplementation(() =&gt; ({
  trackEvent,
}))
</code></pre>

<p>Let's break that down section by section. First:</p>

<pre><code class="ts">window.analytics = {
  track: jest.fn(),
}
</code></pre>

<p>As noted above, all of our tracking calls assume <code>window.analytics</code> exists and that it has a <code>.track()</code> method. We
started by mocking that setup.</p>

<p>Next:</p>

<pre><code class="ts">jest.mock("react-tracking", () =&gt; ({
  useTracking: jest.fn(),
  track: () =&gt; (children) =&gt; children,
}))
</code></pre>

<p>Here we mock the <code>react-tracking</code> package and two specific methods it exports, <code>useTracking</code> and <code>track</code>. We made
<code>useTracking</code> a Jest function - we'll flesh it out further a few lines farther down in the file.</p>

<p>Then there's the mocking of <code>track</code>. To put it in words, our mock is: a function that returns a function that takes
in <code>children</code> and returns those <code>children</code>. That might sound like gibberish at first blush, but essentially what
we're doing is mocking the function composition we performed earlier when creating <code>TrackerContextCohesion</code>. We
needed something that was the same shape as <code>react-tracking</code>'s <code>track()</code>, but we don't care about overriding
<code>dispatch</code> in our mocks.</p>

<p>Last:</p>

<pre><code class="ts">const trackEvent = jest.fn((args) =&gt; {
  args.callback()
})
useTracking.mockImplementation(() =&gt; ({
  trackEvent,
}))
</code></pre>

<p><code>trackEvent</code> is a mock function that takes in an <code>args</code> object and executes <code>args.callback()</code>. We then update our
<code>useTracking</code> mock to make it return a function that returns an object with a <code>trackEvent</code> property. What a
mouthful! That sounds super confusing, but remember that we're trying to mock something that we actually use like
this:</p>

<pre><code class="ts">const { trackEvent } = useTracking()
</code></pre>

<p>So basically, our goal was to mock <code>trackEvent</code> and we needed to emulate the shape it has when it's exported by
<code>react-tracking</code>. Hopefully that makes things a little clearer.</p>

<p>After some tinkering and eventually getting the mocks to work in a single test file, we moved these mocked
functions to a <code>setup.ts</code> file that all of our Jest tests load automatically. We chose to make these mocks
available to all tests because then we wouldn't get surprising test failures if we, say, forgot that we were making
a tracking call in a component and didn't explicitly mock the tracking calls in those tests.</p>

<p>At the end of the day, we can use these mocked calls in our test files by doing the following:</p>

<pre><code class="ts">import React from "react"
import { mount } from "enzyme"
import { MyComponent } from "./MyComponent"
import { TestApp } from "testing/components/TestApp"
import { useTracking } from "react-tracking"

// This only works because we mock tracking in setup.ts, and we only need to
// declare it because we want to check how many times it was called. Also, it
// would break the rules of hooks (https://reactjs.org/docs/hooks-rules.html)
// if it wasn't mocked. Tread cautiously!
const { trackEvent } = useTracking()

window.location.assign = jest.fn()

it("Calls tracking and navigates to artworks page", () =&gt; {
  const wrapper = mount(
    &lt;TestApp&gt;
      &lt;MyComponent {...props} /&gt;
    &lt;/TestApp&gt;
  )

  wrapper.find("Button").simulate("click")
  expect(trackEvent).toHaveBeenCalledTimes(1)
  expect(window.location.assign).toBeCalledWith("/artworks")
})
</code></pre>

<p>That's it! If you're trying to test something similar and found this post, I hope it helps you out. If so, or if
you're still confused, leave a comment!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Switch from Capybara Webkit to Chrome]]></title>
    <link href="https://artsy.github.io/blog/2018/11/27/switch-from-capybara-webkit-to-chrome/"/>
    <updated>2018-11-27T00:00:00-06:00</updated>
    <id>https://artsy.github.io/blog/2018/11/27/switch-from-capybara-webkit-to-chrome</id>
    <content type="html"><![CDATA[<p>Volt is the internal app name of Artsy CMS, and our partners use it to manage their inventory and presence on artsy.net. It's a Rails-based UI app that talks to many API services. We use <a href="https://github.com/rspec/rspec">RSpec</a> extensively to cover controller, model, view, and feature specs. As of Jun. 2018, Volt had 3751 specs and 495 of them were run with JavaScript enabled. It took about 16 mins to run on CircleCI with 6x parallelism.</p>

<p>Capybara-webkit was introduced from the very beginning of Volt for testing JavaScript-enabled specs via headless WebKit browser. It's been providing a lot of confidence for the past 4+ years; however, a few reasons/growing concerns have encouraged us to look for (more modern) alternatives:</p>

<!-- more -->


<a name="The.Problem"></a>
<h2>The Problem</h2>

<ul>
<li>The <a href="https://github.com/thoughtbot/capybara-webkit/tree/v1.14.0#qt-dependency-and-installation-issues">dependency of a specific versions of Qt</a> has been causing frustrations to set it up properly both on engineers' local machines and on CI.</li>
<li>The roadmap of capybara-webkit development is <a href="https://github.com/thoughtbot/capybara-webkit/issues/885#issuecomment-193988527">unclear</a>.</li>
<li>It's been hard to truly identify the root cause of "flickering" feature specs (i.e. tests that fail intermittently and are hard to reliably reproduce), while retrying tended to resolve it on CI.</li>
<li>The entire RSpec tests took about 16 mins to complete on CI, with 6 parallelism. The slowness made it unrealistic to run the whole tests locally.</li>
</ul>


<a name="The.Goal"></a>
<h2>The Goal</h2>

<p>Headless Chrome has gained a lot of attention in the past few years and migrations done by companies such as <a href="https://about.gitlab.com/2017/12/19/moving-to-headless-chrome/">GitLab</a> and <a href="https://robots.thoughtbot.com/headless-feature-specs-with-chrome">thoughtbot</a> have proven it to be a promising alternative to capybara-webkit. In fact, it's been <a href="http://guides.rubyonrails.org/5_1_release_notes.html#system-tests">officially included in Rails 5.1</a> for <a href="https://guides.rubyonrails.org/testing.html#system-testing">system tests</a>.</p>

<p>The goal of this project is to switch to Headless Chrome and maintain the same feature sets we have now. This includes:</p>

<ul>
<li>Making all existing specs pass</li>
<li>Running in container environments and using Artsy <a href="https://github.com/artsy/hokusai">Hokusai</a></li>
<li>Supporting mechanisms to debug specs, e.g. examining browser console logs for JavaScript behavior, taking screenshots on demand and automatically on failure, etc.</li>
<li>Bonus point to improve the stability of feature specs</li>
<li>Bonus point to improve the speed of running the entire test suite</li>
</ul>


<a name="The.How"></a>
<h2>The How</h2>

<p>First, we replaced <code>capybara-webkit</code> with <code>selenium-webdriver</code> and <code>chromedriver-helper</code>:</p>

<pre><code class="ruby">gem 'selenium-webdriver'
gem 'chromedriver-helper'
</code></pre>

<p><a href="https://github.com/flavorjones/chromedriver-helper"><code>chromedriver-helper</code></a> was useful to help install <a href="https://sites.google.com/a/chromium.org/chromedriver/">chromedriver</a> in different environments, e.g. an engineer's local machine, CI, etc.</p>

<p>Second, we registered both <code>:chrome</code> and <code>:headleass_chrome</code> drivers. By default, it used Headless Chrome as the JavaScript driver, and we could easily switch to Chrome and observe the actual interaction happening in a real browser.</p>

<pre><code class="ruby">Capybara.register_driver :chrome do |app|
  Capybara::Selenium::Driver.new(app, browser: :chrome)
end

Capybara.register_driver :headless_chrome do |app|
  caps = Selenium::WebDriver::Remote::Capabilities.chrome(loggingPrefs: { browser: 'ALL' })
  opts = Selenium::WebDriver::Chrome::Options.new

  chrome_args = %w[--headless --window-size=1920,1080 --no-sandbox --disable-dev-shm-usage]
  chrome_args.each { |arg| opts.add_argument(arg) }
  Capybara::Selenium::Driver.new(app, browser: :chrome, options: opts, desired_capabilities: caps)
end

Capybara.configure do |config|
  # change this to :chrome to observe tests in a real browser
  config.javascript_driver = :headless_chrome
end
</code></pre>

<p>We were on Rails v5.0.2 and Capybara v2.18.0 during the migration. We will be able to simplify the configuration by using the default <code>:selenium_chrome</code> and <code>:selenium_chrome_headless</code> drivers introduced in <a href="https://github.com/teamcapybara/capybara/blob/3.11.1/lib/capybara.rb#L535-L545">Capybara v3.11.1</a>. In addition, Rails v5.1 introduced the new system tests, and it'll be even simpler by using the <a href="https://api.rubyonrails.org/v5.1.3/classes/ActionDispatch/SystemTestCase.html#method-c-driven_by"><code>driven_by</code></a> method.</p>

<a name="Lessons.Learned"></a>
<h2>Lessons Learned</h2>

<p>Naively switching to Headless Chrome caused about 60 spec failures on my local machine. We simply went through them one by one and fixed them. A big part of failures was due to <a href="https://github.com/thoughtbot/capybara-webkit/tree/v1.14.0#non-standard-driver-methods">capybara-webkit's non-standard driver methods</a>, such as setting cookies, inspecting console logs, etc., and we just had to migrate to Selenium WebDriver's equivalents.</p>

<p>However, we still observed flickering specs on CI, while the exact failures seemed to be different than previously observed with Capybara Webkit. We will have to investigate further for possible causes. Regarding speed, we didn't see significant improvement after switching to Headless Chrome, as mentioned in GitLab's and others' blog post, too.</p>

<a name="Next.Steps"></a>
<h2>Next Steps</h2>

<p>The naive migration to Chrome (and removal of the Qt dependency) already improved the developer experience quite a lot (e.g. no more wrestling with installing Capybara Webkit and Qt 5.5 on every engineer's local machine <em>and</em> CI.) There are many next steps we can keep experimenting with and improving our tests, for example</p>

<ul>
<li>Updating Volt to Rails >= 5.1 and switching to system tests</li>
<li>Investigating the causes of the flickering specs by looking into intermittent failures reported on CI</li>
<li>Improving speed by using Docker <a href="https://docs.docker.com/develop/develop-images/multistage-build/">multi-stage builds</a>, caching, writing the right type and amount of tests, etc.</li>
</ul>


<p>It's a long journey, and we were all excited about the migration and the new future. We'd love to hear your experience, too!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How To Debug Jest Tests]]></title>
    <link href="https://artsy.github.io/blog/2018/08/24/How-to-debug-jest-tests/"/>
    <updated>2018-08-24T00:00:00-05:00</updated>
    <id>https://artsy.github.io/blog/2018/08/24/How-to-debug-jest-tests</id>
    <content type="html"><![CDATA[<p>Hey there! My name is Anson and I work on the Platform team at Artsy. Recently, we faced an issue where a certain
<a href="https://github.com/airbnb/enzyme">Enzyme</a> test we wrote using mock tracking was failing, but we couldn't figure
out why. Luckily, with some help from <a href="/author/orta">Orta</a> and some clever thinking, we figured out what was going
on.</p>

<!-- more -->


<p>We thought it was an issue with the mock testing library we had written. We tried to fix the problem by sprinkling
<code>console.log</code> calls throughout the test, but it was still hard to figure out what was going on, especially without
knowing how to peek into the properties of certain objects.</p>

<p>Instead, <a href="/author/orta">Orta</a> suggested we used the Chrome Node DevTools. Since the Enzyme test is run via
<code>yarn jest</code>, yarn is acting as a frontend for running the Enzyme test with Node. This means that we can use the
Chrome Node DevTools as a debugger to run the Enzyme test. This was super useful since the one thing we needed was
to be able to peek inside certain objects to see what they looked like and how they were failing. It was a much
faster, more methodical way to approach debugging this test. Here are the steps we took:</p>

<ul>
<li>First, insert a new line in your test where you think it might be failing and type <code>debugger</code>. This will serve as
a break point for the debugger to stop at.</li>
<li>Open up Chrome and type in the address bar : <code>chrome://inspect</code></li>
<li>Click on "Open dedicated DevTools for Node"</li>
<li>In your terminal, instead of typing <code>yarn jest &lt;path_to_test&gt;</code>, type this:</li>
</ul>


<pre><code class="bash">node --inspect node_modules/.bin/jest --runInBand &lt;path_to_test&gt;
</code></pre>

<p>Or you can add it to your <code>package.json</code> as a script:</p>

<pre><code class="diff">  {
    "scripts" : {
+    "test:debug": "node --inspect node_modules/.bin/jest --runInBand",
    }
  }
</code></pre>

<p>Which you can then run as <code>yarn test:debug &lt;path_to_test&gt;</code>.</p>

<p>Voila! Your test should now be running in the Chrome debugger. And you get your handy console to poke around all
sorts of stuff!</p>

<p>You also have the option of using this with Jest's <code>--watch</code> mode in order easily re-run tests, after changes to
app or test code.</p>

<pre><code class="bash">node --inspect node_modules/.bin/jest --watch --runInBand &lt;path_to_test&gt;
</code></pre>

<p>Now simply hit Enter in the terminal running your Jest process anytime you want to re-run your currently selected
specs. You'll be dropped right back into the Chrome debugger.</p>

<p>You might be wondering how this fixed our tests. Well, turns out that we missed a <code>jest.unmock()</code> call at the top
of the test file. <em>Facepalm.</em> To prevent this from biting other developers in the future, <a href="/author/orta">Orta</a>
whipped up a <a href="https://github.com/artsy/reaction/pull/1174">pull request</a> to add a rule in our TypeScript linter,
check it out!</p>

<p>Either way, in the future, this will probably be my first step in debugging non-obvious issues in tests, if only to
eliminate possible sources of the issues. I'm glad I was able to learn with <a href="/author/orta">Orta</a> about a methodical
way to debug test failures. Hope this helps, and happy hacking!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Splitting up a large test suite]]></title>
    <link href="https://artsy.github.io/blog/2015/09/24/splitting-up-a-large-test-suite/"/>
    <updated>2015-09-24T22:13:00-05:00</updated>
    <id>https://artsy.github.io/blog/2015/09/24/splitting-up-a-large-test-suite</id>
    <content type="html"><![CDATA[<p>A while back, we wrote about <a href="/blog/2012/10/09/how-to-run-rspec-test-suites-in-parallel-with-jenkins-ci-build-flow/">How to Run RSpec Test Suites in Parallel with Jenkins CI Build Flow</a>. A version of that still handles our largest test suite, but over time the initial division of specs became unbalanced. We ended up with some tasks that took twice as long as others. Even worse, in an attempt to rebalance task times, we ended up with awkward file patterns like <code>'spec/api/**/[a-m]*_spec.rb'</code>.</p>

<p>To keep our parallel spec tasks approximately equal in size and to support arbitrary concurrency, we've added a new <code>spec:sliced</code> task:</p>

<!-- more -->


<pre><code class="ruby">namespace :spec do
  task :set_up_spec_files do
    spec_files = Dir['spec/**/*_spec.rb']
    @spec_file_digests = Hash[spec_files.map { |f| [f, Zlib.crc32(f)] }]
  end

  RSpec::Core::RakeTask.new(:sliced, [:index, :concurrency] =&gt; :set_up_spec_files) do |t, args|
    index = args[:index].to_i
    concurrency = args[:concurrency].to_i
    t.pattern = @spec_file_digests.select { |f, d| d % concurrency == index }.keys
  end
end
</code></pre>

<p>As you can see, the <code>set_up_spec_files</code> helper task builds a hash of spec file paths and corresponding checksums. When we invoke the <code>sliced</code> task with <code>index</code> and <code>concurrency</code> values (e.g., <code>0</code> and <code>5</code>), only the spec files with checksums equal to <code>0</code> when mod-ed by <code>5</code> are run. Thus, the Jenkins build flow would look like:</p>

<pre><code class="java">parallel (
  {build("master-ci-task", tasks: "spec:sliced[0,5]")},
  {build("master-ci-task", tasks: "spec:sliced[1,5]")},
  {build("master-ci-task", tasks: "spec:sliced[2,5]")},
  {build("master-ci-task", tasks: "spec:sliced[3,5]")},
  {build("master-ci-task", tasks: "spec:sliced[4,5]")}
)
build("master-ci-succeeded")
</code></pre>

<p>Now, spec times <em>might</em> continue to be unbalanced despite files being split up approximately evenly. (For a more thorough approach based on recording spec times, see <a href="https://github.com/ArturT/knapsack">knapsack</a>.) However, this little bit of randomness was a big improvement over our previous approach, and promises to scale in a uniform manner.</p>
]]></content>
  </entry>
  
</feed>
