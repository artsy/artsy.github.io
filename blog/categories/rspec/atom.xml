<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: RSpec | Art.sy Engineering]]></title>
  <link href="http://artsy.github.com/blog/categories/rspec/atom.xml" rel="self"/>
  <link href="http://artsy.github.com/"/>
  <updated>2012-06-25T09:01:48-04:00</updated>
  <id>http://artsy.github.com/</id>
  <author>
    <name><![CDATA[Art.sy]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[How to Organize Over 3000 RSpec Specs and Retry Test Failures]]></title>
    <link href="http://artsy.github.com/blog/2012/05/15/how-to-organize-over-3000-rspec-specs-and-retry-test-failures/"/>
    <updated>2012-05-15T12:00:00-04:00</updated>
    <id>http://artsy.github.com/blog/2012/05/15/how-to-organize-over-3000-rspec-specs-and-retry-test-failures</id>
    <content type="html"><![CDATA[<p>Having over three thousand RSpec tests in a single project has become difficult to manage. We chose to organize these into suites, somewhat mimicking our directory structure. And while we succeeded at making our Capybara integration tests more reliable (see <a href="/blog/2012/02/03/reliably-testing-asynchronous-ui-w-slash-rspec-and-capybara/">Reliably Testing Asynchronous UI with RSpec and Capybara</a>), they continue relying on finicky timeouts. To avoid too many false positives we've put together a system to retry failed tests. We know that a spec that fails twice in a row is definitely not a fluke!</p>

<p>Create a new Rake file in <code>lib/tasks/test_suites.rake</code> and declare an array of test suites.</p>

<p>``` ruby lib/tasks/test_suites.rake
  SPEC_SUITES = [</p>

<pre><code>{ :id =&gt; :models, :pattern =&gt; "spec/models/**/*_spec.rb" },
{ :id =&gt; :controllers, :pattern =&gt; "spec/controllers/**/*_spec.rb" },
{ :id =&gt; :views, :pattern =&gt; "spec/views/**/*_spec.rb" }
</code></pre>

<p>  ]
```</p>

<!-- more -->


<p><code>RSpec::Core</code> contains a module called <code>RakeTask</code> that will programmatically create Rake tasks for you.</p>

<p>``` ruby lib/tasks/test_suites.rake
require 'rspec/core/rake_task'</p>

<p>namespace :test
  namespace :suite</p>

<pre><code>RSpec::Core::RakeTask.new("#{suite[:id]}:run") do |t|
  t.pattern = suite[:pattern]
  t.verbose = false
  t.fail_on_error = false
end
</code></pre>

<p>  end
end
```</p>

<p>Run <code>rake -T</code> to ensure that the suites have been generated. To execute a suite, run <code>rake test:suite:models:run</code>. Having a test suite will help you separate spec failures and enables other organizations than by directory, potentially allowing you to tag tests across multiple suites.</p>

<p><code>bash
rake spec:suite:models:run
rake spec:suite:controllers:run
rake spec:suite:views:run
</code></p>

<p>Retrying failed specs has been a long requested feature in RSpec (see <a href="https://github.com/rspec/rspec-core/issues/456">#456</a>). A viable approach has been finally implemented by <a href="https://github.com/antifun">Matt Mitchell</a> in <a href="https://github.com/rspec/rspec-core/pull/596">#596</a>. There're a few issues with that pull request, but two pieces have already been merged that make retrying specs feasible outside of RSpec.</p>

<ul>
<li><a href="https://github.com/rspec/rspec-core/pull/610">#610</a>:
A fix for incorrect parsing input files specified via <code>-O</code>.</li>
<li><a href="https://github.com/rspec/rspec-core/pull/614">#614</a>:
A fix for making the <code>-e</code> option cumulative, so that one can pass multiple example names to run.</li>
</ul>


<p>Both will appear in the 2.11.0 version of RSpec, in the meantime you have to point your <code>rspec-core</code> dependency to the latest version on Github.</p>

<p><code>ruby Gemfile
"rspec-core", :git =&gt; "https://github.com/rspec/rspec-core.git"
</code></p>

<p>Don't forget to run <code>bundle update rspec-core</code>.</p>

<p>The strategy to retry failed specs is to output a file that contains a list of failed ones and to feed that file back to RSpec. The former can be accomplished with a custom logger. Create <code>spec/support/formatters/failures_formatter.rb</code>.</p>

<p>``` ruby spec/support/formatters/failures_formatter.rb
require 'rspec/core/formatters/base_formatter'</p>

<p>module RSpec
  module Core</p>

<pre><code>module Formatters
  class FailuresFormatter &lt; BaseFormatter

    # create a file called rspec.failures with a list of failed examples
    def dump_failures
      return if failed_examples.empty?
      f = File.new("rspec.failures", "w+")
      failed_examples.each do |example|
        f.puts retry_command(example)
      end
      f.close
    end

    def retry_command(example)
      example_name = example.full_description.gsub("\"", "\\\"")
      "-e \"#{example_name}\""
    end

  end
end
</code></pre>

<p>  end
end
```</p>

<p>In order to use the formatter, we must tell RSpec to <code>require</code> it with <code>--require</code> and to use it with <code>--format</code>. We don't want to lose our settings in <code>.rspec</code> either - all these options can be combined in the Rake task.</p>

<p>``` ruby lib/tasks/test_suites.rake
RSpec::Core::RakeTask.new("#{suite[:id]}:run") do |t|
  t.pattern = suite[:pattern]
  t.verbose = false
  t.fail_on_error = false
  t.spec_opts = [</p>

<pre><code>"--require", "#{Rails.root}/spec/support/formatters/failures_formatter.rb",
"--format", "RSpec::Core::Formatters::FailuresFormatter",
File.read(File.join(Rails.root, ".rspec")).split(/\n+/).map { |l| l.shellsplit }
</code></pre>

<p>  ].flatten
end
```</p>

<p>Once a file is generated, we can feed it back to RSpec in another task, called <code>suite:suite[:id]:retry</code>.</p>

<p>``` ruby lib/tasks/test_suites.rake
RSpec::Core::RakeTask.new("#{suite[:id]}:retry") do |t|
  t.pattern = suite[:pattern]
  t.verbose = false
  t.fail_on_error = false
  t.spec_opts = [</p>

<pre><code>"-O", File.join(Rails.root, 'rspec.failures'),
File.read(File.join(Rails.root, '.rspec')).split(/\n+/).map { |l| l.shellsplit }
</code></pre>

<p>  ].flatten
end
```</p>

<p>Finally, lets combine the two tasks and invoke <code>retry</code> when the <code>run</code> task fails.</p>

<p>``` ruby lib/tasks/test_suites.rake
task "#{suite[:id]}" do
  rspec_failures = File.join(Rails.root, 'rspec.failures')
  FileUtils.rm_f rspec_failures
  Rake::Task["spec:suite:#{suite[:id]}:run"].execute
  unless $?.success?</p>

<pre><code>puts "[#{Time.now}] Failed, retrying #{File.read(rspec_failures).split(/\n+/).count} failure(s) in spec:suite:#{suite[:id]} ..."
Rake::Task["spec:suite:#{suite[:id]}:retry"].execute
</code></pre>

<p>  end
end
```</p>

<p>A complete version of our <code>test_suites.rake</code>, including a <code>spec:suite:all</code> task that executes all specs can be found <a href="https://gist.github.com/2597305">in this gist</a>. Our Jenkins CI runs <code>rake spec:suite:all</code>, with a much improved weather report since we started using this system.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Reliably Testing Asynchronous UI w/ RSpec and Capybara]]></title>
    <link href="http://artsy.github.com/blog/2012/02/03/reliably-testing-asynchronous-ui-w-slash-rspec-and-capybara/"/>
    <updated>2012-02-03T11:45:00-05:00</updated>
    <id>http://artsy.github.com/blog/2012/02/03/reliably-testing-asynchronous-ui-w-slash-rspec-and-capybara</id>
    <content type="html"><![CDATA[<p>tl;dr - You can write 632 rock solid UI tests with Capybara and RSpec, too.</p>

<p><img src="/images/2012-02-03-reliably-testing-asynchronous-ui-w-slash-rspec-and-capybara/jenkins-ci.png" title="[Miami Weather in NYC]" ></p>

<p>We have exactly 231 integration tests and 401 view tests out of a total of 3086 in our core application today. This adds up to 632 tests that exercise UI. The vast majority use <a href="http://rspec.info/">RSpec</a> with <a href="https://github.com/jnicklas/capybara">Capybara</a> and <a href="http://seleniumhq.org/">Selenium</a>. This means that every time the suite runs we set up real data in a local MongoDB and use a real browser to hit a fully running local application, 632 times. The suite currently takes 45 minutes to run headless on a slow Linode, UI tests taking more than half the time.</p>

<p>While the site is in private beta (request your invite <a href="http://art.sy/request_invite">here</a>), you can get a glimpse of the complexity of the UI from the <a href="http://art.sy">splash page</a>. It's a rich client-side Javascript application that talks to an API. You can open your browser's developer tools and watch a combination of API calls and many asynchronous events.</p>

<p>Keeping the UI tests reliable is notoriously difficult. For the longest time we felt depressed under the Pacific Northwest -like weather of our Jenkins CI and blamed every possible combination of code and infrastructure for the many intermittent failures. We've gone on sprees of marking many such tests "pending" too.</p>

<p>We've learned a lot and stabilized our test suite. This is how we do UI testing.</p>

<!-- more -->


<h2>An Asynchronous Application</h2>

<p>The splash page on <a href="http://art.sy">art.sy</a> is a <a href="http://documentcloud.github.com/backbone/">Backbone.js</a> application where views fade in and out depending on user actions. It also implements a responsive layout because some elements cannot render on mobile devices or shouldn't depending on the size of your browser.</p>

<p>The application is initialized in a usual Backbone way.</p>

<p>``` coffeescript
window.Splash =
  Views: {}
  Routers: {}
  Models: {}
  initialize: -></p>

<pre><code>contentWindow = new @Models.ContentWindow()
@router = new @Routers.Client contentWindow
new @Views.Responsive contentWindow
</code></pre>

<p>```</p>

<p>From here, everything is asynchronous. The router will wire up the events and the different views that make up the page will render themselves.</p>

<h2>Testing a Login Form</h2>

<p>When a user clicks on a "Log In" link, he sees the <code>Splash.Views.Login</code> Backbone view. There's no page reload or server roundtrip: the current view is swapped out by the Backbone view coming in. Some CSS animates the transition.</p>

<p>``` coffeescript</p>

<p>class Splash.Routers.Client extends Backbone.Router</p>

<p>  routes:</p>

<pre><code>'log_in' : 'log_in'
</code></pre>

<p>  log_in: -></p>

<pre><code>Splash.login = new Splash.Views.Login()
@navigate 'log_in'
</code></pre>

<p>```</p>

<p>The log-in view has two input fields: an e-mail address and password. We can write a Capybara test that enters valid values and ensures that the user logged in by checking for a specific header.</p>

<p>``` ruby
require 'spec_helper'</p>

<p>feature "Log In" do
  context "using a browser", :js => true do</p>

<pre><code>scenario "allows a user to login" do
  user = Fabricate(:user)
  visit "/"
  click_link "log_in"
  fill_in "user[email]", :with =&gt; user.email
  fill_in "user[password]", :with =&gt; user.password
  click_button "sign in"
  find("h1", :visible =&gt; true).text.should == "Login Successful"
end
</code></pre>

<p>  end
end
```</p>

<p>This test works well with Capybara, because it tries to wait for elements to appear on the page. For example, when you use <code>fill_in</code> it attempts to locate an element with the <code>user[email]</code> id, several times, until it times out or until the element is on the page.</p>

<h2>Waiting for Explicit Events</h2>

<p>The above test is "reliable" within some limits. It works as long as all the necessary asynchronous events run within a timeout period. But what if they don't? What if the test hardware is taking a break from flushing to disk? Or waiting on Google Analytics when the network cable is unplugged, which shouldn't affect the outcome of the test? These external issues make this code very brittle, so everyone keeps increasing the default timeout values.</p>

<p>A winning strategy to avoid this is to introduce explicit wait controls inside the tests. These wait <code>Capybara.default_wait_time</code> for a true result and no longer force you to know which method in Capybara waits for a timeout and which doesn't. It effectively breaks up a single wait into multiple waits.</p>

<p>Consider a widget that needs to be saved by making a postback.</p>

<p>``` coffeescript
@$el.removeClass("saved").addClass('saving')
@widget.save
  success: =></p>

<pre><code>@$el.removeClass("saving").addClass("saved")
</code></pre>

<p>```</p>

<p>When the widget is saved, its element will get a <code>.saved</code> CSS class. The test can wait for it.</p>

<p><code>ruby
it "saves the widget" do
  widget_count = Widget.count
  find("save").click
  wait_until { find(".saved", visible: true) }
  Widget.count.should == widget_count + 1
end
</code></p>

<h2>There's Just Too Much Going On</h2>

<p>Sometimes, waiting on explicit events is just not practical. You may have many AJAX requests going on at the same time and after those are done, you may still be executing JavaScript that modifies the DOM in meaningful ways. Lets attempt to answer the following two questions:</p>

<ul>
<li>How can we wait on all remaining AJAX requests to finish?</li>
<li>How can we wait on all remaining DOM events to finish?</li>
</ul>


<h2>Remaining AJAX Requests</h2>

<p>If you're using jQuery, you can test the number of active connections to a server. The number is zero when all pending AJAX requests have finished. This was an original idea from <a href="http://pivotallabs.com/users/mgehard/blog/articles/1671-waiting-for-jquery-ajax-calls-to-finish-in-cucumber">Pivotal</a>.</p>

<p>``` ruby spec/support/wait_for_ajax_helper.rb
def wait_for_ajax(timeout = Capybara.default_wait_time)
  page.wait_until(timeout) do</p>

<pre><code>page.evaluate_script 'jQuery.active == 0'
</code></pre>

<p>  end
end
```</p>

<h2>Remaining DOM Events</h2>

<p>This one is a bit tricker. We can leverage the fact that JavaScript engines are updating the UI on a single thread. If you defer an action it will execute after everything else that has been deferred before it. Therefore we can queue an addition of an empty DIV with a new id and finally wait for it. By using a unique ID we allow the waits to stack up nicely in a single spec.</p>

<p>``` ruby spec/support/wait_for_dom_helper_.rb
def wait_for_dom(timeout = Capybara.default_wait_time)
  uuid = SecureRandom.uuid
  page.find("body")
  page.evaluate_script &lt;&lt;-EOS</p>

<pre><code>_.defer(function() {
  $('body').append("&lt;div id='#{uuid}'&gt;&lt;/div&gt;");
});
</code></pre>

<p>  EOS
  page.find("##{uuid}")
end
```</p>

<p>We do have to make sure that the body element is loaded, first. This allows a <code>wait_for_dom</code> right after we navigate to a page that executes AJAX queries on load.</p>

<h2>Combining Techniques</h2>

<p>With enough attention we were able to explain and fix most spec failures. When implementing Capybara tests we favor explicit waits and use the combination of the two wait functions above when we just want to generically make sure that everything on the page has loaded and is ready for more action.</p>

<p>Finally, integration tests are essential for continuous deployment. They are very much worth the extra development effort.</p>
]]></content>
  </entry>
  
</feed>
