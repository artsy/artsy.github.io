<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: ruby | Artsy Engineering]]></title>
  <link href="http://artsy.github.io/blog/categories/ruby/atom.xml" rel="self"/>
  <link href="http://artsy.github.io/"/>
  <updated>2019-04-04T10:53:26+00:00</updated>
  <id>http://artsy.github.io/</id>
  <author>
    <name><![CDATA[]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[GraphQL: Union vs. Interface]]></title>
    <link href="http://artsy.github.io/blog/2019/01/14/graphql-union-vs-interface/"/>
    <updated>2019-01-14T00:00:00+00:00</updated>
    <id>http://artsy.github.io/blog/2019/01/14/graphql-union-vs-interface</id>
    <content type="html"><![CDATA[<p>At Artsy we’ve been moving towards GraphQL for all of our new services. Acknowledging GraphQL is a relatively new
technology, we faced some challenging questions as we were developing one our most recent services.</p>

<p>Naively as my first attempt to define GraphQL types and schemas, I naturally tried to map our database models to
GraphQL types. While this may work for lot of cases, we may not be utilizing some of the useful features that come
with GraphQL that can make the consuming our data a lot easier.</p>

<a name="GraphQL:.Interface.or.Union."></a>
<h2>GraphQL: Interface or Union?</h2>

<p>Think of the case where we are trying to expose search functionality and the result of our search can be either a
<code>Book</code> , <code>Movie</code> or <code>Album</code>. One way to think about this is to have our search query return something like:</p>

<!-- more -->


<pre><code class="js">search(term: "something") {
  books {
    id
    title
    author
  }
  movies {
    id
    title
    director
  }
  albums {
    id
    name
  }
}
</code></pre>

<p>While ☝️ works, we can’t rank the result based on relevance in one result set. Ideally, we would return one result
set that can have different types in it. A naive approach for this could be to only return one type in the results:</p>

<pre><code class="js">search(term: "something") {
  results {
    id
    name
    author   // when a book
    director // when a movie
    title    // when a movie/book
  }
}
</code></pre>

<p>We could have a single object that has all these values as optional properties:</p>

<pre><code class="js">type Result {
  id: ID!
  name: String!

  // All of the optional data, available as nullable types
  author: String
  director: String
  title: String
}
</code></pre>

<p>But returning these Result objects would be very messy on the server and for clients, plus it would undermine using
GraphQL's type system.</p>

<p>There are two main solutions in the GraphQL toolkit for this problem:
<a href="https://graphql.org/learn/schema/#union-types">Unions</a> and
<a href="https://graphql.org/learn/schema/#interfaces">Interfaces</a>.</p>

<a name="Union"></a>
<h3>Union</h3>

<p>GraphQL interfaces are useful to solve problems like above where we want to have the returned type possibly from
different types.</p>

<p>For this to work, we can define a <code>Union</code> type that can resolve to either one of <code>Book</code>, <code>Movie</code> or <code>Album</code> and
then each type can have its own set of fields.</p>

<p>In <code>graphql-ruby</code> you can define Unions with:</p>

<pre><code class="ruby">class Types::Movie &lt; Types::BaseObject
  field :id, ID, null: false
  field :title, String, null: false
  field :director, String, null: false
end

class Types::Book &lt; Types::BaseObject
  field :id, ID, null: false
  field :title, String, null: false
end

class Types::Album &lt; Types::BaseObject
  field :id, ID, null: false
  field :name, String, null: false
end

class SearchResultUnionType &lt; Types::BaseUnion
  description 'Represents either a Movie, Book or Album'
  possible_types Book, Movie, Album
  def self.resolve_type(object, _context)
    case object
    when Movie then Types::Movie
    when Book then Types::Book
    when Album then Types::Album
    else
      raise "Unknown search result type"
    end
  end
end
</code></pre>

<p>With the above change you can now query for search results and use specific fragments for different result type:</p>

<pre><code class="js">query {
  search(term: "something") {
    ... on Movie {
      __typename
      id
      title
    }
    ... on Book {
      __typename
      id
      title
    }
    ... on Album {
      __typename
      id
      name
    }
  }
}
</code></pre>

<pre><code class="json">{
  "data": [
    {
      "__typename": "Movie",
      "id": 1,
      "title": "Close-Up"
    },
    {
      "__typename": "Album",
      "id": 2,
      "name": "Dark Side Of The Moon"
    }
  ]
}
</code></pre>

<a name="Interface"></a>
<h3>Interface</h3>

<p>Unions are useful when we are trying to group different types together in one field. Now let’s think of the case
where we are trying to expose models of the same Type that can have different fields populated.</p>

<p>For example a music <code>Instrument</code> can have strings or not. If it has strings we want to mention how many strings it
has in <code>numberOfStrings</code> field. For any non-string instrument this field would be <code>null</code> in the database.</p>

<p>One way to do this is to have the <code>Instrument</code> Type always have <code>numberOfStrings</code> and in the case of non-string
instruments return <code>nil</code>. Sample result for this would be:</p>

<pre><code class="json">{
  "data": [
    {
      "id": 1,
      "name": "Guitar",
      "numberOfStrings": 6
    },
    {
      "id": 2,
      "name": "Drums",
      "numberOfStrings": null
    }
  ]
}
</code></pre>

<p>The above solution would work, but it will add extra work on the clients to decide if <code>numberOfStrings</code> is even
applicable to this current instrument or not.</p>

<p>The more GraphQL approach for this would be to use an <code>Interface</code>. We can define a generic <code>Instrument</code> interface and
have all the common fields between all instruments defined there. Then we can have each specific category of
instruments define its own special fields and then access those specific fields using fragments.</p>

<p>In <code>graphql-ruby</code> you can define an Interface with:</p>

<pre><code class="ruby">module Types::InstrumentInterface
  include Types::BaseInterface

  description 'A Musical Instrument'
  graphql_name 'Musical Instrument'

  field :id, ID, null: false
  field :name, String, null: false
  field :category, String, null: false

  definition_methods do
    def resolve_type(object, _context)
      case object.category
      when "string" then Types::StringInstrument
      when "drums" then Types::DrumInstrument
      else
        raise 'Unknown instrument type'
      end
    end
  end
end
</code></pre>

<p>Then we can have our specific types implementing this interface.</p>

<pre><code class="ruby">class Types::StringInstrument &lt; Types::BaseObject
  implements Types:: InstrumentInterface

  field :number_of_strings, Integer, null: false
end
</code></pre>

<p>For types that don’t have any extra field, they can just reuse everything from interface.</p>

<pre><code class="ruby">class Types::DrumInstrument &lt; Types::BaseObject
  implements Types:: InstrumentInterface
end
</code></pre>

<p>This way the query for getting instruments can look like</p>

<pre><code class="ruby">query {
  instruments {
    id
    name
    category
    ... on StringInstrument {
       numberOfStrings
    }
  }
}
</code></pre>

<p>Sample response can look like</p>

<pre><code class="json">{
  "data": [
    {
      "id": 1,
      "name": "Guitar",
      "category": "StringInstrument",
      "numberOfStrings": 6
    },
    {
      "id": 2,
      "name": "Drums",
      "category": "StringInstrument"
    }
  ]
}
</code></pre>

<p>One issue we found after doing the above was, since this way we don’t reference <code>StringInstrument</code> and <code>DrumInstrument</code>
types anywhere in our schema, they actually don’t end up showing in the generated schema. For them to show up we
have to add them as <code>orphan_types</code> in the interface. So the interface definition will look like:</p>

<pre><code class="ruby">module Types::InstrumentInterface
  include Types::BaseInterface

  description 'A Music Album'
  graphql_name 'Album'

  field :id, ID, null: false
  field :name, String, null: false
  field :category, String, null: false

  ## Changes
  orphan_types Types::StringInstrument, Types::DrumInstrument

  definition_methods do
    def resolve_type(object, _context)
      case object.category
      when "string" then Types::StringInstrument
      when "drums" then Types::DrumInstrument
      else
        raise 'Unknown instrument type'
      end
    end
  end
end
</code></pre>

<a name="Conclusion"></a>
<h2>Conclusion</h2>

<p>The biggest learning experience for us was to realize that with GraphQL we have the option to decouple our database
modeling with how the data is exposed to consumers. This way when designing our persistence layer, we can focus on
the needs of that layer and then separately think about whats the best way to expose the data to the outside world.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Apogee Technical Retrospective]]></title>
    <link href="http://artsy.github.io/blog/2018/02/06/apogee-technical-retrospective/"/>
    <updated>2018-02-06T00:00:00+00:00</updated>
    <id>http://artsy.github.io/blog/2018/02/06/apogee-technical-retrospective</id>
    <content type="html"><![CDATA[<p>We've previously covered <a href="/blog/2018/02/02/artsy-apogee/">what Apogee is</a> and <a href="/blog/2018/01/24/kubernetes-and-hokusai/">how it's deployed</a>, so all that's left to cover is the technology used to build it. As a refresher: Apogee is a Google Sheets Add-on we built to help our Auctions Ops team transform the data given to us by our partners into a format that our CMS can understand. This process, done manually up until now, takes a long time and is a perfect candidate for automation.</p>

<p>Apogee had some really interesting technical challenges that I enjoyed solving, and I'm excited to share some lessons I learned. So let's dive in!</p>

<!-- more -->


<p>We built a prototype as a "pure" Add-on, written only inside Google's sandbox, but that approach wouldn't work for us in production: the Add-on environment was just too difficult to work with. Google expects you to write Add-ons in their in-browser <a href="http://script.google.com">Script Editor</a> and – while whether or not that editor is <em>good</em> is a matter of preference – the environment isn't suited for collaborating or unit testing. Additionally, we could not get Add-on deploys automated, so we'd like to minimize how often we <em>have</em> to deploy.</p>

<p>So we split things up. Instead of building all Apogee's logic into an Add-on, we decided to build two pieces: a very thin Add-on and a Rails server with all the real logic.</p>

<p>(Because Apogee necessarily includes information about how our partners format their data, we decided not to open source it. Data formats are <em>probably</em> not sensitive, but that's a judgement best left up to our partners.)</p>

<a name="Apogee.Add-on"></a>
<h2>Apogee Add-on</h2>

<p>The Add-on we built is very simple, by design. Our goal was to make an Add-on that was flexible enough such that we would need to deploy it less frequently than adding new parsers.</p>

<p>Add-on responsibilities include:</p>

<ul>
<li>fetching the available parsers from the server.</li>
<li>setting up an Add-on user interface (a menu of partners, each with available parsers).</li>
<li>responding to invocations from that interface.</li>
</ul>


<p>Based on the parser selected by the user, Apogee gathers the required data from the current spreadsheet, sends it to the server for processing, and appends the results to the sheet. Pretty straightforward, you'd think.</p>

<p>Unfortunately, Google Add-ons are a bit... strange. The Add-on itself is executed in Google's datacentres (not the user's browser) and is written in <a href="https://developers.google.com/apps-script/guides/services/#basic_javascript_features">JavaScript 1.6-ish</a>. Specifically, it runs with JavaScript 1.6, plus some features from 1.7, plus some other features from 1.8, and also <a href="https://developers.google.com/apps-script/guides/services/advanced">"Google Advanced Services"</a>. The execution environment also lacks an event loop, which makes sense from Google's perspective (their servers need to know if a script execution has completed) but is still a bit unusual.</p>

<p>Rather than deal with a weird version of JavaScript, we decided to write the Add-on in <a href="https://www.typescriptlang.org">TypeScript</a> and compile down to something Google can execute. We also found <a href="https://www.npmjs.com/package/@types/google-apps-script">open source typings</a> for the Google APIs, which helped a lot. Google also provides access to certain whitelisted libraries, including <a href="https://lodash.com">Lodash</a>, which is handy.</p>

<p>Add-ons also have a somewhat complex permissions and authentication model. The <a href="https://developers.google.com/apps-script/add-ons/lifecycle">documentation</a> provided is a great illustration of why <em>complete</em> documentation is not necessarily <em>effective</em> documentation. If you already understand what you're doing, the docs are a good reference, but I found them difficult to learn from. I really like <a href="https://twitter.com/kosamari/status/852319140060823553">this explanation</a> of how to structure documentation like unit tests.</p>

<p>Permissions vary wildly depending on the execution context. For example, the <code>onOpen</code> callback is able to make network requests when the script is run as an attachment to a spreadsheet, but not when deployed. This makes it difficult to populate our menu UI, which is based off an API response. I learned to not have confidence everything was working until I saw it work end-to-end.</p>

<p>One other peculiarity of Google's API is how UI callbacks work. You could create a menu for your Add-on with the following code:</p>

<pre><code class="js">SpreadsheetApp.getUi()
  .createAddonMenu()
  .addItem('Do something', 'doSomething')
  .addToUi()

function doSomething() {
}
</code></pre>

<p>You'll notice that the callback function is specified by a <em>string</em> representing a function name (and not as a function itself, which would be more idiomatic). So, for every menu item, there must exist a corresponding function in the global scope with a corresponding name. Sadly, no parameters are passed to these callbacks, so it's impossible for a function to determine which menu item it was invoked by. Therefore, every menu item <em>must</em> have exactly <em>one</em> corresponding function. That presents a problem for an Add-on with a dynamic menu.</p>

<p>The Add-on isn't executed in a browser; we're running on Google's datacentres so let's just brute-force this. Our menu is a list of partner names, which is itself a submenu of parsers specific to that partner. That means that each menu item (and corresponding callback) can be indexed by two integers: a partner index and a operation index. So now we have a way to map from our user interface to a specific operation to perform inside <em>one</em> common menu handler.</p>

<p>Let's take a look at the actual code.</p>

<pre><code class="ts">interface Operation {
  name: string
  columns: string[]
  token: string
}

interface Partner {
  name: string
  operations: Operation[]
}

// Sets up the Add-on menu and submenus.
function setupAddon(ui: Partner[]) {
  // Reduce the ui to a list of submenus.
  const addOnMenu = ui.reduce((menu, partner, partnerIndex) =&gt; {
    // Reduce the operations list to a list of menu items.
    return menu.addSubMenu(partner.operations.reduce((memo, operation, operationIndex) =&gt; {
      return memo.addItem(operation.name, `partner${partnerIndex}Operation${operationIndex}`)
    }, SpreadsheetApp.getUi().createMenu(partner.name)))
  }, SpreadsheetApp.getUi().createAddonMenu())
  // Add the generated menu to the Add-on UI.
  addOnMenu.addToUi()
}
</code></pre>

<p>Each menu has a callback function named something like <code>partnerXOperationY</code>. Then we just generated a few thousand functions that match that format and call a shared handler <em>with</em> <code>X</code> and <code>Y</code> as parameters. The generated code looks like this:</p>

<pre><code class="js">function partner0Operation0() {
    sharedHandler(0, 0);
}
function partner0Operation1() {
    sharedHandler(0, 1);
}
function partner0Operation2() {
    sharedHandler(0, 2);
}

function sharedHandler(partnerIndex, operationIndex) {
    // TODO: Look up the appropriate parser to use.
}
</code></pre>

<p>It's not elegant, but it works. Actually, I think it does have a certain elegance, given the constraints it has to operate within.</p>

<p>So that's it! The rest of the challenges were just weird permissions issues or config problems, but the Add-on was pretty easy to build. The file generated by the TypeScript compiler is only 166 lines long, and the file with all our menu callbacks is "only" 8000 lines long. Next, let's talk about the server.</p>

<a name="Apogee.Server"></a>
<h2>Apogee Server</h2>

<p>So, Rails' philosophy is "<a href="https://en.wikipedia.org/wiki/Convention_over_configuration">convention over configuration</a>", which is pretty great as long as you know the conventions. I'd never run <code>rails new</code> before. Also, that philosophy works best when you're building <em>conventional</em> apps. Because Apogee is a bit unconventional, I was going to write Apogee in Sinatra before my colleague suggested I use Rails in <a href="http://guides.rubyonrails.org/api_app.html">API-only mode</a> instead. It seemed a bit overkill, but I also didn't want to pass up the chance to finally learn Rails.</p>

<p>The server has two endpoints:</p>

<ul>
<li><code>/ui</code> provides a list of partners and their respective parsers.</li>
<li><code>/columns</code> accepts spreadsheet columns and returns processed data (cell contents and a background colour to indicate our confidence in parsed results).</li>
</ul>


<p>We needed a way for the server to specify all its operations in a way that they could be invoked through the second endpoint. We decided to use a token-based approach: each parser has a token that can be used to invoke the parser later on. This dovetails with how I structured the parsers, too.</p>

<p>Each partner is defined by a submodule within the <code>Apogee::Parser</code> module, and each parser is defined by a class within that partner module. Let's take a look at some code.</p>

<pre><code class="rb">module Apogee
  module Parser
    module Skinner
      extend Apogee::BaseParser

      class DimensionsParser
        # Name to show in Add-on UI.
        def self.menu_name
          "Parse dimensions from Description column"
        end

        # Columns required by the `/columns` endpoint.
        def self.column_names
          %w[Description]
        end

        # Parse the columns, called from the `/columns` endpoint.
        def self.parse(columns)
          # TODO: parse the columns.
        end
      end
    end
  end
end
</code></pre>

<p>Each class within a partner is expected to have those three class methods.</p>

<p>So now that we have a defined structure for our parsers, we can use Ruby reflection to collect a list of partner modules:</p>

<pre><code class="rb">Parser.constants
  .select { |c| Parser.const_get(c).is_a? Module }
  .map do |c|
    {
      name: c,
      operations: Parser.const_get(c).public_parsers
    }
end
</code></pre>

<p>Each module also has a <code>public_parsers</code> function (inherited from <code>Apogee::BaseParser</code>) which also uses reflection:</p>

<pre><code class="rb">def public_parsers
  constants
    .select { |c| const_get(c).is_a? Class }
    .map { |c| const_get(c) }
    .map do |klass|
      {
        klass: klass.to_s,
        name: klass.menu_name,
        columns: klass.column_names,
        token: Digest::SHA256.base64digest(klass.to_s)
      }
    end
end
</code></pre>

<p>This code collects all the Ruby classes inside a module into a data structure that can be consumed by the Apogee Add-on through the <code>/ui</code> endpoint. As a bonus, the tokens are generated from the SHA256 hash of the fully-qualified parser class names. And we also avoid having to maintain a separate list of parsers that I would inevitably forget to update. Win-win.</p>

<p>All that's left to do is to lookup a parser class from a token. This is as easy as finding the class with the matching token and calling its <code>parse</code> function.</p>

<pre><code class="rb">parser = partners
  .map { |p| p[:operations] }
  .flatten
  .find { |op| op[:token] == token }
Object.const_get(parser[:klass]).parse(columns)
</code></pre>

<p>Neat!</p>

<p>This approach is <em>good</em>, but strikes me as overly object-oriented. <em>Most</em> of the parsers we're going to write are going to do the same thing: they have the same three methods and the <code>parse</code> method is basically just matching each spreadsheet cell against a regular expression. We can make a better abstraction.</p>

<p>Since the parsers are defined by the presence of a class within a partner module, we can use metaprogramming to abstract away all the common pieces and add classes to the module programmatically. The implementation is too in-depth to explain in detail here, but our partner module above could be rewritten to look like the following:</p>

<pre><code class="rb">module Apogee
  module Parser
    module Skinner
      extend Apogee::BaseParser

      add_single_column_parser(
        class_name: 'DimensionsParser',
        menu_name: 'Parse dimensions from Description column',
        column_name: 'Description',
        regex: %r{REGEX GOES HERE},
        new_columns: %w[Height Width Depth Unit]
      ) do |match|
        # TODO: Process each cell.
      end
    end
  end
end
</code></pre>

<p>I created two such methods: one that uses a single regex, and another that uses multiple regexes (for more complex needs). I also wrote a handy <code>add_all_parser</code> method which adds a sort of meta-parser, which collates the results from calling <code>parse</code> on all the <em>other</em> parsers in that module. Our Ops team just needs to click "Parse everything" and the entire spreadsheet is processed with all the parsers in seconds.</p>

<p>And of course, since all our parsers are just Ruby classes, they were easy to unit test.</p>

<p>I've done metaprogramming in other languages, and it was a lot of fun to use it in Ruby. I ran the code by my colleagues who are more experienced in Ruby than I am, and documented everything thoroughly. It's a real shame the codebase isn't open source, because I'm really proud of the approach and would love to share it with you.</p>

<a name="Apogee.Authentication"></a>
<h2>Apogee Authentication</h2>

<p>We needed to make sure that only the Add-on itself was invoking the server's endpoints. Not because the server has sensitive data – Apogee's server has no database and doesn't access any APIs – but just because it's good practice to limit access to services to only who needs them.</p>

<p>We evaluated a bunch of prospective auth strategies, including (but not limited to) the following:</p>

<ul>
<li>Whitelist Google datacentre IP addresses, block all others.</li>
<li>HTTP Basic Auth.</li>
<li>Shared secret.</li>
<li>OAuth with Artsy's API, by the user upon Add-on installation.</li>
<li>Something totally custom, or a combination of any of these.</li>
</ul>


<p>After thoughtful discussion, we decided on a solution that works for us. I'm not going to specify what we used – not because I'm that concerned about the security, but because each project and team will have their own needs. If you build a server, think carefully about what kind of authentication makes sense for you and your team.</p>

<a name="Conclusion"></a>
<h2>Conclusion</h2>

<p>Apogee was a really fun project. It had a defined scope, so it was a good first Rails project for me to tackle. The Add-on helps my colleagues on the Auctions Ops team do their jobs easier, so it was intrinsically rewarding to build. And it turns out that our Gallery Partnerships team also has to import a lot of partner data into Artsy's CMS, so I'm now exploring ways Apogee can help them, too.</p>

<p>As a closing note, I want to discuss something that's been on my mind lately. I've been developing iOS apps <a href="https://ashfurrow.com/blog/5-years-of-ios/">since 2009</a>, and have a <a href="https://ashfurrow.com/books/">very intimate knowledge</a> of Objective-C, Swift, and UIKit. For a long time, I actually avoided learning new languages and frameworks because they intimidated me – starting over in a new framework, from scratch, felt like a step backward.</p>

<p>I think this is a common frame of mind, among iOS developers, among all developers. But now I regret avoiding new technology for so long. The languages and tools that I knew had become part of my identity: I was an "iOS Developer." That identity was a source of strength, but was also a limitation.</p>

<p>Developers solve problems. Sometimes those problems are best solved with iOS apps. And sometimes, they're best solved with spreadsheet plugins. After <a href="https://ashfurrow.com/blog/swift-vs-react-native-feels/">realizing</a> last year that I was limiting myself, I'm still coming to terms with how that impacts my identity. But I'll say this: if <em>I</em> can leave the safety blanket of the iOS world and build something completely new, so can you. Don't let your expertise and experience limit what you think you can build.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using OCR To Fix a Hilarious Bug]]></title>
    <link href="http://artsy.github.io/blog/2015/11/05/Using-OCR-To-Fix-A-Hilarious-Bug/"/>
    <updated>2015-11-05T00:00:00+00:00</updated>
    <id>http://artsy.github.io/blog/2015/11/05/Using-OCR-To-Fix-A-Hilarious-Bug</id>
    <content type="html"><![CDATA[<p>For a little while, we would get very strange bug reports. People would complain that artist thumbnails (viewed in several different contexts across the web and our iOS apps) would not be an image of the artist's work, but rather text, which had inexplicably become an actual JPG. This wasn't just text appearing in a <code>div</code> that should contain an <code>img</code> or something like that, these were actual JPG's that were pictures of text.</p>

<p>We would fix these as they came up, chalking the strangeness up to some relic of an old image processing pipeline, data being migrated, etc.</p>

<p>However, the reports kept coming in. This blog post is about how we diagnosed this actual bug, and how we used a simple Ruby script and OCR to help us detect and fix the existing images.</p>

<!-- more -->


<p>Here's an example of a bug report where the thumbnail for <a href="https://www.artsy.net/artist/marina-abramovic-1">Marina Abramović</a> became the text of her bio.</p>

<p><img src="/images/2015-11-12-hilarious-bug/search.png" alt="Bad Search" /></p>

<p>Here's one from our <a href="https://github.com/artsy/eigen">iOS app</a> showing that thumbnails for related artists are set to their bios as well.</p>

<p><img src="/images/2015-11-12-hilarious-bug/eigen.png" alt="Bad Related Artists" /></p>

<p>Weird right? We eventually tracked down what was going on, and it's actually perfectly summarized in <a href="https://github.com/blueimp/jQuery-File-Upload/pull/3356">this issue</a>. When someone copies text from Excel, it also generates an image of that cell or cells, and puts it into the clipboard. We immediately suspected something with <code>pasteZone</code>, and the bug was easy to reproduce - have an image in your clipboard and paste anywhere on the page.</p>

<p>We have an admin panel that allows some metadata about an artist to be edited. This includes their bio, as well as a place to upload a representative image as their 'cover thumbnail'.</p>

<p>As the issue describes, we had some text input fields, as well as a file upload form using <a href="https://github.com/blueimp/jQuery-File-Upload">Blueimp's jQuery File Upload</a>. When you don't specify a <code>pasteZone</code> it defaults to the entire document. This means that a paste event anywhere on the page will trigger that event.</p>

<p>Our editorial team was using Microsoft Excel and Word to organize some data about the artist, including bios. When ready, a team member would copy and paste the bio into the bio input text field. This would also immediately fire the event for the image upload, which now automagically became an actual picture of the text of the bio. Our API and image processing pipeline would happily accept that, leading to the incredibly bizarre bug reports.</p>

<p>My immediate fix was to specify and scope <code>pasteZone</code> (and similarly, <code>dropZone</code>) to the element the file upload widget was bound to. That would prevent the problem from happening again. Taking a quick look art some random samples of artists, it looked like potentially thousands of records might have been affected and I became interested in a programmatic way to detect these images. A manual approach would have been very cumbersome.</p>

<p>Since the images were that of text, I decided to use OCR to remove artist thumbnails that it determined had 'too much text'. This may have unset valid covers from artists that use lots of text in their work, such as <a href="https://www.artsy.net/artist/joseph-kosuth">Joseph Kosuth</a>. However, this was safe to do since we have some custom logic to fall back to an image of an iconic artwork by the artist in the case of a missing thumbnail.</p>

<p>To get OCR functionality in Ruby, I decided to use <a href="https://github.com/tesseract-ocr/tesseract">Tesseract</a>, a great OSS library. Once I had it installed, I used a <a href="https://github.com/meh/ruby-tesseract-ocr">ruby wrapper</a> to make using it easier.</p>

<p>The script eventually turned into something like:</p>

<pre><code class="ruby"># initialize and configure Tesseract
engine = Tesseract::Engine.new do |config|
  config.language  = :eng
  config.blacklist = '|'
end

# iterate over artists and pull their thumbnails
# given the URL to a publicly accessible image at img

text = engine.text_for(img)
text = text.gsub(/[^a-z ]/i, '').gsub(' ', '')
if text.length &gt; 30
  puts "Found problematic artist #{artist_doc['last']}"
  # ...
end
</code></pre>

<p>So all we do is find all the text in an image, and then remove any garbage characters or artifacts from the OCR analysis, and then use 30 as an arbitrary cutoff to determine if an image was problematic. If the image had more than 30 characters as detected by the OCR library, we wound up unsetting it from the artist.</p>

<p>The additional logic to set artist covers from their iconic artworks was already in place, and I ran this script in production, identifying and unsetting over 1000 problematic thumbnails. And we haven't gotten any new reports of this bug since then :)</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Splitting up a large test suite]]></title>
    <link href="http://artsy.github.io/blog/2015/09/24/splitting-up-a-large-test-suite/"/>
    <updated>2015-09-24T22:13:00+00:00</updated>
    <id>http://artsy.github.io/blog/2015/09/24/splitting-up-a-large-test-suite</id>
    <content type="html"><![CDATA[<p>A while back, we wrote about <a href="/blog/2012/10/09/how-to-run-rspec-test-suites-in-parallel-with-jenkins-ci-build-flow/">How to Run RSpec Test Suites in Parallel with Jenkins CI Build Flow</a>. A version of that still handles our largest test suite, but over time the initial division of specs became unbalanced. We ended up with some tasks that took twice as long as others. Even worse, in an attempt to rebalance task times, we ended up with awkward file patterns like <code>'spec/api/**/[a-m]*_spec.rb'</code>.</p>

<p>To keep our parallel spec tasks approximately equal in size and to support arbitrary concurrency, we've added a new <code>spec:sliced</code> task:</p>

<!-- more -->


<pre><code class="ruby">namespace :spec do
  task :set_up_spec_files do
    spec_files = Dir['spec/**/*_spec.rb']
    @spec_file_digests = Hash[spec_files.map { |f| [f, Zlib.crc32(f)] }]
  end

  RSpec::Core::RakeTask.new(:sliced, [:index, :concurrency] =&gt; :set_up_spec_files) do |t, args|
    index = args[:index].to_i
    concurrency = args[:concurrency].to_i
    t.pattern = @spec_file_digests.select { |f, d| d % concurrency == index }.keys
  end
end
</code></pre>

<p>As you can see, the <code>set_up_spec_files</code> helper task builds a hash of spec file paths and corresponding checksums. When we invoke the <code>sliced</code> task with <code>index</code> and <code>concurrency</code> values (e.g., <code>0</code> and <code>5</code>), only the spec files with checksums equal to <code>0</code> when mod-ed by <code>5</code> are run. Thus, the Jenkins build flow would look like:</p>

<pre><code class="java">parallel (
  {build("master-ci-task", tasks: "spec:sliced[0,5]")},
  {build("master-ci-task", tasks: "spec:sliced[1,5]")},
  {build("master-ci-task", tasks: "spec:sliced[2,5]")},
  {build("master-ci-task", tasks: "spec:sliced[3,5]")},
  {build("master-ci-task", tasks: "spec:sliced[4,5]")}
)
build("master-ci-succeeded")
</code></pre>

<p>Now, spec times <em>might</em> continue to be unbalanced despite files being split up approximately evenly. (For a more thorough approach based on recording spec times, see <a href="https://github.com/ArturT/knapsack">knapsack</a>.) However, this little bit of randomness was a big improvement over our previous approach, and promises to scale in a uniform manner.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Releasecop Tracks Stale Releases]]></title>
    <link href="http://artsy.github.io/blog/2015/09/01/releasecop-tracks-stale-releases/"/>
    <updated>2015-09-01T17:30:00+00:00</updated>
    <id>http://artsy.github.io/blog/2015/09/01/releasecop-tracks-stale-releases</id>
    <content type="html"><![CDATA[<p>Artsy practices a sort of <a href="http://en.wikipedia.org/wiki/Continuous_delivery">continuous delivery</a>. We keep release cycles short and the process of reviewing, testing, and deploying our software as reliable, fast, and automated as possible. (This blog has touched on these practices <a href="http://artsy.github.io/blog/categories/testing/">multiple</a> <a href="http://artsy.github.io/blog/categories/continuous-integration">times</a>.)</p>

<p>Usually, commits that have been reviewed and merged are immediately built and tested. Successfully built versions of the codebase are often automatically deployed to a staging environment. On an automated or frequent-but-manual basis, that version is deployed to a production environment. Thus, commits form a pipeline:</p>

<ul>
<li>From developers' working branches</li>
<li>To the master branch</li>
<li>Through a hopefully-successful build</li>
<li>To a staging environment</li>
<li>To production</li>
</ul>


<p>The number of apps and services we deploy has grown to <em>dozens</em> per team, so sometimes things fall through the cracks. We've been using <a href="https://github.com/joeyAghion/releasecop">Releasecop</a> for the last few months to get gentle email reminders when an environment could use a deploy.</p>

<!-- more -->


<pre><code>gem install releasecop
releasecop edit
</code></pre>

<p>This opens a <em>manifest</em> file where you can describe the sequence of git remotes and branches that make up your own release pipeline. For example:</p>

<pre><code>{
  "projects": {
    "charge": [
      { "name": "master", "git": "git@github.com:artsy/charge.git" },
      { "name": "staging", "git": "git@heroku.com:charge-staging.git" },
      { "name": "production", "git": "git@heroku.com:charge-production.git" }
    ],
    "gravity": [
      { "name": "master", "git": "git@github.com:artsy/gravity.git" },
      { "name": "master-succeeded", "git": "git@github.com:artsy/gravity.git", "branch": "master-succeeded" },
      { "name": "staging", "git": "git@github.com:artsy/gravity.git", "branch": "staging" },
      { "name": "production", "git": "git@github.com:artsy/gravity.git", "branch": "production" }
    ]
  }
}
</code></pre>

<p>The <code>charge</code> app is a typical deployment to Heroku. Work progresses from the <code>master</code> branch to a <code>charge-staging</code> app to a <code>charge-production</code> app. The <code>gravity</code> app is a more complicated, non-Heroku deployment. It updates git branches to reflect what has been merged (<code>master</code>), tested (<code>master-succeeded</code>), deployed to staging, and deployed to production.</p>

<p>Run the <code>releasecop check [app]</code> command to report the status of your apps' releases:</p>

<pre><code>$ releasecop check --all
charge...
  staging is up-to-date with master
  production is up-to-date with staging
gravity...
  master-succeeded is up-to-date with master
  staging is up-to-date with master-succeeded
  production is behind staging by:
    06ca969 2015-09-04 [config] Replace Apple Push Notification certificates that expire today. (Eloy Durán)
    171121f 2015-09-03 Admin-only API for cancelling a bid (Matthew Zikherman)
    4c5feea 2015-09-02 install mongodb client in Docker so that import rake tasks can run (Barry Hoggard)
    95347d1 2015-08-31 Update to delayed_job cookbook that works with Chef 11.10 (Joey Aghion)
2 project(s) checked. 1 environment(s) out-of-date.
</code></pre>

<p>A nightly <a href="https://jenkins-ci.org/">Jenkins</a> job emails us the results, but a cron job could work equally well.</p>

<p><a href="https://github.com/joeyAghion/releasecop">Releasecop</a> reminds us to deploy ready commits and close the loop on in-progress work. We hope you find it useful. (Pull requests are welcome!)</p>
]]></content>
  </entry>
  
</feed>
