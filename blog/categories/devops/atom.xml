<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: devops | Artsy Engineering]]></title>
  <link href="https://artsy.github.io/blog/categories/devops/atom.xml" rel="self"/>
  <link href="https://artsy.github.io/"/>
  <updated>2024-05-17T14:49:02+00:00</updated>
  <id>https://artsy.github.io/</id>
  <author>
    <name><![CDATA[]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Servers for Everyone: Review Apps @ Artsy]]></title>
    <link href="https://artsy.github.io/blog/2020/08/21/review-apps-post/"/>
    <updated>2020-08-21T00:00:00+00:00</updated>
    <id>https://artsy.github.io/blog/2020/08/21/review-apps-post</id>
    <content type="html"><![CDATA[<p>This blog post is going to motivate and describe Artsy’s adoption and evolution
of the usage of review apps.</p>

<p>This first part of this post covers a couple of common problems where
topic-specific servers (i.e. review apps) are useful.</p>

<p>The rest of the post describes Artsy’s history with review app automation via
incremental problem solving and the composition of a few well-known technologies.</p>

<!-- more -->

<h2 id="problem-10-a-single-shared-staging-environment">Problem 1.0: A Single Shared Staging Environment</h2>

<p>At Artsy, we have a sizable engineering organization: ~40 engineers organized
across many teams. Engineers on those teams work on many codebases, some are
exclusive to a team, but many codebases are worked on by engineers across many
teams. Artsy’s website (www.artsy.net), <a href="https://github.com/artsy/force">Force</a>, is a good example of such a shared
service.</p>

<p>These different teams of developers working on a shared service have (mostly
hidden) dependencies upon each other, most visible when a shared service is being
deployed to production.</p>

<p>Let’s work the following example:</p>

<ul>
  <li>Team A is hard at work finishing up a new feature (say a new search
page for a list of art auctions) on Service S and is now doing a final round
of QA before deploying to production</li>
  <li>Team B is fixing a bug in an unrelated part of Service S</li>
  <li>Team B confirms that the bug was squashed on staging</li>
  <li>Team B deploys Service S to production</li>
</ul>

<p>Artsy’s production deployment flow is rooted in a GitHub Pull Request, meaning
that the commits that differ between staging (on the <code class="language-plaintext highlighter-rouge">staging</code> Git branch) and
production (on the <code class="language-plaintext highlighter-rouge">release</code> Git branch) are clear to whomever is managing a
deploy (<a href="https://github.com/artsy/force/pull/6106">example</a>).</p>

<p>While it’s great that our deploys are easy to understand, ensuring that a deploy
of a service is safe <em>requires communicating with the teams that contributed
code to ensure that their work is “safe to deploy”</em>.</p>

<p>“Safe to deploy” might mean different things depending on the nature of the
work.</p>

<p>For example, Team A’s new list of art auctions might require an API endpoint in
another Service Z to be deployed for their part of the deploy of Service S to
be safe to deploy. Team B’s bugfix might just requires a quick visual confirmation.</p>

<p>Suffice to say, it’s <em>hard to independently confirm that another team’s work is
safe to deploy</em>.</p>

<p>Despite the mitigation strategies discussed next, there’s risk of deploying unsafe
code whenever a single staging environment is used across many teams.</p>

<h4 id="shared-staging-mitigation-strategies">Shared Staging Mitigation Strategies</h4>

<p>There are a couple of ways Artsy mitigates against the possible pitfalls of a
shared staging environment:</p>

<ol>
  <li>
    <p>Having a culture of quickly deploying code to production</p>

    <p>By building a culture that views frequent deploys positively, there’s, on average,
less diff in every deploy, mitigating the risk of unintentionally deploying code
that’s not safe to deploy.</p>
  </li>
  <li>
    <p>Using automated quality processes geared towards a stable staging environment</p>

    <p>We do our best to feel as confident as possible in a change <em>before it is deployed
 to staging</em> by creating automated tests for changes, sharing visual changes over
 Slack and in PRs, and other strategies relevant to the work at hand.</p>
  </li>
  <li>
    <p>Communicating deploys</p>

    <p>When deploying a service, Artsy engineers typically send a message to our #dev
Slack channel notifying of their plan to deploy a particular service, cc’ing the
engineers that are involved in other PRs that are part of the deploy. In the example
above, an engineer on Team B would notify relevant stakeholders of Team A, giving
that team the opportunity to flag if their work is not yet safe to deploy.</p>
  </li>
</ol>

<p>While these strategies are impactful:</p>

<ul>
  <li>
    <p>Semi-unstructured communication is prone to breakdown: the notified engineers
on Team A might be pairing or in a meeting and Team B deploys anyways.</p>
  </li>
  <li>
    <p>Without a true continuous delivery model, it’s a challenge to operationalize
very frequent production deploys. Moreover, the problem compounds as the team
grows and the velocity of work increases.</p>
  </li>
  <li>
    <p>Particularly when working in a large distributed system, automated testing at
the individual service level can only provide a certain level of safety for a
change. Visual changes which require correctness on different viewports and devices
are, pragmatically, often best to test manually.</p>
  </li>
</ul>

<p>If only there was a way to allow Team A and B to work without risking stepping
on each other toes!</p>

<p>We’ll discuss how review apps provide this safety, but first another related
problem.</p>

<h2 id="problem-20-complex-work-needs-many-eyes">Problem 2.0: Complex Work Needs Many Eyes</h2>

<p>But before it gets better, it’s going to get a bit worse.</p>

<p>While working on any mature distributed system is a challenge, some work is
particularly challenging or risky. Generally, risk is reduced when many
skilled eyes can review the risky thing in an environment that closely mimics
the production environment.</p>

<p>This class of work might include changes to authorization flows, page
redesigns or infrastructural upgrades (e.g. a NodeJS upgrade).</p>

<p>For example, to feel safe deploying <a href="https://github.com/artsy/force/pull/5697">a major version upgrade of Artsy’s design
system in Force</a> the most pragmatic way forward was
to deploy that PR to a server where other engineers could collaborate on QA.</p>

<p><em>If a single shared staging environment is the only non-production server to
deploy to, the chances that work lands on staging in an unsafe state is high</em>. While
staging being unsafe isn’t <em>itself</em> a bad thing, many bad things can result:</p>

<ol>
  <li>
    <p>[Bad] Blocked Deploys</p>

    <p>If staging is unsafe and this dangerous state is discovered, then top priority
is getting a service back into a safe state. While working to get a service
back to a healthy state, new features can’t be delivered.</p>

    <p>In aggregate, this can be a sizeable productivity loss.</p>
  </li>
  <li>
    <p>[Worse] Unsafe Deploys</p>

    <p>If staging is unsafe and this dangerous state is not discovered before a production
deploy (for example, the unsafe code might be from another team, as described above),
then end-users might interact with a service that just isn’t working. No good.</p>
  </li>
  <li>
    <p>[Terrible] Fear</p>

    <blockquote>
      <p>Fear is the mind-killer.
 <a href="https://www.goodreads.com/quotes/2-i-must-not-fear-fear-is-the-mind-killer-fear-is">Dune</a></p>
    </blockquote>

    <p>Alright, a bit over the top, but the risk of unsafe or blocked deploys can
implicitly or explicitly result in teams shying away from complex work.</p>

    <p>This avoidable fear might result in increased technical debt or not taking on
certain projects.</p>

    <p>It’s generally bad for business and does not lead to a pleasant work environment!</p>
  </li>
</ol>

<h2 id="problem-recap--review-app-introduction">Problem Recap &amp; Review App Introduction</h2>

<p>To recap, there is an increased risk of unsafe or blocked deploys whenever there
is a single staging environment for a shared service. Certain types of
(incredibly useful) changes require interactive review on a live server
before we feel comfortable with those changes, which magnifies the risk of a unsafe or
blocked deploy.</p>

<p>Review apps are simply other staging environments that are easy to spin up and
are deployed with the version of the service that you are working on.</p>

<p>By QA-ing on a review app instead of a shared staging environment, teams can
take their time ensuring the safety of their changes, removing the risks
detailed above.</p>

<p>Larger infrastructure upgrades (including upgrades to the
underlying K8s configuration, in addition to app-level changes) can sit on a
server for hours or days, allowing any slow-moving issue to show itself in a
lower risk environment.</p>

<p>Artsy has iterated on its review app tooling to the point where Team A and Team B
can each deploy their changes to isolated servers of our main website, Force,
on a single <code class="language-plaintext highlighter-rouge">git push</code> to a branch matching a naming convention.</p>

<p>The rest of this post describes Artsy’s evolution of its review app tooling
and areas for continued improvement.</p>

<h2 id="review-app-tooling">Review App Tooling</h2>

<h3 id="heroku-days">Heroku Days</h3>

<p>In the beginning, Artsy deployed most services on Heroku. There were fewer engineers
and teams, so the engineering organization was less impacted by the problems described above.</p>

<p><a href="https://devcenter.heroku.com/articles/github-integration-review-apps">Heroku review apps</a> were used on some teams sparingly.</p>

<h3 id="enter-kubernetes-and-hokusai">Enter Kubernetes and Hokusai</h3>

<p>For many reasons outside of the scope of this post, Artsy began migrating
services off of Heroku and onto an AWS-backed Kubernetes (K8s) deployment model
starting in <a href="https://github.com/artsy/force/pull/953">February 2017</a>.</p>

<p>In order to allow application engineers to reasonably interface with K8s-backed
services, Artsy developed a command line interface,
<a href="https://github.com/artsy/hokusai"><code class="language-plaintext highlighter-rouge">hokusai</code></a>, to provide a Heroku CLI-like interface for
configuring and deploying these services.</p>

<p>Check out some <a href="https://artsy.github.io/blog/2019/10/18/kubernetes-and-hokusai/">prior</a> <a href="https://artsy.github.io/blog/2018/01/24/kubernetes-and-hokusai/">posts</a> discussing the experience of working with Kubernetes
and <code class="language-plaintext highlighter-rouge">hokusai</code>.</p>

<p>About a year after <code class="language-plaintext highlighter-rouge">hokusai</code>’s initial release, the tool released <a href="https://github.com/artsy/hokusai/pull/62">its initial
support for review apps</a>.</p>

<p>Via subcommands within the <code class="language-plaintext highlighter-rouge">hokusai review_app</code> namespace, developers were able to:</p>

<ul>
  <li>Create the needed K8s YAML configuration file from an existing staging configuration file</li>
  <li>Execute this configuration: creating a running server within a dedicated namespace</li>
  <li>Perform other server management tasks: re-deploying to the server, setting ENV variables, etc</li>
</ul>

<h3 id="problem-more-steps-needed">Problem: More Steps Needed</h3>

<p>While <code class="language-plaintext highlighter-rouge">hokusai</code>’s official review app feature handles much of the core infrastructure
needed to get a service deployed to a new server, additional tasks are required
to have a working review app, which can be categorized into:</p>

<ol>
  <li>
    <p>Service Agnostic Tasks</p>

    <p>These include:</p>

    <ul>
      <li>Pushing a Docker image of the Git revision in question to the appropriate
Docker registry</li>
      <li>Editing the generated YAML configuration file to reference this Docker image</li>
      <li>Sourcing the appropriate ENV variables (typically from the shared staging
server)</li>
    </ul>

    <p>Check out <a href="https://github.com/artsy/hokusai/blob/master/docs/Review_Apps.md"><code class="language-plaintext highlighter-rouge">hokusai</code>’s review app docs</a> for more
 details.</p>
  </li>
  <li>
    <p>Service Specific Tasks</p>

    <p>In addition, certain services have service-specific operational requirements that
 need to be met before a review app is fully functional.</p>

    <p>For example, in Force, we need to:</p>

    <ul>
      <li>Publish front-end assets to S3 for the specific revision being deployed, and</li>
      <li>Tweak some ENV variables from the values copied over from the shared staging
server</li>
    </ul>
  </li>
</ol>

<p><strong>Impact</strong>: Due to the manual labor required to (re)-learn and execute the
commands needed to build a review app, they were used sparingly by a few engineers
that already invested time in learning up on them.</p>

<h3 id="solution-a-bash-script">Solution: A bash script</h3>

<p>While these tasks described above are tedious, they don’t really require a
decision-making human behind the computer and can be automated.</p>

<p>In August 2019, we <a href="https://github.com/artsy/force/pull/4412">automated</a> these tasks via a Bash script.</p>

<p><strong>Impact</strong>: A developer is able take a Force commit and get it deployed to K8s
by running a single script on their laptop. Folks became excited about review
apps and started to use them more for Force development.</p>

<h3 id="problem-depending-upon-a-developers-laptop-doesnt-scale">Problem: Depending upon a developer’s laptop doesn’t scale</h3>

<p>The increased excitement and usage of review apps in Force revealed a new problem:</p>

<p>Building and pushing &gt;2 GB Docker image across home WiFi networks can be incredibly
slow, decreasing the usefulness and adoption of the Bash script.</p>

<h3 id="solution-run-the-bash-script-on-ci">Solution: Run the bash script on CI</h3>

<p>After discussions within Artsy’s Platform Practice, a possible solution
emerged: build the review app by running this Bash script on CircleCI upon push
to a branch starting with <code class="language-plaintext highlighter-rouge">review-app</code>.</p>

<p>This means that a developer’s laptop is then only responsible for pushing a
commit to a branch and CircleCI does all the heavy lifting.</p>

<p>Moreover, the process of centralizing the review app creation into CI helped us realize
the subsequent requirement: updating, not creating, review apps when a review app
already exists for a given branch.</p>

<p>Check out the <a href="https://github.com/artsy/force/pull/5370">pull request</a> for the nitty gritty on how
we leveraged CircleCI branch filtering and more Bash to move this workload into
CircleCI and intelligently determine when to update or create a review app.</p>

<p><strong>Impact</strong>: Any developer can spin up a Force review app in ~15 minutes on a <code class="language-plaintext highlighter-rouge">git push</code>.
Review app are being used often for major and minor changes alike.</p>

<h2 id="future-iterations">Future Iterations</h2>

<p>Artsy has come far with its tooling for review applications, but, as always,
there’s areas for us for to grow in, including:</p>

<ol>
  <li>
    <p>Automating the de-provisioning of review apps that no
longer useful.</p>
  </li>
  <li>
    <p>Automating the creation of DNS CNAME records within Cloudflare, removing one
final manual step.</p>
  </li>
  <li>
    <p>While the improvements to review app infrastructure has sparked similar
investments in other codebases, there’s a lot of work we could do to bring
this Git-CircleCI-Bash based approach to other shared services we deploy at
Artsy.</p>
  </li>
</ol>

<h2 id="on-incremental-improvement">On Incremental Improvement</h2>

<p>One of Artsy’s Engineering Principles is <a href="https://github.com/artsy/README/blob/master/culture/engineering-principles.md#incremental-revolution">“Incremental
Revolution”</a>, which begins with:</p>

<blockquote>
  <p>Introduce new technologies slowly and incrementally.</p>
</blockquote>

<p>I think Artsy’s approach to review apps is a great example of this principle
implemented.</p>

<p>As opposed to finding a silver bullet technology or strategy, our approach has
been to build off of a working current state, layering on a new component to
solve the next problem.</p>

<p>At each point along our solution journey, we’re left with a working and more valuable
solution to the problem at hand.</p>

<p>Thanks for reading!</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Xcode 8 Manual Codesigning with Fastlane]]></title>
    <link href="https://artsy.github.io/blog/2017/01/13/xcode-8-fastlane-codesigning/"/>
    <updated>2017-01-13T14:00:00+00:00</updated>
    <id>https://artsy.github.io/blog/2017/01/13/xcode-8-fastlane-codesigning</id>
    <content type="html"><![CDATA[<p>New year, new deploy process! Late last year our mobile team completed the update to Swift 3 (and thus, the update to Xcode 8). The latest version of Apple’s IDE includes a lovely feature: automating provisioning profile management! (Note: not sarcasm, the feature is really nice. Check out the <a href="https://developer.apple.com/videos/play/wwdc2016/401/">WWDC video</a> for an in-depth exploration.)</p>

<p><img src="/images/2017-01-13-xcode-8-fastlane-codesigning/xcode-screenshot.png" alt="Automatic code signing settings" /></p>

<p>However, when I went to make our first <a href="http://artsy.github.io/blog/2015/12/15/Automating-Testflight-Deploys/">automated deploy</a> today, things didn’t work; I got a somewhat cryptic error about code signing.</p>

<!-- more -->

<blockquote>
  <p>Code signing is required for product type ‘Application’ in SDK ‘iOS 10.1’</p>
</blockquote>

<p>Code signing was failing for our project. Hmm. First step in fixing a bug is always to reproduce it, which I could do locally. I started looking into the code that manages our deploys’ signing process and got lost. My colleague Orta was kind enough to give me a hand.</p>

<p>Some background: the Fastlane suite of tools includes <a href="https://github.com/fastlane/fastlane/tree/master/match">Match</a>, which manages your signing certificates and provisioning profiles in a private GitHub repository. We don’t use match due to complications with our multiple apps, but we use <a href="https://github.com/artsy/eigen/blob/608f60860165dd9b3c376da00492a3cb36bf5214/fastlane/Fastfile#L95-L130">very similar logic</a> to clone the repo, extract the certificate and profile, and install the keys on CI.</p>

<p>So what wasn’t working?</p>

<p>Well it turns out that Xcode’s fancy new automatic code signing was incompatible with our manual process of specifying certificates and profiles. The easy solution would be to simply disable that setting, but that would be a shame: the new automatic code signing makes developing on devices way easier and we didn’t want to sacrifice that for the sake of our deploys.</p>

<p>So we went looking and luckily found <a href="https://github.com/artsy/eigen/pull/2104">the solution</a>. We amended our codesigning setup with the <a href="https://docs.fastlane.tools/actions/#update_project_provisioning">update_project_provisioning</a> and <a href="https://docs.fastlane.tools/actions/#update_project_team">update_project_team</a> Fastlane actions, and the <a href="https://github.com/hjanuschka/fastlane-plugin-update_project_codesigning">update_project_codesigning plugin</a>. Basically, we disable the automatic signing feature and then manually set the provisioning profile to the one we cloned from our private GitHub repo.</p>

<p>So remember folks, if you’re ever asked to sacrifice ease of development for the sake of getting computers to behave, there’s probably a better way.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Automating TestFlight Deploys using Fastlane]]></title>
    <link href="https://artsy.github.io/blog/2015/12/15/Automating-Testflight-Deploys/"/>
    <updated>2015-12-15T00:00:00+00:00</updated>
    <id>https://artsy.github.io/blog/2015/12/15/Automating-Testflight-Deploys</id>
    <content type="html"><![CDATA[<p>I’ve been a really <a href="http://artsy.github.io/blog/2015/09/18/Cocoa-Architecture-Dependencies/">strong supporter</a> of the <a href="https://fastlane.tools">fastlane</a> toolset. I think it fixes a lot of common developer problems, in a space that Apple doesn’t really touch. The command line.</p>

<p>We’ve added hints of fastlane to our apps at different rates, <a href="https://github.com/artsy/eidolon/">Eidolon</a> uses fastlane for everything but <a href="https://github.com/artsy/eigen/">Eigen</a>/<a href="https://github.com/artsy/energy">Energy</a>/<a href="https://github.com/artsy/emergence">Emergence</a> have been pretty slow on the uptake, though they have more complicated setups, being App Store apps.</p>

<p>When <a href="https://krausefx.com/">Felix</a> announced <a href="https://krausefx.com/blog/introducing-match-a-new-approach-to-code-signing">match</a> this week, I felt like he tackled a problem we face in our <a href="http://artsy.net/job/mobile-engineer">small dev team</a>. I integrated this, only to find that it could also fix my problems with deployment. The rest of this post goes into the “how I did this.” You can also cheat and look at the <a href="https://github.com/artsy/eigen/compare/d06270882aadec8f03927455a5229b53dd0a73c8...9eaf9082ebdcdf75f12ad2804260587e01526f2d">commits</a> directly.</p>

<!-- more -->

<p>First up, a TLDR for <a href="https://github.com/fastlane/match">match</a>. <em>match is a tool that keeps all of your code-signing setup in a private git repo.</em> We currently keep them in a shared 1Password vault. By switching to using a private git repo we can can use our existing GitHub authentication for CI to provide access to the certificates for signing on circle.</p>

<p>We use a <a href="https://github.com/artsy/eigen/blob/master/Makefile">Makefile</a>, I know that fastlane provides an awesome tool in the form of <a href="https://github.com/fastlane/fastlane#features">fastlane lanes</a> - but we’re pretty happy with a Makefile, they’re the simplest tool that does what we need.</p>

<p>I wanted to lower the barrier for us shipping betas, so I opted to add another build step in the CI process. This step checks what branch is it, and if it’s the beta branch, grab the certs, then deploy.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>deploy_if_beta_branch:
	<span class="k">if</span> <span class="o">[</span> <span class="s2">"</span><span class="si">$(</span>LOCAL_BRANCH<span class="si">)</span><span class="s2">"</span> <span class="o">==</span> <span class="s2">"beta"</span> <span class="o">]</span><span class="p">;</span> <span class="k">then </span>make certs<span class="p">;</span> make ipa<span class="p">;</span> make distribute<span class="p">;</span> <span class="k">fi</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">make certs</code> is really simple, it runs: <code class="language-plaintext highlighter-rouge">bundle exec match appstore --readonly</code> which and pulls metadata from a <a href="https://github.com/artsy/eigen/blob/9eaf9082ebdcdf75f12ad2804260587e01526f2d/fastlane/Matchfile">Matchfile</a>. This means we can sign app store builds on CI.</p>

<p>If you don’t know what the <code class="language-plaintext highlighter-rouge">bundle exec</code> prefix is, I’d recommend reading my guide on the CocoaPods website for <a href="https://guides.cocoapods.org/using/a-gemfile.html">Gemfile</a>s.</p>

<p>The next step is generating an ipa, we do this with <a href="https://github.com/fastlane/gym">gym</a> via <code class="language-plaintext highlighter-rouge">make ipa</code> which looks like this:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ipa: set_git_properties change_version_to_date
	bundle <span class="nb">exec </span>gym
</code></pre></div></div>

<p>It executes some make tasks to ensure we know what git commit each build is, and we use the date to provide a faux-<a href="http://semver.org">semver</a> for apps.</p>

<p>Gym will build our app, according to our <a href="https://github.com/artsy/eigen/blob/9eaf9082ebdcdf75f12ad2804260587e01526f2d/fastlane/GymFile">Gymfile</a>. Nothing too surprising in there. It will output an <a href="http://apple.stackexchange.com/questions/26550/what-does-ipa-stand-for">ipa</a> and a <a href="http://stackoverflow.com/questions/3656391/whats-the-dsym-and-how-to-use-it-ios-sdk">dsym</a> that <code class="language-plaintext highlighter-rouge">make distribute</code> can handle.</p>

<p><code class="language-plaintext highlighter-rouge">make distribute</code> is a pretty easy one, we generate a CHANGELOG via Ruby, then run the command <code class="language-plaintext highlighter-rouge">bundle exec pilot upload -i build/Artsy.ipa</code>, it will ship it to iTunes Connect after configuration from the <a href="https://github.com/artsy/eigen/blob/9eaf9082ebdcdf75f12ad2804260587e01526f2d/fastlane/AppFile">Appfile</a>. This is great, but it goes one better. It will, by default, run a synchronous check for whether the App has finished processing.</p>

<p>&lt;/div&gt;&lt;/div&gt;<a href="/images/2015-12-15-Automating-Testflight-Deploys/ci-itunes-screenshot.png"><img src="/images/2015-12-15-Automating-Testflight-Deploys/ci-itunes-screenshot.png" /></a>&lt;div class='meta-container'&gt;&lt;header&gt; &lt;/header&gt;&lt;/div&gt;&lt;div class='date-container'&gt; &lt;/div&gt;&lt;div class='content-container'&gt;&lt;div class='entry-content'&gt;</p>

<p>This is awesome. I’d like to add a Slack message to tell us that it’s shipped too, which would be much easier if we used a <a href="https://github.com/fastlane/fastlane/tree/master/docs#after_all-block">Fastfile</a>. We’ve not entirely moved all of our apps to TestFlight, this is our first experiment in the space, we’ve been really happy with Hockey, and still are. However, without trying new things we’ll never be able to know what we should consider internal best practices.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Releasecop Tracks Stale Releases]]></title>
    <link href="https://artsy.github.io/blog/2015/09/01/releasecop-tracks-stale-releases/"/>
    <updated>2015-09-01T17:30:00+00:00</updated>
    <id>https://artsy.github.io/blog/2015/09/01/releasecop-tracks-stale-releases</id>
    <content type="html"><![CDATA[<p>Artsy practices a sort of <a href="http://en.wikipedia.org/wiki/Continuous_delivery">continuous delivery</a>. We keep release cycles short and the process of reviewing, testing, and deploying our software as reliable, fast, and automated as possible. (This blog has touched on these practices <a href="http://artsy.github.io/blog/categories/testing/">multiple</a> <a href="http://artsy.github.io/blog/categories/continuous-integration">times</a>.)</p>

<p>Usually, commits that have been reviewed and merged are immediately built and tested. Successfully built versions of the codebase are often automatically deployed to a staging environment. On an automated or frequent-but-manual basis, that version is deployed to a production environment. Thus, commits form a pipeline:</p>

<ul>
  <li>From developers’ working branches</li>
  <li>To the master branch</li>
  <li>Through a hopefully-successful build</li>
  <li>To a staging environment</li>
  <li>To production</li>
</ul>

<p>The number of apps and services we deploy has grown to <em>dozens</em> per team, so sometimes things fall through the cracks. We’ve been using <a href="https://github.com/joeyAghion/releasecop">Releasecop</a> for the last few months to get gentle email reminders when an environment could use a deploy.</p>

<!-- more -->

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>gem install releasecop
releasecop edit
</code></pre></div></div>

<p>This opens a <em>manifest</em> file where you can describe the sequence of git remotes and branches that make up your own release pipeline. For example:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{
  "projects": {
    "charge": [
      { "name": "master", "git": "git@github.com:artsy/charge.git" },
      { "name": "staging", "git": "git@heroku.com:charge-staging.git" },
      { "name": "production", "git": "git@heroku.com:charge-production.git" }
    ],
    "gravity": [
      { "name": "master", "git": "git@github.com:artsy/gravity.git" },
      { "name": "master-succeeded", "git": "git@github.com:artsy/gravity.git", "branch": "master-succeeded" },
      { "name": "staging", "git": "git@github.com:artsy/gravity.git", "branch": "staging" },
      { "name": "production", "git": "git@github.com:artsy/gravity.git", "branch": "production" }
    ]
  }
}
</code></pre></div></div>

<p>The <code class="language-plaintext highlighter-rouge">charge</code> app is a typical deployment to Heroku. Work progresses from the <code class="language-plaintext highlighter-rouge">master</code> branch to a <code class="language-plaintext highlighter-rouge">charge-staging</code> app to a <code class="language-plaintext highlighter-rouge">charge-production</code> app. The <code class="language-plaintext highlighter-rouge">gravity</code> app is a more complicated, non-Heroku deployment. It updates git branches to reflect what has been merged (<code class="language-plaintext highlighter-rouge">master</code>), tested (<code class="language-plaintext highlighter-rouge">master-succeeded</code>), deployed to staging, and deployed to production.</p>

<p>Run the <code class="language-plaintext highlighter-rouge">releasecop check [app]</code> command to report the status of your apps’ releases:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ releasecop check --all
charge...
  staging is up-to-date with master
  production is up-to-date with staging
gravity...
  master-succeeded is up-to-date with master
  staging is up-to-date with master-succeeded
  production is behind staging by:
    06ca969 2015-09-04 [config] Replace Apple Push Notification certificates that expire today. (Eloy Durán)
    171121f 2015-09-03 Admin-only API for cancelling a bid (Matthew Zikherman)
    4c5feea 2015-09-02 install mongodb client in Docker so that import rake tasks can run (Barry Hoggard)
    95347d1 2015-08-31 Update to delayed_job cookbook that works with Chef 11.10 (Joey Aghion)
2 project(s) checked. 1 environment(s) out-of-date.
</code></pre></div></div>

<p>A nightly <a href="https://jenkins-ci.org/">Jenkins</a> job emails us the results, but a cron job could work equally well.</p>

<p><a href="https://github.com/joeyAghion/releasecop">Releasecop</a> reminds us to deploy ready commits and close the loop on in-progress work. We hope you find it useful. (Pull requests are welcome!)</p>
]]></content>
  </entry>
  
</feed>
