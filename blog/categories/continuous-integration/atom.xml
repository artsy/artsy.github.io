<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Continuous Integration | Artsy Engineering]]></title>
  <link href="http://artsy.github.io/blog/categories/continuous-integration/atom.xml" rel="self"/>
  <link href="http://artsy.github.io/"/>
  <updated>2015-04-29T11:05:07-04:00</updated>
  <id>http://artsy.github.io/</id>
  <author>
    <name><![CDATA[Artsy]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Using CocoaPods Caching with Travis CI]]></title>
    <link href="http://artsy.github.io/blog/2014/08/08/CocoaPods-Caching/"/>
    <updated>2014-08-08T11:46:00-04:00</updated>
    <id>http://artsy.github.io/blog/2014/08/08/CocoaPods-Caching</id>
    <content type="html"><![CDATA[<p>As <a href="http://artsy.github.io/blog/2014/08/07/taking-a-snapshot-with-second-curtain/">Ash said earlier</a> we like using Continuous Integration. Today I spent a large amount of time migrating us to use the new CocoaPods caching system in Travis CI. To make up for my lost time I'm passing on what I've learned and also showing how we do CI at Artsy with Objective-C apps. If you're interested in how we do it in Swift, you can just check <a href="https://github.com/artsy/eidolon">Eidolon</a>.</p>

<!-- more -->


<p>First and foremost, this only works if you are paying for Travis CI.</p>

<p>Travis CI recently merged in support for <a href="http://docs.travis-ci.com/user/caching/">Caching of CocoaPods</a> - this is great! By using this, we've reduced our build times from an average of about 10 minutes, to about 7 minutes. It works by using your <code>Podfile.lock</code> as a key to cache your <code>Pods</code> directory, if the lock hasn't changed then there's no need to update the Cache and so <code>pod install</code> is not called on your project. This caused me an issue as the <code>[Project].xcworkspace</code> file that CocoaPods generates was not in source control, and the app wouldn't build. Useful note, if you're using <a href="http://guides.cocoapods.org/syntax/podfile.html#pod">development pods</a> in your build you probably shouldn't use this as your Pods directory can get out of sync with the cached version.</p>

<p>We use a <a href="https://github.com/artsy/eidolon/blob/master/Makefile">Makefile</a> to separate the tasks required to build, test and deploy an app. The general structure of our Makefile is:</p>

<table>
<thead>
<tr>
<th> Action        </th>
<th> Reason </th>
</tr>
</thead>
<tbody>
<tr>
<td> Constants </td>
<td> A collection of constants that get resued by different make tasks. </td>
</tr>
<tr>
<td> CI Tasks </td>
<td> Separate commands necessary for running Xcode projects from the terminal. </td>
</tr>
<tr>
<td> Actions </td>
<td> Commands that manipulate your project state, or maintainance commands. </td>
</tr>
<tr>
<td> Deployment </td>
<td> Commands to get your app ready for the App Store, or Hockey. </td>
</tr>
</tbody>
</table>


<p>If you don't know the syntax for Make, essentially if it's on the same line you're either setting constants or calling other make commands. If it's on a separate line then you are running a shell command.</p>

<p>This is the <a href="http://orta.io/#folio-header-unit">Artsy Folio</a> Makefile in full:</p>

<p>``` make</p>

<h1>Constants</h1>

<p>WORKSPACE = Artsy Folio.xcworkspace
XCPROJECT = Artsy\ Folio.xcodeproj
SCHEME = ArtsyFolio
CONFIGURATION = Beta
APP_PLIST = Info.plist
PLIST_BUDDY = /usr/libexec/PlistBuddy
TARGETED_DEVICE_FAMILY = \"1,2\"</p>

<p>BUNDLE_VERSION = $(shell $(PLIST_BUDDY) -c "Print CFBundleVersion" $(APP_PLIST))
GIT_COMMIT = $(shell git log -n1 --format='%h')
ALPHA_VERSION = $(BUNDLE_VERSION)-$(BUILD_NUMBER)-$(GIT_COMMIT)</p>

<p>GIT_COMMIT_REV = $(shell git log -n1 --format='%h')
GIT_COMMIT_SHA = $(shell git log -n1 --format='%H')
GIT_REMOTE_ORIGIN_URL = $(shell git config --get remote.origin.url)</p>

<p>DATE_MONTH = $(shell date "+%e %h")
DATE_VERSION = $(shell date "+%Y.%m.%d")</p>

<p>CHANGELOG = CHANGELOG.md
CHANGELOG_SHORT = CHANGELOG_SHORT.md</p>

<p>IPA = ArtsyFolio.ipa
DSYM = ArtsyFolio.app.dSYM.zip</p>

<h1>Phony tasks are tasks that could potentially have a file with the same name in the current folder</h1>

<p>.PHONY: build clean test ci</p>

<h1>CI Tasks</h1>

<p>ci: CONFIGURATION = Debug
ci: pods build</p>

<p>build:
    set -o pipefail &amp;&amp; xcodebuild -workspace "$(WORKSPACE)" -scheme "$(SCHEME)" -sdk iphonesimulator -destination 'name=iPad Retina' build | xcpretty -c</p>

<p>clean:
    xctool -workspace "$(WORKSPACE)" -scheme "$(SCHEME)" -configuration "$(CONFIGURATION)" clean</p>

<p>test:
    set -o pipefail &amp;&amp; xcodebuild -workspace "$(WORKSPACE)" -scheme "$(SCHEME)" -configuration Debug test -sdk iphonesimulator -destination 'name=iPad Retina' | second_curtain | xcpretty -c --test</p>

<p>lint:
    bundle exec fui --path Classes find</p>

<pre><code>bundle exec obcd --path Classes find HeaderStyle
bundle exec obcd --path "ArtsyFolio Tests" find HeaderStyle
</code></pre>

<h1>Actions</h1>

<p>ipa:
    $(PLIST_BUDDY) -c "Set CFBundleDisplayName $(BUNDLE_NAME)" $(APP_PLIST)
    $(PLIST_BUDDY) -c "Set CFBundleVersion $(DATE_VERSION)" $(APP_PLIST)
    ipa build --scheme $(SCHEME) --configuration $(CONFIGURATION) -t</p>

<p>alpha_version:
    $(PLIST_BUDDY) -c "Set CFBundleVersion $(ALPHA_VERSION)" $(APP_PLIST)</p>

<p>change_version_to_date:
    $(PLIST_BUDDY) -c "Set CFBundleVersion $(DATE_VERSION)" $(APP_PLIST)</p>

<p>set_git_properties:
    $(PLIST_BUDDY) -c "Set GITCommitRev $(GIT_COMMIT_REV)" $(APP_PLIST)
    $(PLIST_BUDDY) -c "Set GITCommitSha $(GIT_COMMIT_SHA)" $(APP_PLIST)
    $(PLIST_BUDDY) -c "Set GITRemoteOriginURL $(GIT_REMOTE_ORIGIN_URL)" $(APP_PLIST)</p>

<p>pods: remove_debug_pods
pods:
    rm -rf Pods
    bundle install
    bundle exec pod install</p>

<p>remove_debug_pods:
    perl -pi -w -e "s{pod 'Reveal-iOS-SDK'}{}g" Podfile</p>

<p>update_bundle_version:
    @printf 'What is the new human-readable release version? '; \
        read HUMAN_VERSION; \
        $(PLIST_BUDDY) -c "Set CFBundleShortVersionString $$HUMAN_VERSION" $(APP_PLIST)</p>

<p>mogenerate:
    @printf 'What is the new Core Data version? '; \
        read CORE_DATA_VERSION; \
        mogenerator -m "Resources/CoreData/ArtsyPartner.xcdatamodeld/ArtsyFolio v$$CORE_DATA_VERSION.xcdatamodel/" --base-class ARManagedObject --template-path config/mogenerator/artsy --machine-dir Classes/Models/Generated/ --human-dir /tmp/ --template-var arc=true</p>

<h1>Deployment</h1>

<p>deploy: ipa distribute</p>

<p>alpha: BUNDLE_NAME = 'Folio α'
alpha: NOTIFY = 0
alpha: alpha_version deploy</p>

<p>appstore: BUNDLE_NAME = 'Artsy Folio'
appstore: TARGETED_DEVICE_FAMILY = 2
appstore: remove_debug_pods update_bundle_version set_git_properties change_version_to_date</p>

<p>next: TARGETED_DEVICE_FAMILY = \"1,2\"
next: update_bundle_version set_git_properties change_version_to_date</p>

<p>distribute:
  cat $(CHANGELOG) | head -n 50 | awk '{ print } END { print "..." }' > $(CHANGELOG_SHORT)
  curl \
   -F status=2 \
   -F notify=$(NOTIFY) \
   -F "notes=&lt;$(CHANGELOG_SHORT)" \
   -F notes_type=1 \
   -F ipa=@$(IPA) \
   -F dsym=@$(DSYM) \
   -H 'X-HockeyAppToken: $(HOCKEYAPP_TOKEN)' \
   https://rink.hockeyapp.net/api/2/apps/upload \
   | grep -v "errors"</p>

<pre><code>
That gives you a sense of the commands that you can run from the terminal in our projects, next we need to look at the `.travis.yml` file.

``` make
language: objective-c
cache:
  - bundler
  - cocoapods

env:
  - UPLOAD_IOS_SNAPSHOT_BUCKET_NAME=eigen-ci UPLOAD_IOS_SNAPSHOT_BUCKET_PR...

before_install:
  - 'echo ''gem: --no-ri --no-rdoc'' &gt; ~/.gemrc'
  - cp .netrc ~
  - chmod 600 .netrc
  - pod repo add artsy https://github.com/artsy/Specs.git

before_script:
  - gem install second_curtain
  - make ci

script:
  - make test
  - make lint
</code></pre>

<p>This is nice and simple. It was built to use multiple travis build steps. This makes the CI output a lot more readable as an end user. Travis will by default collapse the shell output for different build stages leaving only the <code>script</code> stage defaulting to being exposed. Here is an example of what you see on a failing test:</p>

<center>
<img src="http://artsy.github.io/images/2014-08-08-CocoaPods-Caching/failing_travis_screenshot.png" alt='Travis CI Failure'>
</center>


<p>We use a gem with a binary in <a href="https://github.com/AshFurrow/second_curtain/">second_curtain</a>, and this came with bundler caching issues in Travis. The solution was to ignore bundler and run <code>gem install second_curtain</code> each time. To increase the speed we also ensured that documentation is not being generated. If you are interested in what's going on with the <code>.netrc</code>, read my blog post on <a href="http://artsy.github.io/blog/2014/06/20/artsys-first-closed-source-pod/">Artsy's first Closed Source Pod</a>.</p>

<p>We will continue pushing the state of the art in iOS deployment, in building our own tools and using everything available to increase developer happiness. If you're into this we're always looking to hire people with a good open source track record or street smarts. Here's <a href="https://artsy.net/job/mobile-engineer">the jobs page</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Taking a Snapshot with Second Curtain]]></title>
    <link href="http://artsy.github.io/blog/2014/08/07/taking-a-snapshot-with-second-curtain/"/>
    <updated>2014-08-07T11:46:00-04:00</updated>
    <id>http://artsy.github.io/blog/2014/08/07/taking-a-snapshot-with-second-curtain</id>
    <content type="html"><![CDATA[<p>At Artsy, we try hard to <a href="https://speakerdeck.com/orta/getting-eigen-out?slide=35">test</a>
our iOS applications to ensure that we avoid regressions and have a clearly
defined spec of how our apps should look and behave. One of the core pieces of
our testing setup is <a href="https://github.com/facebook/ios-snapshot-test-case">FBSnapshotTestCase</a>,
a library written by Facebook to compare views at runtime with images of those
views that are known to be correct. If the images differ, the test fails. We
also use <a href="https://travis-ci.org">Travis</a> for continuous integration.</p>

<p>Lately, we've been noticing a friction between the developers on the iOS team
and the tools we're using to test our apps: while Travis allows us to easily
access the logs of test runs, it can only indicate that a snapshot test failed,
not why it failed. That's because the images that are compared are locked on
Travis' machine – we cannot access those images, so we can't see the
differences. This is <em>really</em> promblematic when the tests pass locally but fail
only on Travis.</p>

<!-- more -->


<p>A few weeks ago, <a href="http://twitter.com/orta">Orta</a> and I were discussing this
problem and we came up with a potential solution. Since the images were stored
on disk on Travis' machine, why not just upload them somewhere we <em>can</em> see
them? Like an S3 bucket. We could even generate a basic HTML page showing you
the different test failures.</p>

<p>Time passed and, later on, I had tests passing locally but failing on Travis.
I saw an opportunity to build something new. I'm not a proficient Ruby developer,
but I enjoy learning new things, so I decided to create a Ruby gem that could
fit within our existing testing pipeline. A lot of the structure for the code
came from an existing gem we already use with Travis, <a href="https://github.com/supermarin/xcpretty">xcpretty</a>.
With an example of how gems that support iOS testing are written, I was on my
way to creating my own.</p>

<p>At first, things were very difficult. While I had contributed small patches to
existing Ruby projects before, I had never created a brand new one from scratch.
The existing <a href="http://guides.rubygems.org/make-your-own-gem/">guides</a> were very
helpful, and I found help from the CocoaPods developers when I had questions
about things like the arcane semantics of Ruby's <code>require</code> syntax.</p>

<p>Eventually, I had a working proof-of-concept. Everything seemed ready to go, and
I prepared to incorporate my new tool, which I called <a href="https://github.com/AshFurrow/second_curtain">Second Curtain</a>,
into my pull request on the Artsy repo. But there was a problem.</p>

<p>Second Curtain relies on environment variables to get access to the S3 bucket
where it stores the images. I planned on using Travis' system to <a href="http://docs.travis-ci.com/user/encryption-keys/">encrypt</a>
those credentials. It turns out, for very good reasons, encrypted environment
variables are not available on pull requests created on forks of repositories.
This is a problem because of the way that <a href="http://artsy.github.io/blog/2012/01/29/how-art-dot-sy-uses-github-to-build-art-dot-sy/">Artsy uses GitHub</a>.
While it's not a problem for a closed-source repository to have (restrictive)
access to an S3 bucket, it would be irresponsible to expose S3 credentials for
an open-source project. I'm <a href="https://github.com/AshFurrow/second_curtain/issues/5">working</a>
on a solution.</p>

<p>Orta helped with the design aspect of the tool; while uploading the images was
sufficient, we could make the process of seeing the differences between the two
images even easier. He created a <a href="https://eigen-ci.s3.amazonaws.com/snapshots/2014-08-04--15-47/index.html">HTML page</a>
that would allow developers to see the before-and-after versions by moving their
mouse cursor over the different images.</p>

<p><img src="http://static.ashfurrow.com/github/second_curtain.png" alt="Image Diff" /></p>

<p>In the end, I got Second Curtain to work with Artsy's iOS repository and I
discovered the discrepency between the two images: due to a timezone difference
between my computer and Travis', a date was being formatted differently. Not a
difficult thing to fix, but not something I would have ever been able to
discover had I not been able to see the images side-by-side.</p>

<p>So after all that, one line of Objective-C was changed and the tests passed – my
pull request was merged. I learnt a lot about how Ruby developers structure
their code and what tools they use to write software. While I'm happy to return
to iOS apps for a while, it was a great experience and I'm hoping to bring some
of the ideas I discovered writing Ruby back to Objective-C.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Continuous integration for service-oriented architectures]]></title>
    <link href="http://artsy.github.io/blog/2014/05/12/continuous-integration-for-service-oriented-architectures/"/>
    <updated>2014-05-12T10:50:00-04:00</updated>
    <id>http://artsy.github.io/blog/2014/05/12/continuous-integration-for-service-oriented-architectures</id>
    <content type="html"><![CDATA[<p>Whatever you have against monolithic architectures, at least they're easy to test. And when those tests succeed, you can be reasonably confident the live app will work the same way.</p>

<p>Artsy began as one such monolithic app, but we've been refactoring into an ecosystem of related APIs and sites. Today, when you search for <a href="https://artsy.net/gene/cultural-commentary">"cultural commentary"</a> or visit <a href="https://artsy.net/artist/robert-longo">Robert Longo</a> on <a href="https://artsy.net">artsy.net</a>, the page is rendered by a web app, sources data from an API, retrieves recommendations from a separate service, tracks trends in another, and records analytics in yet another.</p>

<p>This was a boost for developer productivity and scaling, but eviscerated the value of our tests. We repeatedly encountered bugs that were failings of <em>the interaction between codebases</em> rather than failings of individual ones. Test libraries and tools typically concern themselves with one isolated app. When you have services that consume services that consume services, those isolated tests (with their stubs of everything else) don't necessarily reflect production's reality.</p>

<p>So how should we develop our small, focused apps (or <a href="http://en.wikipedia.org/wiki/Service-oriented_architecture">service-oriented architecture</a>, or <a href="http://martinfowler.com/articles/microservices.html">microservices</a>...) with confidence? We set out to build a dedicated acceptance test suite that would run tests across multiple services, configuring and integrating them in a way that closely matches the production environment.</p>

<!-- more -->


<h2>The code</h2>

<p>We'll take the simplest example possible of 2 related applications: a trivial Ruby API serving a Node.js-based web app. (You can also go directly to <a href="https://github.com/joeyAghion/multiapp_example-tests">the source</a>.)</p>

<p><a href="http://david.heinemeierhansson.com/2014/tdd-is-dead-long-live-testing.html">Recent</a> <a href="http://blog.8thlight.com/uncle-bob/2014/04/25/MonogamousTDD.html">debates</a> <a href="https://news.ycombinator.com/item?id=7659251">aside</a>, I like to start with a test:</p>

<p>```ruby
feature "home", js: true do</p>

<p>  scenario "welcomes visitor" do
    visit "/"
    expect(page).to have_content("Browse products")
  end
end
```</p>

<p>We're using the popular [and familiar] <a href="https://github.com/jnicklas/capybara">Capybara</a> with <a href="https://relishapp.com/rspec">RSpec</a> and <a href="http://docs.seleniumhq.org/">Selenium</a>. Naturally, our test fails right away:</p>

<p>```bash
$ bundle exec rspec</p>

<h1>...</h1>

<pre><code> Failure/Error: visit "/"
 Selenium::WebDriver::Error::UnknownError:
   Target URL / is not well-formed.
</code></pre>

<pre><code>
There are a few steps to getting our projects installed and running as part of the test suite. First, we'll add git submodules in the `/api` and `/web` subdirectories that [track the master branch](http://stackoverflow.com/questions/9189575/git-submodule-tracking-latest) of each project.

```bash
git submodule add -b master git@github.com:joeyAghion/multiapp_example-api.git api
git submodule add -b master git@github.com:joeyAghion/multiapp_example-web.git web
</code></pre>

<p>Next, create Rake tasks to install prerequisites for each project.</p>

<p>```ruby</p>

<h1>Rakefile</h1>

<p>require 'childprocess'
require 'rspec/core/rake_task'</p>

<p>RSpec::Core::RakeTask.new(:spec)</p>

<p>task :ci => ['checkout', 'install', 'spec']</p>

<p>task :checkout do
  sh %{git submodule update --remote --init} do |ok, res|
    raise "Submodule update failed with status #{res.exitstatus}" unless ok
  end
end</p>

<p>task :install => ['api:install', 'web:install']</p>

<p>namespace :api do
  task :install do
    Bundler.with_clean_env do
      proc = ChildProcess.build('bundle', 'install')
      proc.io.inherit!
      proc.cwd = './api'
      proc.start
      proc.wait
      raise "bundle install exited with status #{proc.exit_code}" unless proc.exit_code == 0
    end
  end
end</p>

<p>namespace :web do
  task :install do
    proc = ChildProcess.build('npm', 'install')
    proc.io.inherit!
    proc.cwd = './web'
    proc.start
    proc.wait
    raise "npm install existed with status #{proc.exit_code}" unless proc.exit_code == 0
  end
end
```</p>

<p>The new <code>checkout</code> and <code>install</code> tasks make sure we have the latest code and all prerequisites installed. Note how we use <code>Bundler.with_clean_env</code> to isolate the API (which has its own Gemfile and bundler environment) from the test suite.</p>

<p>Now that the API and web apps are set up, we'll use RSpec's <code>before(:suite)</code> and <code>after(:suite)</code> hooks to start and stop them around each test run.</p>

<p>```ruby</p>

<h1>spec/spec_helper.rb</h1>

<p>require 'capybara/rspec'
require 'childprocess'</p>

<p>API_PORT = 7000
WEB_PORT = 7001</p>

<p>Capybara.configure do |config|
  config.current_driver = :selenium
  config.run_server = false
  config.app_host = "http://localhost:#{WEB_PORT}"
end</p>

<p>RSpec.configure do |config|
  # ...
  config.before(:suite) do
    start_api
    start_web
  end</p>

<p>  config.after(:suite) do
    stop_api
    stop_web
  end
end</p>

<p>def start_api
  $stderr.puts "Starting API..."
  Bundler.with_clean_env do
    $api = ChildProcess.build('bundle', 'exec', 'ruby', 'app.rb')
    $api.cwd = './api'
    $api.io.inherit!
    $api.environment['PORT'] = API_PORT
    $api.start
    $stderr.puts "Waiting for API to start listening..."
    sleep(1) while !listening_on?(API_PORT) &amp;&amp; $api.alive?
  end
end</p>

<p>def stop_api
  $stderr.puts "Stopping API..."
  $api.stop
end</p>

<p>def start_web
  $stderr.puts "Starting web..."
  $web = ChildProcess.build('node', 'app.js')
  $web.cwd = './web'
  $web.io.inherit!
  $web.environment['API_URL'] = "http://localhost:#{API_PORT}"
  $web.environment['PORT'] = WEB_PORT
  $web.start
  $stderr.puts "Waiting for web to start listening..."
  sleep(1) while !listening_on?(WEB_PORT) &amp;&amp; $web.alive?
end</p>

<p>def stop_web
  $stderr.puts "Stopping web..."
  $web.stop
end</p>

<p>def listening_on?(port)
  system("netstat -an | grep #{port} | grep LISTEN")
end
```</p>

<p>Running <code>rake spec</code> now starts up and waits for both apps, runs our test, and...</p>

<pre><code>Starting API...
Waiting for API to start listening...
# ...
Starting web...
Waiting for web to start listening...
# ...
home
  welcomes visitor
Stopping API...
# ...
Stopping web...

Finished in 4.67 seconds
1 example, 0 failures
</code></pre>

<p>Success!</p>

<p>Well, sort of. Our test of the home page doesn't even depend on both systems. Let's try a more meaningful test, listing products from the API.</p>

<p>```ruby
feature "shop", js: true do</p>

<p>  scenario "list widgets" do
    visit "/"
    click_link "Browse products"
    expect(page).to have_content("Foo Widget")
  end
end
```</p>

<p>Will it work?</p>

<p>```bash
Failures:</p>

<p>  1) shop list widgets
     Failure/Error: expect(page).to have_content("Foo Widget")
       expected to find text "Foo Widget" in ""
     # ./spec/shop_spec.rb:8:in `block (2 levels) in <top (required)>'
```</p>

<p>The web app isn't authenticated to use the API! This brings up a more general question:</p>

<h2>How to bootstrap test data</h2>

<p>Most testing frameworks offer fixtures or direct access to the database. Because the API runs in a separate process, things are a little more difficult. We opt for 1 of 2 approaches, depending on the context:</p>

<ul>
<li><strong>Insert data directly into the API's database.</strong> We tend to do this only as a last resort, because tests would presume knowledge of the API's implementation.</li>
<li><strong>Perform test set-up via the API.</strong> Slightly nicer, and closer to real-life clients. (However, the API must be fairly complete.)</li>
</ul>


<p>In practice, we "cheat" and use direct database-insertion to initially bootstrap an API client application, then perform further test set-up through the API. You should choose what's most convenient.</p>

<p>Our simple example will register the web application as an API client, then pass a key via basic authentication. We'll have to modify the <code>start_web</code> helper:</p>

<p>```ruby
def start_web
  $stderr.puts "Starting web..."
  $web = ChildProcess.build('node', 'app.js')
  $web.cwd = './web'
  $web.io.inherit!
  $api_base_url = "http://#{api_client['key']}:@localhost:#{API_PORT}"
  $web.environment['API_URL'] = $api_base_url
  $web.environment['PORT'] = WEB_PORT
  $web.start
  $stderr.puts "Waiting for web to start listening..."
  sleep(1) while !listening_on?(WEB_PORT) &amp;&amp; $web.alive?
end</p>

<p>def api_client
  $api_client ||= begin
    response = Net::HTTP.post_form(URI("http://localhost:#{API_PORT}/api/clients"), {})
    JSON.parse(response.body)
  end
end
```</p>

<p>And the test will need to set up the data it expects to find listed:</p>

<p>```ruby
feature "shop", js: true do</p>

<p>  scenario "list widgets" do
    create_widget(name: 'Foo Widget', price_cents: 100_00)
    visit "/"
    click_link "Browse products"
    expect(page).to have_content("Foo Widget")
  end
end</p>

<h1>spec/spec_helper.rb</h1>

<p>def create_widget(params = {})
  Net::HTTP.post_form(URI("#{$api_base_url}/api/widgets"), params)
end
```</p>

<p>Lo and behold, our entire "suite" now passes:</p>

<p><code>bash
2 examples, 0 failures
</code></p>

<p>This basic structure has accommodated dozens of test scenarios. We've extended it with database- and cache-clearing between tests, and organized helpers into modules under <code>spec/support</code>. The suite is built nightly against the latest versions of our codebases, and has caught a few significant bugs.</p>

<p>A caveat: with so many layers and dependencies involved, there are often spurious failures. We've picked up a few practices that help:</p>

<ul>
<li><a href="http://artsy.github.io/blog/2012/05/15/how-to-organize-over-3000-rspec-specs-and-retry-test-failures/">Automatic retries</a></li>
<li><a href="http://artsy.github.io/blog/2014/01/30/isolating-spurious-and-nondeterministic-tests/">Quarantine for problematic tests</a></li>
<li><a href="https://github.com/mattheworiordan/capybara-screenshot">Failure screenshots</a></li>
</ul>


<p>You can <a href="https://github.com/joeyAghion/multiapp_example-tests">grab the example code</a>. And make sure to let us know in the comments how <em>you</em> approach testing across applications.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How to Run RSpec Test Suites in Parallel with JenkinsCI Build Flow]]></title>
    <link href="http://artsy.github.io/blog/2012/10/09/how-to-run-rspec-test-suites-in-parallel-with-jenkins-ci-build-flow/"/>
    <updated>2012-10-09T21:21:00-04:00</updated>
    <id>http://artsy.github.io/blog/2012/10/09/how-to-run-rspec-test-suites-in-parallel-with-jenkins-ci-build-flow</id>
    <content type="html"><![CDATA[<p>We now have over 4700 RSpec examples in one of our projects. They are stable, using the techniques described in an <a href="/blog/2012/02/03/reliably-testing-asynchronous-ui-w-slash-rspec-and-capybara/">earlier post</a> and organized in <a href="/blog/2012/05/15/how-to-organize-over-3000-rspec-specs-and-retry-test-failures/">suites</a>. But they now take almost 3 hours to run, which is clearly unacceptable.</p>

<p>To solve this, we have parallelized parts of the process with existing tools, and can turn a build around in just under an hour. This post will dive into our <a href="http://jenkins-ci.org/">Jenkins</a> build flow setup.</p>

<p>To keep things simple, we're going to only build the <code>master</code> branch. When a change is committed on <code>master</code> we're going to push <code>master</code> to a <code>master-ci</code> branch and trigger a distributed build on <code>master-ci</code>. Once all the parts have finished, we'll complete the build by pushing <code>master-ci</code> to <code>master-succeeded</code> and notify the dev team of success or failure.</p>

<p>Here's a diagram of what's going on.</p>

<p><img src="http://artsy.github.io/images/2012-10-09-how-to-run-rspec-test-suites-in-parallel-with-jenkins-ci-build-flow/master-ci.png"></p>

<!-- more -->


<h2>Plugins</h2>

<p>Install the <a href="https://wiki.jenkins-ci.org/display/JENKINS/Build+Flow+Plugin">Build Flow</a> and the <a href="https://wiki.jenkins-ci.org/display/JENKINS/Parameterized+Trigger+Plugin">Parameterized Trigger</a> plugin. Grant <code>Anonymous</code> job read permissions in Jenkins system configuration (see <a href="https://issues.jenkins-ci.org/browse/JENKINS-14027">JENKINS-14027</a>).</p>

<p>Create the following Jenkins jobs.</p>

<h2>master-prequel</h2>

<p>A free-style job that connects to the SCM, in our case Git.</p>

<ul>
<li>Set SCM repository URL to your Git repo, eg. <code>git@github.com:spline/reticulator.git</code></li>
<li>Change the default branch specifier from <code>**</code> to <code>master</code>. We'll be pushing a <code>master-ci</code> branch, which could, in turn, cause more builds if you don't do this.</li>
<li>Add a post-build action to build another project. Trigger the <code>master</code> project if the build succeeds.</li>
</ul>


<h2>master</h2>

<p>This is a build-flow job. We'll describe the individual tasks that the flow invokes further. The flow DSL looks as follows.</p>

<p><code>ruby
build("master-ci-init")
parallel (
 { build("master-ci-task", tasks: "spec:suite:models:ci") },
 { build("master-ci-task", tasks: "spec:suite:api:ci") },
 { build("master-ci-task", tasks: "spec:suite:integration:ci") }
)
build("master-ci-succeeded")
</code></p>

<p>This is a good place to add an e-mail notification post-build action for every unstable build.</p>

<h2>master-ci-init</h2>

<p>A free-style job that creates the <code>master-ci</code> branch from master. It needs to be connected to your SCM and executes the following shell script.</p>

<p>``` bash</p>

<h1>!/bin/bash</h1>

<p>git checkout $GIT_BRANCH
git push origin -f $GIT_BRANCH:$GIT_BRANCH-ci
```</p>

<p>Note that we cannot combine this task with <code>master-prequel</code>, because we have to make sure the branch creation runs once under the entire flow, while <code>master-prequel</code> can be run multiple times, once per check-in. Otherwise the <code>master-ci</code> branch could get updated before a <code>master-ci-task</code> runs from a previous flow execution.</p>

<h2>master-ci-task</h2>

<p>A parameterized build that accepts a <code>tasks</code> parameter that the flow will pass in.</p>

<p>Change the default branch specifier to <code>master-ci</code> and execute the following shell script.</p>

<p>``` bash</p>

<h1>!/bin/bash</h1>

<p>bundle install
bundle exec rake $tasks
```</p>

<p>This example runs <code>rake $tasks</code>, which we define to be various test suites in our flow DSL. Our test suite setup is described in <a href="/blog/2012/05/15/how-to-organize-over-3000-rspec-specs-and-retry-test-failures/">this post</a>. Your mileage may vary.</p>

<h2>master-ci-succeeded</h2>

<p>This is an optional step. We use this free-style job to tag <code>master-ci</code> as <code>master-succeeded</code> with the following shell script.</p>

<p>``` bash</p>

<h1>!/bin/bash</h1>

<p>git checkout $GIT_BRANCH
git push origin -f $GIT_BRANCH:${GIT_BRANCH/%-ci/}-succeeded
```</p>

<p>Our deployment to production will pickup the <code>master-succeeded</code> branch when it's time.</p>

<h2>Improvements?</h2>

<p>I see a few possible improvements here that might require a bit of work.</p>

<ul>
<li>The ability to split an RSpec suite up across an arbitrary number N sub-jobs and M executors would create an optimal parallel split based on the resources available.</li>
<li>Passing the value of <code>GIT_BRANCH</code> and <code>GIT_COMMIT</code> across these jobs would enable building any branch and eliminate the need for <code>master-ci-init</code>.</li>
<li>Build flow could support SCM polling the same way as free-style jobs, avoiding the need for <code>master-prequel</code>. We weren't able to get a stable notification of changes from Github with the Jenkins Github plugin.</li>
</ul>


<p>Please suggest further improvements in the comments below!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[On-Demand Jenkins Slaves with Amazon EC2]]></title>
    <link href="http://artsy.github.io/blog/2012/07/10/on-demand-jenkins-slaves-with-amazon-ec2/"/>
    <updated>2012-07-10T13:30:00-04:00</updated>
    <id>http://artsy.github.io/blog/2012/07/10/on-demand-jenkins-slaves-with-amazon-ec2</id>
    <content type="html"><![CDATA[<p>The <a href="http://artsy.net">Artsy</a> team faithfully uses <a href="http://jenkins-ci.org">Jenkins</a> for continuous integration. <a href="http://artsy.github.com/blog/2012/05/27/using-jenkins-for-ruby-and-ruby-on-rails-teams/">As we've described before</a>, our Jenkins master and 8 slaves run on Linode. This arrangement has at least a few drawbacks:</p>

<ul>
<li>Our Linode servers are manually configured. They require frequent maintenance, and inconsistencies lead to surprising build failures.</li>
<li>The fixed set of slaves don't match the pattern of our build jobs: jobs get backed up during the day, but servers are mostly unused overnight and on weekends.</li>
</ul>


<p>The <a href="https://wiki.jenkins-ci.org/display/JENKINS/Amazon+EC2+Plugin">Amazon EC2 Plugin</a> allowed us to replace those slaves with a totally scripted environment. Now, slaves are spun up in the cloud whenever build jobs need them.</p>

<!-- more -->


<p>To set up the build slave's Amazon Machine Image (AMI), we started from an <a href="http://cloud-images.ubuntu.com/releases/oneiric/release/">official Ubuntu 11.10</a> (Oneiric Ocelot) AMI, ran initialization scripts to set up our build dependencies (MongoDB, Redis, ImageMagick, Firefox, RVM, NVM, etc.), packaged our modified instance into its own AMI, and then set up the EC2 Plugin to launch instances from this custom AMI.</p>

<p>Our AMI setup steps are captured entirely in a <a href="https://gist.github.com/3085368">GitHub gist</a>, but because our build requirements are specific to our applications and frameworks, most organizations will need to modify these scripts to their own use cases. Given that caveat, here's how we went from base Ubuntu AMI to custom build slave AMI:</p>

<ol>
<li>We <a href="https://console.aws.amazon.com/ec2/home?region=us-east-1#launchAmi=ami-4dad7424">launched</a> an Ubuntu 11.10 AMI <code>4dad7424</code> via the AWS console.</li>
<li>Once the instance was launched, we logged in with the SSH key we generated during setup.</li>
<li><p>We ran the following commands to configure the instance:</p>

<pre><code> curl -L https://raw.github.com/gist/3085368/_base-setup.sh | sudo bash -s
 sudo su -l jenkins
 curl -L https://raw.github.com/gist/3085368/_jenkins-user-setup.sh | bash -s
</code></pre></li>
<li><p>From the "Instances" tab of the AWS Console, we chose the now-configured instance, and from the "Instance Actions" dropdown, selected "Stop", followed by "Create Image (EBS AMI)".</p></li>
</ol>


<p>Next we installed the Amazon EC2 Plugin on our Jenkins master, and entered the following configuration arguments for the plugin. (Replace the AMI ID with your own, the result of Step 4 above.)</p>

<p><img src="/images/2012-07-10-on-demand-jenkins-slaves-with-amazon-ec2/ec2-plugin-config.png" title="[Jenkins EC2 Plugin configuration]" ></p>

<p>New build slaves began spawning immediately in response to job demand! Our new "Computers" page on Jenkins looks like this:</p>

<p><img src="/images/2012-07-10-on-demand-jenkins-slaves-with-amazon-ec2/computer-list.png" title="[Jenkins computer list]" ></p>

<p>We have the option of provisioning a new build slave via a single click, but so far, this hasn't been necessary, since slaves have automatically scaled up and down with demand. We average around 4-8 build slaves during the day, and 0-1 overnight and on weekends.</p>

<h2>Outcome and Next Steps</h2>

<p>This arrangement hasn't been in place for long, but we're excited about the benefits it's already delivered:</p>

<ul>
<li>Builds now take a predictable amount of time, since slaves automatically scale up to match demand.</li>
<li>Slaves offer a more consistent and easily maintained configuration, so there are fewer spurious failures.</li>
<li>Despite higher costs on EC2, we hope to spend about the same (or maybe even less) now that we'll need to operate only the master server during periods of inactivity (like nights and weekends).</li>
</ul>


<p>As proponents of <em>automating the hard stuff</em>, we get a real kick out of watching identical slaves spin up as builds trickle in each morning, then disappear as the queue quiets down in the evening. Still, there are a few improvements to be made:</p>

<ul>
<li>Our canonical slave's configuration should be scripted with <a href="http://www.opscode.com/chef/">Chef</a>.</li>
<li>Sharp-eyed readers will notice that our Jenkins master is still a Linode server. It might benefit from the same type of scripted configuration as the slaves.</li>
<li>Cooler still would be for the EC2 plugin to take advantage of Amazon's <a href="http://aws.amazon.com/ec2/spot-instances/">spot pricing</a>. Though not supported at the moment, it would allow us to spend a fraction as much (or spend the same amount, but on more powerful resources).</li>
</ul>

]]></content>
  </entry>
  
</feed>
