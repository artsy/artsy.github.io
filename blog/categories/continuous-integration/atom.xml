<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Continuous Integration | Art.sy Engineering]]></title>
  <link href="http://artsy.github.com/blog/categories/continuous-integration/atom.xml" rel="self"/>
  <link href="http://artsy.github.com/"/>
  <updated>2012-08-15T18:55:00+01:00</updated>
  <id>http://artsy.github.com/</id>
  <author>
    <name><![CDATA[Art.sy]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[On-Demand Jenkins Slaves with Amazon EC2]]></title>
    <link href="http://artsy.github.com/blog/2012/07/10/on-demand-jenkins-slaves-with-amazon-ec2/"/>
    <updated>2012-07-10T13:30:00+01:00</updated>
    <id>http://artsy.github.com/blog/2012/07/10/on-demand-jenkins-slaves-with-amazon-ec2</id>
    <content type="html"><![CDATA[<p>The <a href="http://art.sy">Art.sy</a> team faithfully uses <a href="http://jenkins-ci.org">Jenkins</a> for continuous integration. <a href="http://artsy.github.com/blog/2012/05/27/using-jenkins-for-ruby-and-ruby-on-rails-teams/">As we've described before</a>, our Jenkins master and 8 slaves run on Linode. This arrangement has at least a few drawbacks:</p>

<ul>
<li>Our Linode servers are manually configured. They require frequent maintenance, and inconsistencies lead to surprising build failures.</li>
<li>The fixed set of slaves don't match the pattern of our build jobs: jobs get backed up during the day, but servers are mostly unused overnight and on weekends.</li>
</ul>


<p>The <a href="https://wiki.jenkins-ci.org/display/JENKINS/Amazon+EC2+Plugin">Amazon EC2 Plugin</a> allowed us to replace those slaves with a totally scripted environment. Now, slaves are spun up in the cloud whenever build jobs need them.</p>

<!-- more -->


<p>To set up the build slave's Amazon Machine Image (AMI), we started from an <a href="http://cloud-images.ubuntu.com/releases/oneiric/release/">official Ubuntu 11.10</a> (Oneiric Ocelot) AMI, ran initialization scripts to set up our build dependencies (MongoDB, Redis, ImageMagick, Firefox, RVM, NVM, etc.), packaged our modified instance into its own AMI, and then set up the EC2 Plugin to launch instances from this custom AMI.</p>

<p>Our AMI setup steps are captured entirely in a <a href="https://gist.github.com/3085368">GitHub gist</a>, but because our build requirements are specific to our applications and frameworks, most organizations will need to modify these scripts to their own use cases. Given that caveat, here's how we went from base Ubuntu AMI to custom build slave AMI:</p>

<ol>
<li>We <a href="https://console.aws.amazon.com/ec2/home?region=us-east-1#launchAmi=ami-4dad7424">launched</a> an Ubuntu 11.10 AMI <code>4dad7424</code> via the AWS console.</li>
<li>Once the instance was launched, we logged in with the SSH key we generated during setup.</li>
<li><p>We ran the following commands to configure the instance:</p>

<pre><code> curl -L https://raw.github.com/gist/3085368/_base-setup.sh | sudo bash -s
 sudo su -l jenkins
 curl -L https://raw.github.com/gist/3085368/_jenkins-user-setup.sh | bash -s
</code></pre></li>
<li><p>From the "Instances" tab of the AWS Console, we chose the now-configured instance, and from the "Instance Actions" dropdown, selected "Stop", followed by "Create Image (EBS AMI)".</p></li>
</ol>


<p>Next we installed the Amazon EC2 Plugin on our Jenkins master, and entered the following configuration arguments for the plugin. (Replace the AMI ID with your own, the result of Step 4 above.)</p>

<p><img src="/images/2012-07-10-on-demand-jenkins-slaves-with-amazon-ec2/ec2-plugin-config.png" title="[Jenkins EC2 Plugin configuration]" ></p>

<p>New build slaves began spawning immediately in response to job demand! Our new "Computers" page on Jenkins looks like this:</p>

<p><img src="/images/2012-07-10-on-demand-jenkins-slaves-with-amazon-ec2/computer-list.png" title="[Jenkins computer list]" ></p>

<p>We have the option of provisioning a new build slave via a single click, but so far, this hasn't been necessary, since slaves have automatically scaled up and down with demand. We average around 4-8 build slaves during the day, and 0-1 overnight and on weekends.</p>

<h2>Outcome and Next Steps</h2>

<p>This arrangement hasn't been in place for long, but we're excited about the benefits it's already delivered:</p>

<ul>
<li>Builds now take a predictable amount of time, since slaves automatically scale up to match demand.</li>
<li>Slaves offer a more consistent and easily maintained configuration, so there are fewer spurious failures.</li>
<li>Despite higher costs on EC2, we hope to spend about the same (or maybe even less) now that we'll need to operate only the master server during periods of inactivity (like nights and weekends).</li>
</ul>


<p>As proponents of <em>automating the hard stuff</em>, we get a real kick out of watching identical slaves spin up as builds trickle in each morning, then disappear as the queue quiets down in the evening. Still, there are a few improvements to be made:</p>

<ul>
<li>Our canonical slave's configuration should be scripted with <a href="http://www.opscode.com/chef/">Chef</a>.</li>
<li>Sharp-eyed readers will notice that our Jenkins master is still a Linode server. It might benefit from the same type of scripted configuration as the slaves.</li>
<li>Cooler still would be for the EC2 plugin to take advantage of Amazon's <a href="http://aws.amazon.com/ec2/spot-instances/">spot pricing</a>. Though not supported at the moment, it would allow us to spend a fraction as much (or spend the same amount, but on more powerful resources).</li>
</ul>

]]></content>
  </entry>
  
</feed>
