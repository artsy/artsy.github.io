<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: hokusai | Artsy Engineering]]></title>
  <link href="https://artsy.github.io/blog/categories/hokusai/atom.xml" rel="self"/>
  <link href="https://artsy.github.io/"/>
  <updated>2021-02-18T19:50:12+00:00</updated>
  <id>https://artsy.github.io/</id>
  <author>
    <name><![CDATA[]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Servers for Everyone: Review Apps @ Artsy]]></title>
    <link href="https://artsy.github.io/blog/2020/08/21/review-apps-post/"/>
    <updated>2020-08-21T00:00:00+00:00</updated>
    <id>https://artsy.github.io/blog/2020/08/21/review-apps-post</id>
    <content type="html"><![CDATA[<p>This blog post is going to motivate and describe Artsy's adoption and evolution
of the usage of review apps.</p>

<p>This first part of this post covers a couple of common problems where
topic-specific servers (i.e. review apps) are useful.</p>

<p>The rest of the post describes Artsy's history with review app automation via
incremental problem solving and the composition of a few well-known technologies.</p>

<!-- more -->


<a name="Problem.1.0:.A.Single.Shared.Staging.Environment"></a>
<h2>Problem 1.0: A Single Shared Staging Environment</h2>

<p>At Artsy, we have a sizable engineering organization: ~40 engineers organized
across many teams. Engineers on those teams work on many codebases, some are
exclusive to a team, but many codebases are worked on by engineers across many
teams. Artsy's website (www.artsy.net), <a href="https://github.com/artsy/force">Force</a>, is a good example of such a shared
service.</p>

<p>These different teams of developers working on a shared service have (mostly
hidden) dependencies upon each other, most visible when a shared service is being
deployed to production.</p>

<p>Let's work the following example:</p>

<ul>
<li>Team A is hard at work finishing up a new feature (say a new search
page for a list of art auctions) on Service S and is now doing a final round
of QA before deploying to production</li>
<li>Team B is fixing a bug in an unrelated part of Service S</li>
<li>Team B confirms that the bug was squashed on staging</li>
<li>Team B deploys Service S to production</li>
</ul>


<p>Artsy's production deployment flow is rooted in a GitHub Pull Request, meaning
that the commits that differ between staging (on the <code>staging</code> Git branch) and
production (on the <code>release</code> Git branch) are clear to whomever is managing a
deploy (<a href="https://github.com/artsy/force/pull/6106">example</a>).</p>

<p>While it's great that our deploys are easy to understand, ensuring that a deploy
of a service is safe <em>requires communicating with the teams that contributed
code to ensure that their work is "safe to deploy"</em>.</p>

<p>"Safe to deploy" might mean different things depending on the nature of the
work.</p>

<p>For example, Team A's new list of art auctions might require an API endpoint in
another Service Z to be deployed for their part of the deploy of Service S to
be safe to deploy. Team B's bugfix might just requires a quick visual confirmation.</p>

<p>Suffice to say, it's <em>hard to independently confirm that another team's work is
safe to deploy</em>.</p>

<p>Despite the mitigation strategies discussed next, there's risk of deploying unsafe
code whenever a single staging environment is used across many teams.</p>

<a name="Shared.Staging.Mitigation.Strategies"></a>
<h4>Shared Staging Mitigation Strategies</h4>

<p>There are a couple of ways Artsy mitigates against the possible pitfalls of a
shared staging environment:</p>

<ol>
<li><p>Having a culture of quickly deploying code to production</p>

<p> By building a culture that views frequent deploys positively, there's, on average,
less diff in every deploy, mitigating the risk of unintentionally deploying code
that's not safe to deploy.</p></li>
<li><p>Using automated quality processes geared towards a stable staging environment</p>

<p> We do our best to feel as confident as possible in a change <em>before it is deployed
 to staging</em> by creating automated tests for changes, sharing visual changes over
 Slack and in PRs, and other strategies relevant to the work at hand.</p></li>
<li><p>Communicating deploys</p>

<p> When deploying a service, Artsy engineers typically send a message to our #dev
Slack channel notifying of their plan to deploy a particular service, cc'ing the
engineers that are involved in other PRs that are part of the deploy. In the example
above, an engineer on Team B would notify relevant stakeholders of Team A, giving
that team the opportunity to flag if their work is not yet safe to deploy.</p></li>
</ol>


<p>While these strategies are impactful:</p>

<ul>
<li><p>Semi-unstructured communication is prone to breakdown: the notified engineers
on Team A might be pairing or in a meeting and Team B deploys anyways.</p></li>
<li><p>Without a true continuous delivery model, it's a challenge to operationalize
very frequent production deploys. Moreover, the problem compounds as the team
grows and the velocity of work increases.</p></li>
<li><p>Particularly when working in a large distributed system, automated testing at
the individual service level can only provide a certain level of safety for a
change. Visual changes which require correctness on different viewports and devices
are, pragmatically, often best to test manually.</p></li>
</ul>


<p>If only there was a way to allow Team A and B to work without risking stepping
on each other toes!</p>

<p>We'll discuss how review apps provide this safety, but first another related
problem.</p>

<a name="Problem.2.0:.Complex.Work.Needs.Many.Eyes"></a>
<h2>Problem 2.0: Complex Work Needs Many Eyes</h2>

<p>But before it gets better, it's going to get a bit worse.</p>

<p>While working on any mature distributed system is a challenge, some work is
particularly challenging or risky. Generally, risk is reduced when many
skilled eyes can review the risky thing in an environment that closely mimics
the production environment.</p>

<p>This class of work might include changes to authorization flows, page
redesigns or infrastructural upgrades (e.g. a NodeJS upgrade).</p>

<p>For example, to feel safe deploying <a href="https://github.com/artsy/force/pull/5697">a major version upgrade of Artsy's design
system in Force</a> the most pragmatic way forward was
to deploy that PR to a server where other engineers could collaborate on QA.</p>

<p><em>If a single shared staging environment is the only non-production server to
deploy to, the chances that work lands on staging in an unsafe state is high</em>. While
staging being unsafe isn't <em>itself</em> a bad thing, many bad things can result:</p>

<ol>
<li><p>[Bad] Blocked Deploys</p>

<p> If staging is unsafe and this dangerous state is discovered, then top priority
is getting a service back into a safe state. While working to get a service
back to a healthy state, new features can't be delivered.</p>

<p>  In aggregate, this can be a sizeable productivity loss.</p></li>
<li><p>[Worse] Unsafe Deploys</p>

<p> If staging is unsafe and this dangerous state is not discovered before a production
deploy (for example, the unsafe code might be from another team, as described above),
then end-users might interact with a service that just isn't working. No good.</p></li>
<li><p>[Terrible] Fear</p>

<blockquote><p>Fear is the mind-killer.
 <a href="https://www.goodreads.com/quotes/2-i-must-not-fear-fear-is-the-mind-killer-fear-is">Dune</a></p></blockquote>

<p> Alright, a bit over the top, but the risk of unsafe or blocked deploys can
implicitly or explicitly result in teams shying away from complex work.</p>

<p>  This avoidable fear might result in increased technical debt or not taking on
certain projects.</p>

<p>  It's generally bad for business and does not lead to a pleasant work environment!</p></li>
</ol>


<a name="Problem.Recap..amp..Review.App.Introduction"></a>
<h2>Problem Recap &amp; Review App Introduction</h2>

<p>To recap, there is an increased risk of unsafe or blocked deploys whenever there
is a single staging environment for a shared service. Certain types of
(incredibly useful) changes require interactive review on a live server
before we feel comfortable with those changes, which magnifies the risk of a unsafe or
blocked deploy.</p>

<p>Review apps are simply other staging environments that are easy to spin up and
are deployed with the version of the service that you are working on.</p>

<p>By QA-ing on a review app instead of a shared staging environment, teams can
take their time ensuring the safety of their changes, removing the risks
detailed above.</p>

<p>Larger infrastructure upgrades (including upgrades to the
underlying K8s configuration, in addition to app-level changes) can sit on a
server for hours or days, allowing any slow-moving issue to show itself in a
lower risk environment.</p>

<p>Artsy has iterated on its review app tooling to the point where Team A and Team B
can each deploy their changes to isolated servers of our main website, Force,
on a single <code>git push</code> to a branch matching a naming convention.</p>

<p>The rest of this post describes Artsy's evolution of its review app tooling
and areas for continued improvement.</p>

<a name="Review.App.Tooling"></a>
<h2>Review App Tooling</h2>

<a name="Heroku.Days"></a>
<h3>Heroku Days</h3>

<p>In the beginning, Artsy deployed most services on Heroku. There were fewer engineers
and teams, so the engineering organization was less impacted by the problems described above.</p>

<p><a href="https://devcenter.heroku.com/articles/github-integration-review-apps">Heroku review apps</a> were used on some teams sparingly.</p>

<a name="Enter.Kubernetes.and.Hokusai"></a>
<h3>Enter Kubernetes and Hokusai</h3>

<p>For many reasons outside of the scope of this post, Artsy began migrating
services off of Heroku and onto an AWS-backed Kubernetes (K8s) deployment model
starting in <a href="https://github.com/artsy/force/pull/953">February 2017</a>.</p>

<p>In order to allow application engineers to reasonably interface with K8s-backed
services, Artsy developed a command line interface,
<a href="https://github.com/artsy/hokusai"><code>hokusai</code></a>, to provide a Heroku CLI-like interface for
configuring and deploying these services.</p>

<p>Check out some <a href="https://artsy.github.io/blog/2019/10/18/kubernetes-and-hokusai/">prior</a> <a href="https://artsy.github.io/blog/2018/01/24/kubernetes-and-hokusai/">posts</a> discussing the experience of working with Kubernetes
and <code>hokusai</code>.</p>

<p>About a year after <code>hokusai</code>'s initial release, the tool released <a href="https://github.com/artsy/hokusai/pull/62">its initial
support for review apps</a>.</p>

<p>Via subcommands within the <code>hokusai review_app</code> namespace, developers were able to:</p>

<ul>
<li>Create the needed K8s YAML configuration file from an existing staging configuration file</li>
<li>Execute this configuration: creating a running server within a dedicated namespace</li>
<li>Perform other server management tasks: re-deploying to the server, setting ENV variables, etc</li>
</ul>


<a name="Problem:.More.Steps.Needed"></a>
<h3>Problem: More Steps Needed</h3>

<p>While <code>hokusai</code>'s official review app feature handles much of the core infrastructure
needed to get a service deployed to a new server, additional tasks are required
to have a working review app, which can be categorized into:</p>

<ol>
<li><p>Service Agnostic Tasks</p>

<p> These include:</p>

<ul>
<li>Pushing a Docker image of the Git revision in question to the appropriate
Docker registry</li>
<li>Editing the generated YAML configuration file to reference this Docker image</li>
<li>Sourcing the appropriate ENV variables (typically from the shared staging
server)</li>
</ul>


<p> Check out <a href="https://github.com/artsy/hokusai/blob/master/docs/Review_Apps.md"><code>hokusai</code>'s review app docs</a> for more
 details.</p></li>
<li><p>Service Specific Tasks</p>

<p> In addition, certain services have service-specific operational requirements that
 need to be met before a review app is fully functional.</p>

<p> For example, in Force, we need to:</p>

<ul>
<li>Publish front-end assets to S3 for the specific revision being deployed, and</li>
<li>Tweak some ENV variables from the values copied over from the shared staging
server</li>
</ul>
</li>
</ol>


<p><strong>Impact</strong>: Due to the manual labor required to (re)-learn and execute the
commands needed to build a review app, they were used sparingly by a few engineers
that already invested time in learning up on them.</p>

<a name="Solution:.A.bash.script"></a>
<h3>Solution: A bash script</h3>

<p>While these tasks described above are tedious, they don't really require a
decision-making human behind the computer and can be automated.</p>

<p>In August 2019, we <a href="https://github.com/artsy/force/pull/4412">automated</a> these tasks via a Bash script.</p>

<p><strong>Impact</strong>: A developer is able take a Force commit and get it deployed to K8s
by running a single script on their laptop. Folks became excited about review
apps and started to use them more for Force development.</p>

<a name="Problem:.Depending.upon.a.developer.s.laptop.doesn.t.scale"></a>
<h3>Problem: Depending upon a developer's laptop doesn't scale</h3>

<p>The increased excitement and usage of review apps in Force revealed a new problem:</p>

<p>Building and pushing >2 GB Docker image across home WiFi networks can be incredibly
slow, decreasing the usefulness and adoption of the Bash script.</p>

<a name="Solution:.Run.the.bash.script.on.CI"></a>
<h3>Solution: Run the bash script on CI</h3>

<p>After discussions within Artsy's Platform Practice, a possible solution
emerged: build the review app by running this Bash script on CircleCI upon push
to a branch starting with <code>review-app</code>.</p>

<p>This means that a developer's laptop is then only responsible for pushing a
commit to a branch and CircleCI does all the heavy lifting.</p>

<p>Moreover, the process of centralizing the review app creation into CI helped us realize
the subsequent requirement: updating, not creating, review apps when a review app
already exists for a given branch.</p>

<p>Check out the <a href="https://github.com/artsy/force/pull/5370">pull request</a> for the nitty gritty on how
we leveraged CircleCI branch filtering and more Bash to move this workload into
CircleCI and intelligently determine when to update or create a review app.</p>

<p><strong>Impact</strong>: Any developer can spin up a Force review app in ~15 minutes on a <code>git push</code>.
Review app are being used often for major and minor changes alike.</p>

<a name="Future.Iterations"></a>
<h2>Future Iterations</h2>

<p>Artsy has come far with its tooling for review applications, but, as always,
there's areas for us for to grow in, including:</p>

<ol>
<li><p>Automating the de-provisioning of review apps that no
longer useful.</p></li>
<li><p>Automating the creation of DNS CNAME records within Cloudflare, removing one
final manual step.</p></li>
<li><p>While the improvements to review app infrastructure has sparked similar
investments in other codebases, there's a lot of work we could do to bring
this Git-CircleCI-Bash based approach to other shared services we deploy at
Artsy.</p></li>
</ol>


<a name="On.Incremental.Improvement"></a>
<h2>On Incremental Improvement</h2>

<p>One of Artsy's Engineering Principles is <a href="https://github.com/artsy/README/blob/master/culture/engineering-principles.md#incremental-revolution">"Incremental
Revolution"</a>, which begins with:</p>

<blockquote><p>Introduce new technologies slowly and incrementally.</p></blockquote>

<p>I think Artsy's approach to review apps is a great example of this principle
implemented.</p>

<p>As opposed to finding a silver bullet technology or strategy, our approach has
been to build off of a working current state, layering on a new component to
solve the next problem.</p>

<p>At each point along our solution journey, we're left with a working and more valuable
solution to the problem at hand.</p>

<p>Thanks for reading!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Kubernetes and Hokusai]]></title>
    <link href="https://artsy.github.io/blog/2019/10/18/kubernetes-and-hokusai/"/>
    <updated>2019-10-18T00:00:00+00:00</updated>
    <id>https://artsy.github.io/blog/2019/10/18/kubernetes-and-hokusai</id>
    <content type="html"><![CDATA[<p>When I joined Artsy Engineering a few months ago, I had roughly zero knowledge of Kubernetes. I'd heard the term
thrown around a few times, but had no idea how it worked or what it was used for.</p>

<p>Kubernetes is still a bit of a mystery to me, but I'm able to do a lot of Kubernetes operations quickly and easily
thanks to an open-source tool developed at Artsy: <a href="https://github.com/artsy/hokusai">Hokusai</a>.</p>

<p>In this post, I'll give some background on <a href="https://kubernetes.io">Kubernetes</a>, a brief history of Hokusai, a
description of its functionality, and some pointers for how to get started using it.</p>

<!-- more -->


<a name="What.is.Kubernetes."></a>
<h1>What is Kubernetes?</h1>

<p>On a high level, Kubernetes is a tool designed to <em>orchestrate containers at scale.</em></p>

<p>Let's break that down a bit. First, some helpful vocab:</p>

<p><strong>Container</strong>: Effectively code + all necessary dependencies for an application. A
<a href="https://www.docker.com/resources/what-container">"standardized unit of software"</a>.</p>

<p><strong>Pods</strong>: A group of one or more containers. One container per pod is the most common use case.</p>

<p><strong>Deployment</strong>: A Kubernetes component (read: program) that provides declarative updates to pods and manages their
lifecycles (i.e. creating new pods when new code is rolled out, rolling back to an earlier state, scaling up to
more pods, etc.).</p>

<p><strong>Node</strong>: A physical or virtual machine that runs a pod or pods.</p>

<p><strong>Cluster</strong>: A node or group of nodes.</p>

<p><strong>Container orchestration</strong>: A systemized approach to managing containers. Allows for things like auto-scaling,
easy rollouts and rollbacks, and automation of container downtime (i.e. something goes wrong in your process and
causes your app to crash; a new container gets spun up immediately so that your app doesn't go down).</p>

<p>Sources: <a href="https://kubernetes.io/docs/concepts/overview/what-is-kubernetes/">Kubernetes docs</a>,
<a href="https://www.infoworld.com/article/3268073/what-is-kubernetes-your-next-application-platform.html">Infoworld</a>,
<a href="https://www.docker.com/resources/what-container">Docker docs</a></p>

<p>Kubernetes, in a general sense, allows you to configure the containers in which your application will run. With a
properly configured Kubernetes cluster, this makes it easy to scale applications up or down as needed to deal with
traffic patters, maintain a zero-downtime deployment, and more. Very cool.</p>

<p>To sum up the structure of applications running on Kubernetes: clusters contain nodes which contain pods (which are
managed by deployments) which contain containers. This can be tricky to wrap your head around without
experimentation and personal experience—Hokusai aims to simplify the ways in which a developer can interact with
applications running on Kubernetes.</p>

<a name="What.is.Hokusai."></a>
<h1>What is Hokusai?</h1>

<p>When Artsy's Engineering team was contemplating a move to Kubernetes from Heroku, we had beef with a few things.</p>

<p>For one, we wanted to be able to do a few core things simply and easily using the command line. While Kubernetes
has a robust API and CLI tooling using <a href="https://kubernetes.io/docs/reference/kubectl/overview/">kubectl</a>, it's also
very complex. We wanted to be able to quickly and easily do the things we were used to doing with Heroku; we
preferred <code>heroku logs</code> to <code>kubectl logs [POD]</code> (where we would have to either look up or know the specific pod
name we wanted, even though pods are being spun up and taken down all the time).</p>

<p><a href="https://helm.sh">Helm</a>, the de-facto package manager for Kubernetes, also didn't quite fit our needs. Helm is
great for big, complex implementations with Kubernetes, and it's very useful for managing releases. Artsy
Engineering wanted something that didn't involve quite as many complex charts, and we're not as concerned as some
orgs with versioned releases since our focus is mostly on web apps.</p>

<p>Basically, we wanted our commands to be application-level instead of pod- or node-level. We wanted a little more
abstraction than was offered by <code>kubectl</code>, and a little less than Helm.</p>

<p>And there was the issue of review apps. Review apps are basically standalone versions of an application that fall
completely outside a normal production pipeline. They allow you to test big or scary changes in functionality
without even putting them on a staging instance (which could affect other developers' work or be deployed
accidentally).</p>

<p>Kubernetes doesn't support review apps out of the box. There are some add-ons that offer them, but at the time
Artsy was looking to switch, I don't think they existed or were widespread.</p>

<p>Thus was born Hokusai: a tool that makes interacting with applications deployed on Kubernetes from the command line
simple. Need logs? <code>hokusai production logs</code>. Need to run a rake task? <code>hokusai staging run 'rake db:migrate'</code>. Or
want to set up a review app? There are a
<a href="https://github.com/artsy/hokusai/blob/master/docs/Review_Apps.md">few steps involved</a>, but you can have a
fully-featured copy of your app up and running in a few minutes.</p>

<p>The end of this post has a larger cheatsheet for handy Hokusai commands, but for now, let's talk about how you can
use it yourself.</p>

<a name="How.can.I.set.up.Hokusai.with.my.project."></a>
<h1>How can I set up Hokusai with my project?</h1>

<p>I should begin by noting that Hokusai is developed to work with AWS—if your application is running on a different
provider, you might have to hold off on Hokusai for now :( (or
<a href="https://github.com/artsy/hokusai">open a PR in Hokusai</a> yourself!) We do aim to support more clouds in the future,
and Hokusai mostly interacts directly with Kubernetes or Docker APIs.</p>

<p>Installing hokusai is super easy! You can see full instructions in the README on
<a href="https://github.com/artsy/hokusai">GitHub</a>, but if you're already set up with Python, pip, Docker, Docker Compose,
and Git, you can do a quick install of Hokusai packed by <a href="https://www.pyinstaller.org/">PyInstaller</a> with Homebrew:</p>

<pre><code>$ brew tap artsy/formulas
$ brew install hokusai
</code></pre>

<p>There's more robust directions
<a href="https://github.com/artsy/hokusai/blob/master/docs/Getting_Started.md">in the Hokusai repo</a>, but the very short
version is that <code>hokusai setup</code> handles most of the basics (creation of a Dockerfile, a config folder, and a few
other bits and bobs). From there, you can customize according to the needs of your project. It's also possible to
write boilerplate templates to share with developers in your organization—you can see Artsy's
<a href="https://github.com/artsy/artsy-hokusai-templates">here</a>.</p>

<p>You should also check out Ash's <a href="https://artsy.github.io/blog/2018/01/24/kubernetes-and-hokusai/">great post</a> on
setting up a new Hokusai project—he runs through the process of setting up a new Rails application with Hokusai in
an easy-to-follow way that also details small hitches he ran into along the way.</p>

<a name="What.s.next.for.Hokusai."></a>
<h1>What's next for Hokusai?</h1>

<p>As Hokusai has grown and changed over the years (the GH repo was created in November 2016!), a few things have
changed.</p>

<p>For one, it's been increasingly used in coordination with CircleCI. Hokusai has made it really easy to standardize
a lot of application configuration across Artsy's applications. We have
<a href="https://github.com/artsy/orbs/blob/master/src/hokusai">CircleCI orbs</a> set up for Hokusai specifically, which
standardize the way Hokusai is invoked in our CI, among other things. Given how helpful it's been to have a single
source of CircleCI config for many of our apps, we're pondering the idea of a central source for Kubernetes Hokusai
config. In other words, we'd like to have a "baseline" for things like deployments—something that could be
overriden as necessary in specific projects but would make spinning up new projects easy. This would effectively
allow Hokusai to support functionality similar to Helm's <a href="https://helm.sh/docs/chart_template_guide/">templates</a>,
but in a way that can be consumed across project repos.</p>

<a name="Hokusai.and.beyond"></a>
<h1>Hokusai and beyond</h1>

<p>Personally, Hokusai has been very useful to me as a kind of "training wheels" for Kubernetes. To be able to quickly
and easily start interacting with Kubernetes, even as a complete Kubernetes noob, was very empowering and helped me
be less intimidated by it. As I've spent more time interacting with Hokusai, I've started to understand what's
going on behind the scenes, and I've found myself poking around in the Kubernetes docs more than once. I'm excited
to keep learning more about Kubernetes and to start contributing to Hokusai!</p>

<p>Hokusai significantly lowers the barriers to interacting with Kubernetes apps and centralizes the complexity
inherent in doing so. It's been invaluable in transitioning our engineering team to working with Kubernetes. If you
or your organization are in the midst of a similar transition—or if you have a sharp divide in Kubernetes knowledge
and comfort within your team—we suggest giving it a try! Our issues are open for bug reports and feature requests,
and we certainly welcome PRs with improvements.</p>

<a name="Appendix.A:.Useful.Hokusai.commands"></a>
<h2>Appendix A: Useful Hokusai commands</h2>

<p>These are the commands I find myself using on a regular basis. If you're playing around with Hokusai, you can also
run most commands with <code>--help</code> to get more information on their usage.</p>

<ul>
<li><code>hokusai [production|staging] env get</code>: Print all of the environment variables from your application's pod</li>
<li><code>hokusai [production|staging] env set "ENV=value"</code>: Set an environment variable on your application's pod</li>
<li><code>hokusai [production|staging] run 'rake db:migrate'</code>: run a Rails migration</li>
<li><code>hokusai [production|staging] run 'bundle exec rails c' --tty</code>: Open a Rails console for your app (I have this
one aliased to <code>hokusai-[production|staging]-console</code>)</li>
<li><code>hokusai [production|staging] refresh</code>: Refresh the application's deployment by recreating its containers</li>
<li><code>hokusai build</code>: Build your application's Docker image as defined in a <code>hokusai/build.yml</code> file</li>
<li><code>hokusai test</code>: Boot a test environment and run a test suite as defined in <code>hokusai/test.yml</code></li>
<li><code>hokusai pipeline gitcompare --org-name [org]</code>: Spits out a URL for a git comparison between production and
staging images</li>
<li><code>hokusai pipeline gitlog</code>: Print a git log for commits between the image deployed on production and the image on
staging. Handy if you need to get the SHA of a staged commit quickly, e.g. for rollback purposes (?)</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[In the 'Whelp!' of the Great Wave]]></title>
    <link href="https://artsy.github.io/blog/2018/01/24/kubernetes-and-hokusai/"/>
    <updated>2018-01-24T00:00:00+00:00</updated>
    <id>https://artsy.github.io/blog/2018/01/24/kubernetes-and-hokusai</id>
    <content type="html"><![CDATA[<p>This past week has found me working on a brand new Rails project. Now, if I was building this project for my personal needs, I would without a doubt deploy it to <a href="https://www.heroku.com">Heroku</a> – for both the ease of use and the high level of abstraction that <a href="https://www.heroku.com/dynos">Dynos</a> afford. But I'm not building this for myself, I'm building it for my team.</p>

<!-- more -->


<p>While Heroku is easy to get started with, costs scale up quickly. And, as described in our <a href="http://artsy.github.io/blog/2017/04/14/artsy-technology-stack-2017/">2017 tech stack post</a>, our team is moving more and more towards <a href="https://kubernetes.io">Kubernetes</a>. I had almost no experience with Kubernetes before last week, and I was intimidated by the Kubernetes web UI. With some help from my colleague Isac, who wrote the <a href="https://github.com/artsy/hokusai">Hokusai</a> tool, I was able to get a staging environment up and running in under a day.</p>

<p>But let's step back first.</p>

<p>My background is in iOS software development, so spinning up new servers isn't something I do often. When I <em>do</em>, I usually use Heroku. After deploying to it, it feels like Kubernetes is a kind of hosted Heroku: it handles scaling up instances, managing worker/db/other instances, load-balancers, environment variables, promoting from staging to production – all that stuff. But Kubernetes' sophistication comes with a sophisticated user interface.</p>

<p>So basically, Hokusai is to Kubernetes what the Heroku command-line tool is to the Heroku platform.</p>

<p>Hokusai provides <a href="https://github.com/artsy/hokusai/blob/master/docs/Command_Reference.md">a bunch of commands</a> for interacting with the Kubernetes cluster. Deploying my new Rails app to Kubernetes involved a few steps, but most of the work was handled automatically by Hokusai.</p>

<p>First, I installed and setup Hokusai locally (with required environment variables for AWS access). I then ran the following command to scaffold out everything.</p>

<pre><code class="sh">hokusai setup --aws-account-id ARTSY_ACCOUNT_ID --project-type ruby-rails
</code></pre>

<p>In addition to staging- and production-specific config files, this command creates a <code>Dockerfile</code>. See, where Heroku uses Dynos as a high level of abstraction, Kubernetes uses <a href="https://www.docker.com">Docker</a> images (as a slightly less high a level of abstraction). Docker is a technology I'm familiar with, and I managed to configure the generated <code>Dockerfile</code> and <code>hokusai/*.yml</code> config files pretty quickly. At this point, I could run <code>hokusai dev start</code> to start a development Docker container, or <code>hokusai test</code> to run RSpec tests. Nothing fancy yet, but that verifies that everything is working so far.</p>

<p>Next up was to use Hokusai in our CI environment. <a href="https://circleci.com/docs/2.0/">Circle CI 2.0</a> is very Docker-oriented, so we set up everything using their <a href="https://circleci.com/docs/2.0/workflows/">Workflows</a>. This is a much higher level of abstraction for CI configuration than I'm used to, but I got the hang of it quickly. I created a job to run RSpec tests through Hokusai, a job to run <a href="http://danger.systems">Danger</a>, a job to build and push a Docker image to our S3 bucket, and a job to deploy that image to the Kubernetes cluster. Finally, I added the workflows to build and deploy automatically after successful builds on the <code>master</code> branch.</p>

<p>Here's a slightly redacted copy of our Circle config:</p>

<pre><code class="yaml">version: 2
jobs:
  test:
    docker:
      - image: artsy/hokusai:0.4.0
    working_directory: ~/REPO_NAME
    steps:
      - add_ssh_keys
      - checkout
      - setup_remote_docker
      - run:
          name: Test
          command: hokusai test
  danger:
    docker:
      - image: circleci/ruby:2.5.0
    working_directory: ~/apogee
    steps:
      - checkout
      - restore_cache:
          keys:
          - v1-dependencies-
          - v1-dependencies-
      - run:
          name: Install Dependencies
          command: bundle install --with=ci --without development test --path vendor/bundle
      - save_cache:
          paths:
            - ./vendor/bundle
          key: v1-dependencies-
      - run:
          name: Danger
          command: bundle exec danger
  push:
    docker:
      - image: artsy/hokusai:0.4.0
    steps:
      - add_ssh_keys
      - checkout
      - setup_remote_docker
      - run:
          name: Push
          command: hokusai registry push --tag $CIRCLE_SHA1 --force --overwrite
  deploy:
    docker:
      - image: artsy/hokusai:0.4.0
    steps:
      - add_ssh_keys
      - checkout
      - run:
          name: Configure
          command: hokusai configure --kubectl-version 1.6.3 --s3-bucket BUCKET_NAME --s3-key k8s/config --platform linux
      - run:
          name: Deploy
          command: hokusai staging deploy $CIRCLE_SHA1
workflows:
  version: 2
  default:
    jobs:
      - test
      - danger:
          filters:
            branches:
              ignore: master
      - push:
          filters:
            branches:
              only: master
          requires:
            - test
      - deploy:
          filters:
            branches:
              only: master
          requires:
            - push
</code></pre>

<p>The initial build on <code>master</code> built and pushed the server image, but the deploy failed. This is an <a href="https://github.com/artsy/hokusai/issues/50">issue</a> that's being tracked in Hokusai – I'm sure it'll get addressed on the road to a 1.0. To explain, it's a Catch-22: we can't deploy until we have an image, but we only want to build images on CI, so the first deploy on CI is expected to fail.</p>

<p>Once the initial image was pushed, I ran <code>hokusai staging env create</code> locally to create the staging environment. I was able to set staging environment variables using <code>hokusai staging env set NAME=VALUE</code>, but unlike Heroku, I had to manually restart the server using <code>hokusai staging refresh</code> after adding the environment variables.</p>

<p>At this point, my server was working behind a load balancer, but I still had to add a CNAME record for the <code>really-long-url.elb.amazonaws.com</code> domain name. After some DNS propagation, everything worked fine!</p>

<p>So that's it! I was apprehensive about moving to a totally new (to me) deploy infrastructure. But, it's a direction our engineering team has decided to go in, and there's no better time to migrate to a new deploy infrastructure than before your first deploy. With some encouragement and help from my team, I was able to get the entire thing working in under a day (next time will be a lot faster).</p>

<p>I'm very encouraged by Kubernetes. It offers really slick, enterprise-level scaling features in an open source tool. And I've heard really great things about its community practices. Kubernetes is, however, a very specialized tool and its web interface doesn't make any sense to me. With Hokusai, I got a very programmer-friendly interface for a very DevOps-focused tool.</p>
]]></content>
  </entry>
  
</feed>
