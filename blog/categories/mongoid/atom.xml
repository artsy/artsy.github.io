<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Mongoid | Art.sy Engineering]]></title>
  <link href="http://artsy.github.com/blog/categories/mongoid/atom.xml" rel="self"/>
  <link href="http://artsy.github.com/"/>
  <updated>2013-02-09T13:44:12-05:00</updated>
  <id>http://artsy.github.com/</id>
  <author>
    <name><![CDATA[Art.sy]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Data Corruption and Concurrent Updates to Embedded Objects with MongoDB]]></title>
    <link href="http://artsy.github.com/blog/2013/02/09/data-corruption-and-concurrent-updates-to-embedded-objects-with-mongoid/"/>
    <updated>2013-02-09T21:21:00-05:00</updated>
    <id>http://artsy.github.com/blog/2013/02/09/data-corruption-and-concurrent-updates-to-embedded-objects-with-mongoid</id>
    <content type="html"><![CDATA[<p>We use <a href="http://www.mongodb.org/">MongoDB</a> at Artsy as our primary data store via the <a href="http://mongoid.org/">Mongoid ODM</a>. Eventually, we started noticing data corruption inside embedded objects at an alarming rate of 2-3 records a day. The number of occurrences increased rapidly with load as our user growth accelerated.</p>

<p>The root cause was not a HN-worthy sensational declaration about how MongoDB trashes data, but our lack of understanding of what can and cannot be concurrently written to the database, neatly hidden behind the object data mapping layer.</p>

<!-- more -->


<h3>Data Model</h3>

<p>Consider the following artwork model with embedded images.</p>

<p>```ruby
class Artwork
  include Mongoid::Document
  field :title, type: String
  embeds_many :images
end</p>

<p>class Image
  include Mongoid::Document
  embedded_in :artwork
  field :filename, type: String
  field :width, type: Integer<br/>
  field :height, type: Integer
end
```</p>

<p>Let's create a few objects and examine the database queries executed when constructing this relationship by setting a <code>DEBUG</code> logger level on the Moped driver used underneath the ODM.</p>

<p>```ruby
Moped.logger = Logger.new($stdout)
Moped.logger.level = Logger::DEBUG</p>

<h1>db.artworks.insert({</h1>

<h1>_id: ObjectId("510f22c5db8e540aab000001"),</h1>

<h1>title: "Mona Lisa"</h1>

<h1>})</h1>

<p>artwork = Artwork.create!(title: "Mona Lisa")</p>

<p>image1 = Image.new(filename: "framed.jpg")</p>

<h1>db.artworks.update(</h1>

<h1>{ _id: ObjectId("510f22c5db8e540aab000001") },</h1>

<h1>{ $push :</h1>

<h1>{ images:</h1>

<h1>{</h1>

<h1>_id: ObjectId("510f22c5db8e540aab000002"),</h1>

<h1>filename: "framed.jpg"</h1>

<h1>}</h1>

<h1>}</h1>

<h1>}</h1>

<h1>)</h1>

<p>artwork.images &lt;&lt; image1</p>

<p>image2 = Image.new(filename: "unframed.jpg")</p>

<h1>db.artworks.update(</h1>

<h1>{ _id: ObjectId("510f22c5db8e540aab000001") },</h1>

<h1>{ $push :</h1>

<h1>{ images:</h1>

<h1>{</h1>

<h1>_id: ObjectId("510f22c5db8e540aab000003"),</h1>

<h1>filename: "unframed.jpg"</h1>

<h1>}</h1>

<h1>}</h1>

<h1>}</h1>

<h1>)</h1>

<p>artwork.images &lt;&lt; image2
```</p>

<p>Here's the artwork data in MongoDB retrieved from a <code>mongo</code> shell:</p>

<p>```</p>

<blockquote><p>db.artworks.findOne()
{
  "_id" : ObjectId("510f22c5db8e540aab000001"),
  "title" : "Mona Lisa",
  "images" : [</p>

<pre><code>{
  "_id" : ObjectId("510f22c5db8e540aab000002"),
  "filename" : "framed.jpg"
},
{
  "_id" : ObjectId("510f22c5db8e540aab000003"),
  "filename" : "unframed.jpg"
}
</code></pre>

<p>  ]
}
```</p></blockquote>

<p>We can modify the attributes of the second image.</p>

<p>```ruby</p>

<h1>db.artworks.update(</h1>

<h1>{ _id: ObjectId("510f22c5db8e540aab000001") },</h1>

<h1>{ $set : { "images.1.width" : 30, "images.1.height" : 40 } }</h1>

<h1>)</h1>

<p>image2.update_attributes!(width: 30, height: 40)
```</p>

<p>The image has been updated correctly.</p>

<p>```</p>

<blockquote><p>db.artworks.findOne()
{
  "_id" : ObjectId("510f22c5db8e540aab000001"),
  "title" : "Mona Lisa",
  "images" : [</p>

<pre><code>{
  "_id" : ObjectId("510f22c5db8e540aab000002"),
  "filename" : "framed.jpg"
},
{
  "_id" : ObjectId("510f22c5db8e540aab000003"),
  "filename" : "unframed.jpg",
  "height" : 40,
  "width" : 30
}
</code></pre>

<p>  ]
}
```</p></blockquote>

<h3>Incomplete Record Corruption</h3>

<p>Examining the query you will notice that it uses a so-called "positional" operator, <code>images.1.width</code> to update the second record. Imagine what would happen if the first record was deleted from another process immediately before the update. That's right, the update will be performed on a record that doesn't exist, in which case the default MongoDB behavior is to create it!</p>

<p>We can simulate this by loading the object in Ruby, pulling the first record directly from the database and then performing the update.</p>

<p>```ruby
artwork.images &lt;&lt; image2</p>

<h1>pull the first artwork directly from the database</h1>

<p>Artwork.collection.where(<em>id: artwork.id).update(
  "$pull" => { "images" => { </em>id: image1.id } })</p>

<p>image2.update_attributes!(width: 30, height: 40)
```</p>

<p>This yields a nasty surprise. We now have two records in the embedded collection, the second one missing an <code>_id</code>.</p>

<p>```</p>

<blockquote><p>db.artworks.findOne()
{
  "_id" : ObjectId("510f22c5db8e540aab000001"),
  "title" : "Mona Lisa",
  "images" : [</p>

<pre><code>{
  "_id" : ObjectId("510f22c5db8e540aab000003"),
  "filename" : "unframed.jpg"
},
{
  "height" : 40,
  "width" : 30
}
</code></pre>

<p>  ]
}
```</p></blockquote>

<p>When reloaded, Mongoid will assign an automatic <code>_id</code> to the second object, the correct height and width, but no filename.</p>

<h3>Null Record Corruption</h3>

<p>A similar scenario can play out by pulling both image records out of the embedded collection and making a positional update. This will create a <code>null</code> record, which is much worse, because Mongoid can't even destroy it, attempting to pull a record with an <code>_id</code> that does not exist.</p>

<p>```ruby
artwork.images &lt;&lt; image2</p>

<p>Artwork.collection.where(<em>id: artwork.id).update(
  "$pull" => { "images" => { </em>id: image1.id } })
Artwork.collection.where(<em>id: artwork.id).update(
  "$pull" => { "images" => { </em>id: image2.id } })</p>

<p>image2.update_attributes!(width: 30, height: 40)
```</p>

<p>```</p>

<blockquote><p>db.artworks.findOne()
{
  "_id" : ObjectId("510f22c5db8e540aab000001"),
  "title" : "Mona Lisa"
  "images" : [</p>

<pre><code>null,
{
  "height" : 40,
  "width" : 30
}
</code></pre>

<p>  ],
}
```</p></blockquote>

<h3>Solutions</h3>

<p>A first obvious solution is not to use embedded objects or to never modify them. Both <code>$push</code> and <code>$pull</code> are atomic operations, but not the positional update.</p>

<p>A general solution to this problem is to make all update operations transactional. You can take a lock on the parent model by using <a href="https://github.com/afeld/mongoid-locker">mongoid-locker</a>. It works, but can be quite tedious depending on the complexity of your application.</p>

<p>Finally, MongoDB supports something called a "positional operator" for embedded objects. This means you can atomically update a record found by its embedded object's field using a reference to the position of that embedded object. This solves our problem, as long as the object is not embedded below the first level. Mongoid 3.1 (currently HEAD) implements this behavior by default (see <a href="https://github.com/mongoid/mongoid/issues/2545">#2545</a> for details), adjusting the selector to look for the embedded object's <code>_id</code> and replacing the position with a <code>$</code> positional operator.</p>

<p>```ruby</p>

<h1>db.artworks.update(</h1>

<h1>{</h1>

<h1>_id: ObjectId("510f22c5db8e540aab000001"),</h1>

<h1>"images._id" : ObjectId("510f22c5db8e540aab000003")</h1>

<h1>},</h1>

<h1>{ $set : { "images.$.width" : 30, "images.$.height" : 40 }}</h1>

<h1>)</h1>

<p>image2.update_attributes!(width: 30, height: 40)
```</p>

<p>We've been successfully running this in production for a few weeks now, without any more data corruption issues.</p>

<p>While this is a huge step forward, covering all of our application's scenarios, we would like complete native support for atomic updates inside MongoDB at all levels of nesting. Please add your +1 to <a href="https://jira.mongodb.org/browse/SERVER-831">SERVER-831</a>.</p>

<h3>Links</h3>

<ul>
<li><a href="https://gist.github.com/dblock/4699070">Code to Detect Corrupt Embedded Objects</a></li>
<li><a href="https://jira.mongodb.org/browse/SERVER-831">MongoDB SERVER-831: Positional Operator Matching Nested Arrays</a></li>
<li><a href="https://github.com/mongoid/mongoid/issues/2545">Mongoid #2545: Use $ Positional Operator for Updating Embedded Documents</a></li>
<li><a href="https://github.com/dblock/mongoid/tree/master-issues/spec/dblock">Repro Specs for Mongoid #2545 and Similar</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Create MongoDB Command-Lines from Mongoid Configuration]]></title>
    <link href="http://artsy.github.com/blog/2013/01/31/create-mongodb-command-lines-with-mongo/"/>
    <updated>2013-01-31T21:21:00-05:00</updated>
    <id>http://artsy.github.com/blog/2013/01/31/create-mongodb-command-lines-with-mongo</id>
    <content type="html"><![CDATA[<p>We use MongoDB as our primary store and have built a healthy amount of automation around various database instances and operational environments. For example, we backup databases to S3 using <code>mongodump</code>, mirror data between instances with <code>mongorestore</code> and often need to open a MongoDB shell with <code>mongo</code> to examine data at the lowest level.</p>

<p>Generating MongoDB command-lines is tedious and error-prone. Introducing a new gem called <a href="https://github.com/dblock/mongoid-shell">mongoid-shell</a> to help with this. The library can generate command-lines for various MongoDB shell tools from your Mongoid configuration.</p>

<p>For example, connect to your production MongoDB instance from a <code>db:production:shell</code> Rake task.</p>

<p>```ruby
namespace :db
  namespace :production</p>

<pre><code>task :shell
  Mongoid.load! "mongoid.yml", :production
  system Mongoid::Shell::Commands::Mongo.new.to_s
end
</code></pre>

<p>  end
end
```</p>

<!-- more -->


<p>Commands can be created for the current default session or you can pass a session as an argument to a new command.</p>

<p>```ruby</p>

<h1>will use Mongoid.default_session</h1>

<p>Mongoid::Shell::Commands::Mongodump.new</p>

<h1>use a hand-crafted session</h1>

<p>s = Moped::Session.new([ "127.0.0.1:27017" ])
Mongoid::Shell::Commands::Mongodump.new(session: s)
```</p>

<p>Commands accept parameters. Here's how to backup <code>my_database</code> to <code>/tmp/db_backup</code>.</p>

<p>```ruby
out = File.join(Dir.tmpdir, 'db_backup')
db = 'my_database'
dump = Mongoid::Shell::Commands::Mongodump.new(db: db, out: out)</p>

<h1>mongodump --db my_database --out /tmp/db_backup</h1>

<p>system dump.to_s
```</p>

<p>The mongoid-shell gem currently supports <code>mongo</code>, <code>mongodump</code>, <code>mongorestore</code> and <code>mongostat</code> and various MongoDB configurations, including replica-sets.</p>

<p>Please note that we don't recommend you store passwords for production environments in your <code>mongoid.yml</code>. At Artsy, we set all sensitive values directly on our Heroku instances with <code>heroku config:add</code> and use <a href="https://github.com/dblock/heroku-commander">heroku-commander</a> to retrieve these settings in rake. We also have a bit of convention in our application name, such as "app-staging" and "app-production".</p>

<p>Here's a complete Rake task that dynamically fetches Heroku configuration and opens a MongoDB shell on a production or staging environment.</p>

<p>```ruby
namespace :db do
  [ :staging, :production ].each do |env|</p>

<pre><code>namespace env do
  task :shell do
    app = "myapp-#{env}"
    config = Heroku::Commander.new(app: app).config
    config.each_pair do |k, v|
      ENV[k] = v
    end
    mongoid_yml = File.join(Rails.root, "config/mongoid.yml")
    Mongoid.load! mongoid_yml, env
    system Mongoid::Shell::Commands::Mongo.new.to_s
  end
end
</code></pre>

<p>  end
end
```</p>

<p>Run <code>rake db:staging:shell</code> or <code>rake db:production:shell</code>, which works as long as you have access to the Heroku app itself. A bonus feature is that the mongoid-shell gem will automatically connect to the primary node of a replica-set.</p>

<p>```
$ rake db:staging:shell
 mongo db:10007/app-staging --username user --password ************
 MongoDB shell version: 2.0.7
 connecting to: db:10007/app-staging</p>

<blockquote><p>```</p></blockquote>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Improving Performance of Mongoid-Cached-Json]]></title>
    <link href="http://artsy.github.com/blog/2013/01/20/improving-performance-of-mongoid-cached-json/"/>
    <updated>2013-01-20T21:21:00-05:00</updated>
    <id>http://artsy.github.com/blog/2013/01/20/improving-performance-of-mongoid-cached-json</id>
    <content type="html"><![CDATA[<p>Last year, we have open-sourced and made extensive use of two Ruby libraries in our API: <a href="https://github.com/dblock/mongoid-cached-json">mongoid-cached-json</a> and <a href="https://github.com/artsy/garner">garner</a>. Both transform the procedural nightmare of caching and JSON generation into a declarative and easily manageable DSL. It was worth the investment, since our service spends half of its time generating JSON and reading from and writing to Memcached.</p>

<p>Today we've released mongoid-cached-json 1.4 with two interesting performance improvements.</p>

<!-- more -->


<h2>Bulk Reference Resolving with a Local Cache</h2>

<p>Consider an array of database model instances, each with numerous references to other objects. It's typical to see such instances reference the same object: for example we have an <code>Artwork</code> that references an <code>Artist</code>. It's common to see multiple artworks reference the same artist in a collection. Retrieving the artist from cache every time it is referenced is clearly inefficient.</p>

<p><code>Mongoid::CachedJson</code> will now collect all JSON references, then resolve them after suppressing duplicates, in-place within the JSON tree. This significantly reduces the number of cache queries.</p>

<p>Note, that while this optimization reduces load on the Memcached servers, there's a cost of doing additional work after collecting the entire JSON in Ruby.</p>

<h2>Fetching Cache Data in Bulk</h2>

<p>Various cache stores, including Memcached, support bulk read operations. The <a href="https://github.com/mperham/dalli">Dalli</a> gem, which we use in production, exposes this via the <code>read_multi</code> method. With the bulk reference optimization above we now have the entire list of keys to query from cache, at once. <code>Mongoid::CachedJson</code> will always invoke <code>read_multi</code> where available, which significantly reduces the number of network roundtrips to the cache servers.</p>

<p>This is a good example of where declarative models and DSLs have tremendous advantages in enabling massive improvements across the board. Imagine making the <code>read_multi</code> optimization in hundreds of API endpoints!</p>

<h2>Benchmarks</h2>

<p>With the above optimizations the library does more work in order to make less roundtrips to Memcached over the network. Since the network is often the slowest part in any large scale system, the overall production performance should be better as long as we can obtain similar throughput in ideal network conditions on localhost. We've added some common case benchmarks in <a href="https://github.com/dblock/mongoid-cached-json/blob/master/spec/benchmark_spec.rb">spec/benchmark_spec.rb</a> and ran them against 1.2.3 and 1.4.0 to obtain <a href="https://gist.github.com/4583039">these results</a>. The overall performance gain averaged 14.6%, which is quite significant. With real world data in a production environment we're seeing 15-50% less time spent in Memcached, depending on the API.</p>

<h2>Links</h2>

<p>The concepts behind these improvements should be attributed to <a href="https://github.com/aaw">@aaw</a> and <a href="https://github.com/macreery">@macreery</a>. If you want to learn more about the above-mentioned libraries, check out the following links:</p>

<ul>
<li><a href="http://confreaks.com/videos/986-goruco2012-from-zero-to-api-cache-w-grape-mongodb-in-10-minutes">From Zero to API-Cache w/ Grape and MongoDB</a>, video recorded at GoRuCo</li>
<li><a href="/blog/2012/02/20/caching-model-json-with-mongoid-cached-json/">Caching Model JSON with Mongoid-Cached-Json</a></li>
<li><a href="/blog/2012/03/23/simplifying-model-level-json-versioning-with-mongoid-cached-json/">Simplifying Model Level Versioning with Mongoid-Cched-Json</a></li>
<li><a href="/blog/2012/05/30/restful-api-caching-with-garner/">RESTful API Caching with Garner</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Friendly URLs with Mongoid::Slug]]></title>
    <link href="http://artsy.github.com/blog/2012/11/22/friendly-urls-with-mongoid-slug/"/>
    <updated>2012-11-22T21:21:00-05:00</updated>
    <id>http://artsy.github.com/blog/2012/11/22/friendly-urls-with-mongoid-slug</id>
    <content type="html"><![CDATA[<p>All Artsy URLs shared publicly are humanly readable. For example, you'll find all Barbara Kruger's works at <a href="http://artsy.net/artist/barbara-kruger">artsy.net/artist/barbara-kruger</a> and a post by Hyperallergic entitled "Superfluous Men Can't Get No Satisfaction" at <a href="http://artsy.net/hyperallergic/post/superfluous-men-cant-get-no-satisfaction">artsy.net/hyperallergic/post/superfluous-men-cant-get-no-satisfaction</a>. This is a lot prettier than having <code>id=42</code> in the browser's address and is a big improvement for SEO.</p>

<p><img src="http://artsy.github.com/images/2012-11-22-friendly-urls-with-mongoid-slug/barbara-kruger.png"></p>

<p>We construct these URLs with a gem called <a href="https://github.com/digitalplaywright/mongoid-slug">mongoid_slug</a>. Interesting implementation details under the cut.</p>

<!-- more -->


<h2>Mongoid-Slug Basics</h2>

<p>Include the gem in Gemfile.</p>

<p><code>ruby Gemfile
gem "mongoid_slug", "~&gt; 2.0.1"
</code></p>

<p>The basic functionality of mongoid-slug is achieved by adding the <code>Mongoid::Slug</code> a mixin and declaring a slug.</p>

<p>``` ruby post.rb
class Post
  include Mongoid::Document
  include Mongoid::Slug</p>

<p>  belongs_to :author, class_name: "User"</p>

<p>  field :title, type: String
  slug :title, history: true, scope: :author</p>

<p>  field :published, type: Boolean
end
```</p>

<p>This adds a <code>_slugs</code> field of type <code>Array</code> into the <code>Post</code> model. Every time the title of the post changes, a new slug is generated and, depending on the value of the <code>history</code> option, either replaces the existing slug or appends the new slug to the array of slugs. A database index ensures that these are unique: two posts of the same title will have different slugs, such as "post-1" and "post-2". Our example is also scoped to the author of the post.</p>

<p>You can now find this post by <code>_id</code> or <code>slug</code> with the same <code>find</code> method. And with <code>history: true</code>, you can find a document by any of its older slugs!</p>

<p>```</p>

<h1>find by ID</h1>

<p>user.posts.find("47cc67093475061e3d95369d")</p>

<h1>find by slug</h1>

<p>user.posts.find("superfluous-men-cant-get-no-satisfaction")
```</p>

<p>Mongoid-slug is smart enough to figure out whether you're querying by a <code>Moped::BSON::ObjectId</code> or a slug. Performance-wise the lookup by slug is cheap: mongoid_slug ensures an index on <code>_slugs</code>. This all works, of course, because MongoDB builds a B-tree index atop all elements in each <code>_slugs</code> array.</p>

<p>The <code>find</code> method will naturally respect Mongoid's <code>raise_not_found_error</code> option and either raise <code>Mongoid::Errors::DocumentNotFound</code> or return <code>nil</code> in the case the document cannot be found.</p>

<h2>Avoiding Too Many Slugs</h2>

<p>Users writing posts may want to edit them many times before they are published. This can potentially create a large number of unnecessary slugs. We've used a simple trick to generate slugs only after a post has been published by defaulting the slug of an unpublished post to its <code>_id</code>. Mongoid-slug will append <code>-1</code> to such slugs, so we monkey-patch <code>Mongoid::Slug::UniqueSlug</code> with the code in <a href="https://gist.github.com/4131766">this gist</a>. Special care must be taken not to destroy a slug of a post that has been published earlier, then unpublished.</p>

<p>``` ruby
slug :title, :published, scope: :author, history: true do |p|
  if p.published? || p.has_slug?</p>

<pre><code>p.title.to_url
</code></pre>

<p>  else</p>

<pre><code>p.id.to_s
</code></pre>

<p>  end
end</p>

<p>def has_slug?
  ! slug.blank? &amp;&amp; slug != id.to_s
end
```</p>

<p>The parameters to <code>slug</code> include all fields that may cause the slug to change. When a post is published by setting <code>published</code> to <code>true</code>, the slug will be re-generated with a call to <code>build_slug</code> as long as the <code>published</code> field is included in that list.</p>

<p>Please note an interesting discussion about allowing model ids in the <code>_slugs</code> <a href="https://github.com/digitalplaywright/mongoid-slug/pull/91">here</a>.</p>

<h2>Caching by Slug</h2>

<p>Because slugs can now change, but lookups by old slugs should hit the cache, caching by slug makes cache invalidation difficult. A two-layered cache that maps slugs to ids and then caches objects by id can solve this at the expense of an additional cache lookup. We have yet to implement this in <a href="https://github.com/artsy/garner">Garner</a>, see <a href="https://github.com/artsy/garner/issues/13">#13</a>.</p>

<h2>International Slugs</h2>

<p>We have a large international audience with names and posts in all kinds of languages. An escaped UTF-8 URL would be much worse than a BSON ObjectId. Fortunately, mongoid-slug uses <a href="https://github.com/rsl/stringex">stringex</a> under the hood. This gem defines <code>to_url</code> and rewrites special symbols and transliterates strings from many languages into English. Here're some examples of generated slugs.</p>

<p>``` ruby
"ITCZ 1 (21°17ʼ51.78”N / 89°35ʼ28.18”O / 26-04-08 / 09:00 am)".to_url</p>

<h1>=> itcz-1-21-degrees-17-51-dot-78-n-slash-89-degrees-35-28-dot-18-o-slash-26-04-08-slash-09-00-am</h1>

<p>"“水／火”系列 No.8".to_url</p>

<h1>=> "shui-slash-huo-xi-lie-no-dot-8"</h1>

<p>"трактат по теории этики".to_url</p>

<h1>=> "traktat-po-tieorii-etiki"</h1>

<p>```</p>

<p>Pretty amazing!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Simplifying Model-Level JSON Versioning with Mongoid-Cached-Json]]></title>
    <link href="http://artsy.github.com/blog/2012/03/23/simplifying-model-level-json-versioning-with-mongoid-cached-json/"/>
    <updated>2012-03-23T09:14:00-04:00</updated>
    <id>http://artsy.github.com/blog/2012/03/23/simplifying-model-level-json-versioning-with-mongoid-cached-json</id>
    <content type="html"><![CDATA[<p>Did you know that Netflix has hundreds of API versions, one for each device? Daniel Jacobson's <a href="http://www.slideshare.net/danieljacobson/techniques-for-scaling-the-netflix-api-qcon-sf">Techniques for Scaling the Netflix API</a> at QConSF 2011 explained why they chose this model. And while we don't all build distributed services that supply custom-tailored data to thousands of heterogeneous TVs and set-top boxes, we do have to pay close attention to API versioning from day one.</p>

<p>Versioning is hard. Your data models evolve, but you must maintain backward-compatibility for your public interfaces. While many strategies exist to deal with this problem, we'd like to propose one that requires very little programming effort and that is more declarative in nature.</p>

<p>At Artsy we use <a href="http://github.com/intridea/grape">Grape</a> and implement the "path" versioning strategy from the <a href="http://github.com/intridea/grape/tree/frontier">frontier</a> branch. Our initial v1 API is consumed by our own website and services and lives at <a href="https://artsyapi.com/api/v1">https://artsyapi.com/api/v1</a>. We've also prototyped v2 and by the time v1 is frozen, it should already be in production.</p>

<p>Grape takes care of version-based routing and has a system that lets you split version-based presentation of a model from the model implementation. I find that separation forcefully induced by unnecessary implementation complexity around wanting to return different JSON depending on the API version requested. What if implementing versioning in <code>as_json</code> were super simple?</p>

<p>Consider a Person model returned from a v1 API.</p>

<p>``` ruby
class API &lt; Grape::API
  prefix :api
  version :v1
  namespace :person</p>

<pre><code>get ":id"
  Person.find(params[:id]).as_json
end
</code></pre>

<p>  end
end
```</p>

<p>``` ruby
class Person
  include Mongoid::Document</p>

<p>  field :name</p>

<p>  def as_json</p>

<pre><code>{
  name: name
}
</code></pre>

<p>  end</p>

<p>end
```</p>

<p>In v2 the model split <code>:name</code> into a <code>:first</code> and <code>:last</code> name and in v3 <code>:name</code> has finally been deprecated. A version v3 Person model would look as follows.</p>

<p>``` ruby
class Person
  include Mongoid::Document</p>

<p>  field :first
  field :last</p>

<p>  def as_json</p>

<pre><code>{
  first: first,
  last: last
}
</code></pre>

<p>  end</p>

<p>end
```</p>

<p>How can we combine these two implementations and write <code>Person.find(params[:id]).as_json({ :version =&gt; ? })</code>?</p>

<p>In <a href="http://github.com/dblock/mongoid-cached-json">mongoid-cached-json</a> we've introduced a declarative way of versioning JSON. Here's the code for Person v3.</p>

<p>``` ruby
class Person
  include Mongoid::Document
  include Mongoid::CachedJson</p>

<p>  field :first
  field :last</p>

<p>  def name</p>

<pre><code>[ first, last ].join(" ")
</code></pre>

<p>  end</p>

<p>  json_fields \</p>

<pre><code>name: { :versions =&gt; [ :v1, :v2 ] },
first: { :versions =&gt; [ :v2, :v3 ] },
last: { :versions =&gt; [ :v2, :v3 ] }
</code></pre>

<p>end
```</p>

<p>With the <a href="http://github.com/dblock/mongoid-cached-json">mongoid-cached-json</a> gem you also get caching that respects JSON versioning, for free. Read about it <a href="http://artsy.github.com/blog/2012/02/20/caching-model-json-with-mongoid-cached-json/">here</a>.</p>
]]></content>
  </entry>
  
</feed>
